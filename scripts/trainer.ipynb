{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334172b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rosemary import jpt_setup; jpt_setup()\n",
    "import os; os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9294819",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import math\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import chain\n",
    "from typing import Optional\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "import pyarrow\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset, IterableDataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.trainer_callback import TrainerState\n",
    "from transformers.trainer import TRAINER_STATE_NAME\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "\n",
    "from doremi.training_args import ModelArguments, DataTrainingArguments, FullTrainingArguments\n",
    "import doremi.dataloader as data_utils\n",
    "from doremi.trainer import DoReMiTrainer\n",
    "from doremi.dataloader import determine_skip_per_domain\n",
    "from doremi.dataloader import interleave_datasets\n",
    "\n",
    "\n",
    "try:\n",
    "    import doremi.models as doremi_models\n",
    "except Exception:\n",
    "    \n",
    "    pass\n",
    "try:\n",
    "    from flash_attn.models.gpt_neox import gpt_neox_config_to_gpt2_config\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.27.0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/language-modeling/requirements.txt\")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47cef8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_dir = \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi\"\n",
    "cache_dir = '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache'\n",
    "preprocessed_data = \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed\"\n",
    "\n",
    "envs = {\n",
    "    \"CACHE\": cache_dir,\n",
    "    \"DOREMI_DIR\": package_dir,\n",
    "    \"PILE_DIR\": os.path.join(package_dir, \"data\", 'raw'),\n",
    "    \"PREPROCESSED_PILE_DIR\": preprocessed_data,\n",
    "    \"MODEL_OUTPUT_DIR\": os.path.join(package_dir, 'results'),\n",
    "    \"PARTITION\": \"el8\",\n",
    "    \"HF_HOME\": cache_dir,\n",
    "    \"TRANSFORMERS_CACHE\": cache_dir,\n",
    "    \"HF_DATASETS_CACHE\": cache_dir,\n",
    "    \"HF_DATASETS_IN_MEMORY_MAX_SIZE\": \"0\",\n",
    "    \"TORCH_EXTENSIONS_DIR\": cache_dir,\n",
    "    \"TMPDIR\": cache_dir,\n",
    "    \"WANDB_DIR\": os.path.join(cache_dir, \"wandb\"),\n",
    "    \"PREPROCESSED_DATA\": preprocessed_data,\n",
    "    'PREPROCESSED_CACHE': os.path.join(cache_dir, 'preprocessed_cache', 'perdomain_pile_preprocessed'),\n",
    "\n",
    "}\n",
    "\n",
    "for k, v in envs.items():\n",
    "    os.environ[k] = v\n",
    "    \n",
    "os.makedirs(cache_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1d4f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate domain weight config\n",
    "import json\n",
    "domain_config_path = os.path.abspath('../configs/humanmix_baseline_50kvocab.json')\n",
    "domain_weights = {\"cot\": .25, \"flan_v2\": .25, \"dolly\": .25, \"oasst1\": .25}\n",
    "domain_weights = {'cot': 0.5, 'flan_v2': 0.25, 'dolly': 0.12, 'oasst1': 0.13}\n",
    "\n",
    "domain_config = {\"train_domain_weights\": domain_weights, \"eval_domain_weights\": domain_weights}\n",
    "with open(domain_config_path, 'w') as f:\n",
    "    json.dump(domain_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb14c3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ModelArguments(model_name_or_path='gpt2', model_type='gpt2', config_overrides=None, config_name=None, tokenizer_name='gpt2', cache_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache', use_fast_tokenizer=True, model_revision='main', use_auth_token=False, torch_dtype=None),\n",
       " DataTrainingArguments(dataset_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed', dataset_name='', max_train_samples=None, max_eval_samples=None, max_token_length=1024, block_size=None, overwrite_cache=False, do_padding=True, add_domain_id=True, preprocessing_num_workers=None, shuffle=True),\n",
       " FullTrainingArguments(output_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=64, eval_accumulation_steps=None, eval_delay=0, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.99, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=200000, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/runs/Aug02_14-33-01_dcs070', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=True, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=5, save_total_limit=1, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=1111, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='train_baseline', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_HF: 'adamw_hf'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, domain_config_path='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/configs/humanmix_baseline_50kvocab.json', lr_end=0.001, reweight_domains=False, reweight_eta=1.0, reweight_eps=0.0001, doremi_optimizer='doremiv1', reference_model_name_or_path='.', lr_scheduler_name=None, train_domain_weights_tmp_file=None))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_name = 'train_baseline'\n",
    "\n",
    "nodes = 1\n",
    "num_gpus = 1\n",
    "\n",
    "model_name_or_path = 'gpt2'; model_type = 'gpt2'\n",
    "cache_dir = envs['CACHE']\n",
    "domain_config_path = os.path.abspath('../configs/humanmix_baseline_50kvocab.json')\n",
    "output_dir = os.path.join(envs['MODEL_OUTPUT_DIR'], job_name)\n",
    "dataset_dir = envs['PREPROCESSED_CACHE']\n",
    "dataset_dir = preprocessed_data\n",
    " \n",
    "total_batch_size = 128 # # 64*8=512\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_accumulation_steps = int(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "\n",
    "max_steps = 200000; save_steps = 5 # 200k steps.\n",
    "\n",
    "# use `dataset_dir` instead of `dataset_name` to specify `preprocessed_dir`\n",
    "# --dataset_name=pile \\\n",
    "\n",
    "## learning rate for pretraining, substituted with finetuning hyperparameters\n",
    "# --learning_rate 1e-3 \\\n",
    "# --lr_end 1e-4 \\\n",
    "# --adam_epsilon 1e-8 \\\n",
    "\n",
    "## don't need cosine scheduling for finetuning\n",
    "# --weight_decay 0.01 \\\n",
    "# --lr_scheduler_name linear_warmup_cosine \\\n",
    "# --warmup_ratio 0.06 \\\n",
    "\n",
    "## avoids grad scaling error\n",
    "# --fp16 \\\n",
    "## for training model from scratch\n",
    "# --config_overrides=\"n_positions=1024,n_embd=1024,n_layer=18,n_head=16\" \\\n",
    "\n",
    "## added the following\n",
    "# add_domain_id: for non-pile preprocessed dataset\n",
    "# do_padding: true for variable size sequences, as in instruction tuning datasets.\n",
    "# --max_train_samples 1000 \\\n",
    "\n",
    "reference_model_name_or_path = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/train_baseline/checkpoint-10'\n",
    "doremi_options = f\"\"\"\n",
    "--doremi_optimizer=doremiv1 \\\n",
    "--reweight_eta=1 \\\n",
    "--reweight_eps=1e-4 \\\n",
    "--train_domain_weights_tmp_file={os.path.join(output_dir, 'domain_weights')} \\\n",
    "--reweight_domains \\\n",
    "--remove_unused_columns=False \\\n",
    "--reference_model_name_or_path={reference_model_name_or_path} \\\n",
    "\"\"\"\n",
    "doremi_options = ''\n",
    "\n",
    "\n",
    "cmd = f\"\"\"\n",
    "--model_name_or_path={model_name_or_path} \\\n",
    "--model_type={model_type} \\\n",
    "--tokenizer_name=gpt2 \\\n",
    "--do_train \\\n",
    "--cache_dir={cache_dir} \\\n",
    "--dataset_dir={dataset_dir} \\\n",
    "--domain_config_path={domain_config_path} \\\n",
    "--max_token_length=1024 \\\n",
    "--per_device_train_batch_size={per_device_train_batch_size} \\\n",
    "--gradient_accumulation_steps={gradient_accumulation_steps} \\\n",
    "--dataloader_num_workers=1 \\\n",
    "--learning_rate=2e-5 \\\n",
    "--lr_scheduler_type=linear \\\n",
    "--warmup_ratio=0.03 \\\n",
    "--weight_decay=0. \\\n",
    "--max_grad_norm=1.0 \\\n",
    "--max_steps={max_steps} \\\n",
    "--evaluation_strategy=no \\\n",
    "--save_strategy=steps \\\n",
    "--save_steps={save_steps} \\\n",
    "--save_total_limit=1 \\\n",
    "--run_name={job_name} \\\n",
    "--seed=1111 \\\n",
    "--logging_strategy=steps \\\n",
    "--logging_steps=10 \\\n",
    "--logging_first_step \\\n",
    "--report_to=tensorboard \\\n",
    "--optim=adamw_hf \\\n",
    "--adam_beta1=0.9 \\\n",
    "--adam_beta2=0.99 \\\n",
    "--add_domain_id=True \\\n",
    "--do_padding=True \\\n",
    "{doremi_options if doremi_options else ''} \\\n",
    "--output_dir={output_dir} \\\n",
    "\"\"\"\n",
    "# --overwrite_output_dir \\\n",
    "\n",
    "import shlex\n",
    "args = shlex.split(cmd)\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, FullTrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n",
    "model_args, data_args, training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d39f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "if training_args.should_log:\n",
    "    # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "    \n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "num_skip_examples = 0\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n",
    "        state = TrainerState.load_from_json(str(Path(last_checkpoint) / TRAINER_STATE_NAME))\n",
    "        global_batch_size = training_args.train_batch_size * training_args.gradient_accumulation_steps * training_args.world_size\n",
    "        num_skip_examples = state.global_step * global_batch_size\n",
    "        logger.info(f\"Skipping {num_skip_examples} examples\")\n",
    "        \n",
    "last_checkpoint, num_skip_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28eed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f9dc83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "\n",
    "config_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "}\n",
    "if model_args.config_name:\n",
    "    config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
    "elif model_args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "    if model_args.model_type == 'gpt_neox_flash':\n",
    "        config = gpt_neox_config_to_gpt2_config(config)\n",
    "        config.use_flash_attn = True\n",
    "        config.fused_mlp = True\n",
    "        config.fused_bias_fc = True\n",
    "        config.fused_dropout_add_ln = True\n",
    "        config.pad_vocab_size_multiple = 8\n",
    "        config.activation_function = 'gelu_new'\n",
    "        config.n_inner = None\n",
    "        # disable absolute\n",
    "        config.max_position_embeddings = 0\n",
    "else:\n",
    "    if model_args.model_type == 'gpt_flash': \n",
    "        config = GPT2Config(\n",
    "                vocab_size=50257, n_positions=2048, n_embd=2048,\n",
    "                n_layer=24, n_head=16, \n",
    "                scale_attn_by_inverse_layer_idx=True, \n",
    "                rotary_emb_fraction=0.5,\n",
    "                use_flash_attn=True, fused_mlp=True,\n",
    "                fused_bias_fc=True, fused_dropout_add_ln=True, \n",
    "                pad_vocab_size_multiple=8)\n",
    "        # disable absolute\n",
    "        config.max_position_embeddings = 0\n",
    "    elif model_args.model_type == 'gpt_neox_flash':\n",
    "        # convert to GPT2 config\n",
    "        config = CONFIG_MAPPING['gpt_neox']() \n",
    "        config = gpt_neox_config_to_gpt2_config(config)\n",
    "        config.use_flash_attn = True\n",
    "        config.fused_mlp = True\n",
    "        config.fused_bias_fc = True\n",
    "        config.fused_dropout_add_ln = True\n",
    "        config.pad_vocab_size_multiple = 8\n",
    "        config.activation_function = 'gelu_new'\n",
    "        config.n_inner = None\n",
    "        # disable absolute\n",
    "        config.max_position_embeddings = 0\n",
    "    else:\n",
    "        config = CONFIG_MAPPING[model_args.model_type]()\n",
    "    logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "    if model_args.config_overrides is not None:\n",
    "        logger.info(f\"Overriding config: {model_args.config_overrides}\")\n",
    "        config.update_from_string(model_args.config_overrides)\n",
    "        logger.info(f\"New config: {config}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29572f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"use_fast\": model_args.use_fast_tokenizer,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "}\n",
    "\n",
    "if model_args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
    "\n",
    "elif model_args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )\n",
    "    \n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c98bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if model_args.model_name_or_path:\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "    if model_args.model_type in {'gpt_flash', 'gpt_neox_flash'}:\n",
    "        model = doremi_models.GPTFlashAttnLMHeadModel.from_pretrained(\n",
    "            model_args.model_name_or_path, config=config)\n",
    "    elif model_args.model_type in ['gpt2']:\n",
    "        model = doremi_models.GPT2LMHeadModelDoReMi.from_pretrained(\n",
    "            model_args.model_name_or_path, \n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "            torch_dtype=torch_dtype,\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "            torch_dtype=torch_dtype,\n",
    "        )\n",
    "else:\n",
    "    if model_args.model_type in {'gpt_flash', 'gpt_neox_flash'}:\n",
    "        model = doremi_models.GPTFlashAttnLMHeadModel(config)\n",
    "    elif model_args.model_type in {'gpt2'}:\n",
    "        model = doremi_models.GPT2LMHeadModelDoReMi(config)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "    n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())\n",
    "    logger.info(f\"Training new model from scratch - Total size={n_params/2**20:.2f}M params\")\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6364e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Tuple, Union\n",
    "# from transformers import GPT2LMHeadModel\n",
    "# # from doremi.models import CausalLMOutputWithDomainIDs\n",
    "# from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "# from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# @dataclass\n",
    "# class CausalLMOutputWithDomainIDs(CausalLMOutputWithCrossAttentions):\n",
    "#     domain_ids: Optional[torch.LongTensor] = None\n",
    "#     reference_pertoken_loss: Optional[torch.FloatTensor] = None  # corresponds to uniq_domain_ids\n",
    "#     pertoken_loss: Optional[torch.FloatTensor] = None  # corresponds to uniq_domain_ids\n",
    "#     token_mask: Optional[torch.BoolTensor] = None  # 1 for tokens that are not padding\n",
    "\n",
    "\n",
    "\n",
    "# #             model_args.model_name_or_path,\n",
    "# #             from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "# #             config=config,\n",
    "# #             cache_dir=model_args.cache_dir,\n",
    "# #             revision=model_args.model_revision,\n",
    "# #             use_auth_token=True if model_args.use_auth_token else None,\n",
    "# #             torch_dtype=torch_dtype,\n",
    "\n",
    "# # model = doremi_models.GPTFlashAttnLMHeadModel.from_pretrained(\n",
    "# #     model_args.model_name_or_path, config=config)\n",
    "\n",
    "        \n",
    "# model = GPTLMHeadModelDoReMi.from_pretrained(\n",
    "#     model_args.model_name_or_path, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48bb5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device, batch['input_ids'].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(**batch, return_pertoken_losses=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce8ff80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out.pertoken_loss.shape, out.token_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601bf542",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fe33ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collate_fn = data_utils.get_data_collator(tokenizer, do_padding=data_args.do_padding)\n",
    "\n",
    "batch = list(train_dataset.take(3))\n",
    "batch = collate_fn(batch)\n",
    "batch = {k: v.to('cuda') for k,v in batch.items()}\n",
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a8cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf74f1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(training_args.domain_config_path, 'r') as f:\n",
    "    domain_config = json.load(f)\n",
    "\n",
    "train_domain_weights_dict = domain_config['train_domain_weights']\n",
    "eval_domain_weights_dict = domain_config['eval_domain_weights']\n",
    "# whenever we convert dict to array, we sort by key\n",
    "domain_list = list(sorted(train_domain_weights_dict.keys()))\n",
    "num_domains = len(domain_list)\n",
    "\n",
    "print(domain_list, num_domains, train_domain_weights_dict)\n",
    "\n",
    "if training_args.do_train:\n",
    "    # data script could change tokenizer shape\n",
    "    train_dataset = data_utils.get_preprocessed_mixed_dataset(\n",
    "            preprocessed_dir=data_args.dataset_dir,\n",
    "            domain_weights_dict=train_domain_weights_dict,\n",
    "            dataset_name=data_args.dataset_name,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            split='train',\n",
    "            max_samples=data_args.max_train_samples,\n",
    "            add_domain_id=data_args.add_domain_id,\n",
    "            tmp_file=None,\n",
    "            seed=training_args.seed,\n",
    "            tokenizer=tokenizer,\n",
    "            shuffle=data_args.shuffle,\n",
    "            num_skip_examples=num_skip_examples,\n",
    "            shard_reversal=training_args.reweight_domains,\n",
    "            training_args=training_args,\n",
    "    )\n",
    "\n",
    "if training_args.do_eval:\n",
    "    eval_dataset = data_utils.get_preprocessed_mixed_dataset(\n",
    "            preprocessed_dir=data_args.dataset_dir,\n",
    "            domain_weights_dict=eval_domain_weights_dict,\n",
    "            dataset_name=data_args.dataset_name,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            split='validation',\n",
    "            add_domain_id=data_args.add_domain_id,\n",
    "            max_samples=data_args.max_eval_samples,\n",
    "            tokenizer=tokenizer,\n",
    "            no_interleave=True,\n",
    "            training_args=training_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4e911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# preprocessed_dir=data_args.dataset_dir\n",
    "# domain_weights_dict=train_domain_weights_dict\n",
    "# dataset_name=data_args.dataset_name\n",
    "# cache_dir=model_args.cache_dir\n",
    "# split='train'\n",
    "# max_samples=data_args.max_train_samples\n",
    "# add_domain_id=data_args.add_domain_id\n",
    "# tmp_file=None\n",
    "# seed=training_args.seed\n",
    "# tokenizer=tokenizer\n",
    "# shuffle=data_args.shuffle\n",
    "# num_skip_examples=num_skip_examples\n",
    "# shard_reversal=training_args.reweight_domains\n",
    "# no_interleave=False\n",
    "\n",
    "# print(preprocessed_dir)\n",
    "# print(domain_weights_dict)\n",
    "# print(dataset_name)\n",
    "# print(cache_dir)\n",
    "# print(split)\n",
    "# print(max_samples)\n",
    "# print(add_domain_id)\n",
    "# print(seed)\n",
    "# print(shuffle)\n",
    "# print(num_skip_examples)\n",
    "# print(shard_reversal)\n",
    "\n",
    "\n",
    "# domain_names = list(sorted(domain_weights_dict.keys()))\n",
    "# domain_to_idx = {domain_names[i]: i for i in range(len(domain_names))}\n",
    "# domain_weights = np.asarray([domain_weights_dict[domain_name] for domain_name in domain_names])\n",
    "# domain_weights = domain_weights / domain_weights.sum()\n",
    "\n",
    "# print()\n",
    "# print(json.dumps({'domain_names': domain_names, \n",
    "#                   'domain_to_idx': domain_to_idx, \n",
    "#                   'domain_weights': list(domain_weights)},\n",
    "#                 indent=4))\n",
    "\n",
    "\n",
    "# # write domain weights to file if tmp_file is set\n",
    "# if tmp_file is not None:\n",
    "#     probabilities_tmp_file = tmp_file\n",
    "\n",
    "#     with open(str(probabilities_tmp_file), 'wb') as f:\n",
    "#         pickle.dump(domain_weights, f)\n",
    "#     probabilities = None\n",
    "# else:\n",
    "#     probabilities = domain_weights\n",
    "#     probabilities_tmp_file = None\n",
    "\n",
    "\n",
    "# print()\n",
    "# print(json.dumps({'probabilities': list(probabilities)}, indent=4))\n",
    "\n",
    "# # from doremi.dataloader import get_perdomain_datasets\n",
    "# # all_ds = get_perdomain_datasets(\n",
    "# #     preprocessed_dir, \n",
    "# #     domain_weights_dict,\n",
    "# #     cache_dir=cache_dir,\n",
    "# #     split=split,\n",
    "# #     seed=seed,\n",
    "# #     domain_weights=domain_weights,\n",
    "# #     domain_names=domain_names,\n",
    "# #     num_skip_examples=num_skip_examples,\n",
    "# #     shuffle=shuffle,\n",
    "# #     shard_reversal=shard_reversal\n",
    "# # )\n",
    "\n",
    "# domain_name_to_skip_num = determine_skip_per_domain(num_skip_examples, seed, domain_weights, domain_names)\n",
    "\n",
    "# preprocessed_dir = Path(preprocessed_dir)\n",
    "# if split is not None and (preprocessed_dir / split).exists():\n",
    "#     preprocessed_dir = preprocessed_dir / split\n",
    "# else:\n",
    "#     logger.warn(f\"No split used or split directory not found: using same data for all splits.\")\n",
    "\n",
    "# domains = list(sorted(domain_weights_dict.keys()))\n",
    "\n",
    "# print(preprocessed_dir)\n",
    "# print(domain_name_to_skip_num)\n",
    "# print()\n",
    "# print(json.dumps({'preprocessed_dir': str(preprocessed_dir), \n",
    "#                   'domain_name_to_skip_num': domain_name_to_skip_num}, indent=4))\n",
    "\n",
    "\n",
    "# all_ds = {}\n",
    "# for domain in domains:\n",
    "#     domain_dir = preprocessed_dir / domain\n",
    "    \n",
    "#     ## wpq: read instruction tuning dataset off `jsonl` files\n",
    "#     if (domain_dir / f'{domain}_data.jsonl').exists():\n",
    "#         from datasets import load_dataset\n",
    "#         from functools import partial\n",
    "#         from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n",
    "#         from doremi.dataloader import skippable_data_gen_dataset\n",
    "\n",
    "#         data_files = {'train': str(domain_dir / f'{domain}_data.jsonl')}\n",
    "#         raw_datasets = load_dataset(\n",
    "#             \"json\",\n",
    "#             data_files=data_files,\n",
    "#             cache_dir=cache_dir,\n",
    "#             use_auth_token=True if model_args.use_auth_token else None,\n",
    "#         )\n",
    "#         # Preprocessing the datasets.\n",
    "#         if \"prompt\" in raw_datasets[\"train\"].column_names and \"completion\" in raw_datasets[\"train\"].column_names:\n",
    "#             encode_function = partial(\n",
    "#                 encode_with_prompt_completion_format,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 max_seq_length=1024,\n",
    "#             )\n",
    "#         elif \"messages\" in raw_datasets[\"train\"].column_names:\n",
    "#             encode_function = partial(\n",
    "#                 encode_with_messages_format,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 max_seq_length=1024,\n",
    "#             )\n",
    "#         else:\n",
    "#             raise ValueError(\"You need to have either 'prompt'&'completion' or 'messages' in your column names.\")\n",
    "\n",
    "#         with training_args.main_process_first(local=False, desc=\"Processing instruction data\"):\n",
    "#             lm_datasets = raw_datasets.map(\n",
    "#                 encode_function,\n",
    "#                 num_proc=16,\n",
    "#                 batched=False,\n",
    "#             )\n",
    "#             lm_datasets.set_format(type=\"pt\")\n",
    "#         ds = lm_datasets['train']\n",
    "#         ds = IterableDataset.from_generator(\n",
    "#                 skippable_data_gen_dataset,\n",
    "#                 gen_kwargs={'ds': ds,\n",
    "#                             'num_skip_examples': domain_name_to_skip_num[domain],\n",
    "#                             'loop': (split == 'train'),\n",
    "#                             'seed': seed,\n",
    "#                             'shuffle': shuffle}\n",
    "#                 )\n",
    "#         seed += 1\n",
    "#     elif (domain_dir / 'dataset_info.json').exists():\n",
    "#         ds = load_from_disk(dataset_path=str(domain_dir))\n",
    "#         logger.info(f\"Loaded {domain_dir}. Length: {len(ds)}\")\n",
    "#     else:\n",
    "#         curr_shards = list(domain_dir.iterdir())\n",
    "#         if shard_reversal:\n",
    "#             curr_shards = list(reversed(curr_shards))\n",
    "#         # shuffle shard order\n",
    "#         random.Random(seed).shuffle(curr_shards)\n",
    "#         ds = IterableDataset.from_generator(\n",
    "#                 skippable_data_gen,\n",
    "#                 gen_kwargs={'shards': curr_shards,\n",
    "#                             'num_skip_examples': domain_name_to_skip_num[domain],\n",
    "#                             'loop': (split == 'train'),\n",
    "#                             'seed': seed,\n",
    "#                             'shuffle': shuffle}\n",
    "#                 )\n",
    "#         seed += 1\n",
    "#     all_ds[domain] = ds\n",
    "    \n",
    "\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# def add_domain_id_generator(ds, domain_idx):\n",
    "#     for ex in ds:\n",
    "#         ex['domain_id'] = domain_idx\n",
    "#         yield ex\n",
    "        \n",
    "# domain_ds_ls = []\n",
    "# for domain_name in domain_names:\n",
    "#     domain_idx = domain_to_idx[domain_name]\n",
    "#     domain_ds = all_ds[domain_name]\n",
    "#     # add domain_id if necessary\n",
    "#     if add_domain_id:\n",
    "#         domain_ds = IterableDataset.from_generator(\n",
    "#             add_domain_id_generator, \n",
    "#             gen_kwargs={'ds': domain_ds, 'domain_idx': domain_idx})\n",
    "#     domain_ds_ls.append(domain_ds)\n",
    "\n",
    "# if no_interleave:\n",
    "#     # instead of interleaving, run through each dataset\n",
    "#     def data_generator(shards):\n",
    "#         for shard in shards:\n",
    "#             for ex in shard:\n",
    "#                 yield ex\n",
    "#     ds = IterableDataset.from_generator(data_generator, gen_kwargs={'shards': domain_ds_ls})\n",
    "#     logger.info(\"Not interleaving dataset - will not sample according to domain weights\")\n",
    "\n",
    "# else:\n",
    "#     ds = interleave_datasets(\n",
    "#             domain_ds_ls,\n",
    "#             probabilities=probabilities,\n",
    "#             probabilities_file=probabilities_tmp_file,\n",
    "#             seed=seed)\n",
    "    \n",
    "\n",
    "# def take_data_generator(ds, max_samples):\n",
    "#     idx = 0\n",
    "#     for ex in ds:\n",
    "#         yield ex\n",
    "#         idx += 1\n",
    "#         if max_samples is not None and idx >= max_samples:\n",
    "#             return\n",
    "\n",
    "# ds = IterableDataset.from_generator(take_data_generator, gen_kwargs={'ds': ds, 'max_samples': max_samples})\n",
    "# train_dataset = ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5426031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ds = load_dataset(\n",
    "#     \"json\",\n",
    "#     data_files='test.jsonl',\n",
    "#     cache_dir=model_args.cache_dir)['train']\n",
    "# test_ds[0]\n",
    "\n",
    "# for x in test_ds.to_iterable_dataset():\n",
    "#     print(x)\n",
    "# for i, v in enumerate(ds):\n",
    "#     if i == 10:\n",
    "#         break\n",
    "#     print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if training_args.reweight_domains:\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "    if model_args.model_type in {'gpt_flash', 'gpt_neox_flash'}:\n",
    "        model_cls = doremi_models.GPTFlashAttnLMHeadModel\n",
    "        reference_model = model_cls.from_pretrained(\n",
    "            training_args.reference_model_name_or_path,\n",
    "            config=config)\n",
    "    elif model_args.model_type in {'gpt2'}:\n",
    "        model_cls = doremi_models.GPT2LMHeadModelDoReMi\n",
    "        reference_model = model_cls.from_pretrained(\n",
    "            training_args.reference_model_name_or_path,\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "            torch_dtype=torch_dtype,\n",
    "        )\n",
    "    else:\n",
    "        model_cls = AutoModelForCausalLM\n",
    "\n",
    "        reference_model = model_cls.from_pretrained(\n",
    "            training_args.reference_model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "            torch_dtype=torch_dtype,\n",
    "        )\n",
    "    for param in reference_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    reference_model.eval()\n",
    "    model.reference_model = reference_model\n",
    "    model.register_buffer('train_domain_weights', torch.tensor(\n",
    "            [train_domain_weights_dict[domain] for domain in domain_list]))\n",
    "    model.register_buffer('avg_domain_weights', model.train_domain_weights.clone())\n",
    "    model.register_buffer('perdomain_scores', torch.ones(len(train_domain_weights_dict)) * np.log(len(tokenizer)))\n",
    "    model.register_buffer('update_counter', torch.tensor(1))\n",
    "\n",
    "else:\n",
    "    reference_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# turn off find unused parameters\n",
    "training_args.ddp_find_unused_parameters = False\n",
    "\n",
    "# We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "# on a small vocab and want a smaller embedding size, remove this test.\n",
    "# embedding_size = model.get_input_embeddings.weight.shape[0]\n",
    "# if len(tokenizer) > embedding_size:\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = DoReMiTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_utils.get_data_collator(tokenizer, do_padding=data_args.do_padding),\n",
    ")\n",
    "\n",
    "print(trainer.args.max_grad_norm, \\\n",
    "    trainer.sharded_ddp, \\\n",
    "    trainer.args.half_precision_backend, \\\n",
    "    trainer.do_grad_scaling)\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a27bcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "\n",
    "    if training_args.reweight_domains:\n",
    "        avg_domain_weights_dict = {}\n",
    "        for i in range(len(model.avg_domain_weights)):\n",
    "            domain_name = domain_list[i]\n",
    "            metrics[f'avg_domain_weight:{domain_name}'] = model.avg_domain_weights[i].item()\n",
    "            avg_domain_weights_dict[domain_name] = model.avg_domain_weights[i].item()\n",
    "\n",
    "        # save avg domain weights to json\n",
    "        avg_domain_weights_file = Path(training_args.output_dir) / 'avg_domain_weights.json'\n",
    "        with open(avg_domain_weights_file, 'w') as f:\n",
    "            json.dump(avg_domain_weights_dict, f, indent=2)\n",
    "\n",
    "        # also save to configs dir\n",
    "        config_dict = {\"train_domain_weights\": avg_domain_weights_dict,\n",
    "                       \"eval_domain_weights\": avg_domain_weights_dict}\n",
    "        config_dict_file = Path(__file__).parent.parent / 'configs' / f\"{Path(training_args.output_dir).name}.json\"\n",
    "        with open(config_dict_file, 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584e6abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
