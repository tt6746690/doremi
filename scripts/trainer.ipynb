{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334172b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_setup; jpt_setup()\n",
    "import os; os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9294819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-01 23:13:47,210] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import logging\n",
    "import math\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import chain\n",
    "from typing import Optional\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "import pyarrow\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset, IterableDataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.trainer_callback import TrainerState\n",
    "from transformers.trainer import TRAINER_STATE_NAME\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "\n",
    "from doremi.training_args import ModelArguments, DataTrainingArguments, FullTrainingArguments\n",
    "import doremi.dataloader as data_utils\n",
    "from doremi.trainer import DoReMiTrainer\n",
    "from doremi.dataloader import determine_skip_per_domain\n",
    "from doremi.dataloader import interleave_datasets\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    import doremi.models as doremi_models\n",
    "except Exception:\n",
    "    \n",
    "    pass\n",
    "try:\n",
    "    from flash_attn.models.gpt_neox import gpt_neox_config_to_gpt2_config\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.27.0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/language-modeling/requirements.txt\")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47cef8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_dir = \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi\"\n",
    "cache_dir = '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache'\n",
    "preprocessed_data = \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed\"\n",
    "\n",
    "envs = {\n",
    "    \"CACHE\": cache_dir,\n",
    "    \"DOREMI_DIR\": package_dir,\n",
    "    \"PILE_DIR\": os.path.join(package_dir, \"data\", 'raw'),\n",
    "    \"PREPROCESSED_PILE_DIR\": preprocessed_data,\n",
    "    \"MODEL_OUTPUT_DIR\": os.path.join(package_dir, 'results'),\n",
    "    \"PARTITION\": \"el8\",\n",
    "    \"HF_HOME\": cache_dir,\n",
    "    \"TRANSFORMERS_CACHE\": cache_dir,\n",
    "    \"HF_DATASETS_CACHE\": cache_dir,\n",
    "    \"HF_DATASETS_IN_MEMORY_MAX_SIZE\": \"0\",\n",
    "    \"TORCH_EXTENSIONS_DIR\": cache_dir,\n",
    "    \"TMPDIR\": cache_dir,\n",
    "    \"WANDB_DIR\": os.path.join(cache_dir, \"wandb\"),\n",
    "    \"PREPROCESSED_DATA\": preprocessed_data,\n",
    "    'PREPROCESSED_CACHE': os.path.join(cache_dir, 'preprocessed_cache', 'perdomain_pile_preprocessed'),\n",
    "\n",
    "}\n",
    "\n",
    "for k, v in envs.items():\n",
    "    os.environ[k] = v\n",
    "    \n",
    "os.makedirs(cache_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad168f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CACHE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache;export DOREMI_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi;export PILE_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/raw;export PREPROCESSED_PILE_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed;export MODEL_OUTPUT_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results;export PARTITION=el8;export HF_HOME=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache;export TRANSFORMERS_CACHE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache;export HF_DATASETS_CACHE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache;export HF_DATASETS_IN_MEMORY_MAX_SIZE=0;export TORCH_EXTENSIONS_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache;export TMPDIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache;export WANDB_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/wandb;export PREPROCESSED_DATA=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed;export PREPROCESSED_CACHE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/preprocessed_cache/perdomain_pile_preprocessed\n"
     ]
    }
   ],
   "source": [
    "print(';'.join([f'export {k}={v}' for k, v in envs.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e1d4f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate domain weight config\n",
    "import json\n",
    "domain_config_path = '../configs/humanmix_baseline_50kvocab.json'\n",
    "domain_weights = {\"cot\": .25, \"flan_v2\": .25, \"dolly\": .25, \"oasst1\": .25}\n",
    "domain_weights = {'cot': 0.5, 'flan_v2': 0.25, 'dolly': 0.12, 'oasst1': 0.13}\n",
    "\n",
    "domain_config = {\"train_domain_weights\": domain_weights, \"eval_domain_weights\": domain_weights}\n",
    "with open(domain_config_path, 'w') as f:\n",
    "    json.dump(domain_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "435d9062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # move domain datasets to the right folder.\n",
    "# src_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/data/processed'\n",
    "# tgt_dir = preprocessed_data\n",
    "\n",
    "# def copyanything(src, dst):\n",
    "#     import shutil, errno\n",
    "#     try:\n",
    "#         shutil.copytree(src, dst)\n",
    "#     except OSError as exc: # python >2.5\n",
    "#         if exc.errno in (errno.ENOTDIR, errno.EINVAL):\n",
    "#             shutil.copy(src, dst)\n",
    "#         else: raise\n",
    "\n",
    "# for domain in  domain_weights.keys():\n",
    "#     src_domain_dir = os.path.join(src_dir, domain)\n",
    "#     tgt_domain_dir = os.path.join(tgt_dir, domain)\n",
    "#     print(src_domain_dir, os.path.isdir(src_domain_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb14c3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1301] 2023-08-01 23:22:54,278 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "[INFO|training_args.py:1716] 2023-08-01 23:22:54,279 >> PyTorch: setting up devices\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ModelArguments(model_name_or_path=None, model_type='gpt2', config_overrides=None, config_name=None, tokenizer_name='gpt2', cache_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache', use_fast_tokenizer=True, model_revision='main', use_auth_token=False, torch_dtype=None),\n",
       " DataTrainingArguments(dataset_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed', dataset_name='', max_train_samples=1000, max_eval_samples=None, max_token_length=1024, block_size=None, overwrite_cache=False, do_padding=True, add_domain_id=True, preprocessing_num_workers=None, shuffle=True),\n",
       " FullTrainingArguments(output_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=64, eval_accumulation_steps=None, eval_delay=0, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.99, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=200000, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/runs/Aug01_23-22-54_dcs002', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=True, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=5, save_total_limit=1, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=1111, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='train_baseline', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_HF: 'adamw_hf'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, domain_config_path='../configs/humanmix_baseline_50kvocab.json', lr_end=0.001, reweight_domains=False, reweight_eta=1.0, reweight_eps=0.0001, doremi_optimizer='doremiv1', reference_model_name_or_path='.', lr_scheduler_name=None, train_domain_weights_tmp_file=None))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_name = 'train_baseline'\n",
    "\n",
    "nodes = 1\n",
    "num_gpus = 1\n",
    "\n",
    "model_name_or_path = 'gpt2'\n",
    "cache_dir = envs['CACHE']\n",
    "domain_config_path = '../configs/humanmix_baseline_50kvocab.json'\n",
    "output_dir = os.path.join(envs['MODEL_OUTPUT_DIR'], job_name)\n",
    "dataset_dir = envs['PREPROCESSED_CACHE']\n",
    "dataset_dir = preprocessed_data\n",
    " \n",
    "total_batch_size = 128 # # 64*8=512\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_accumulation_steps = int(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "\n",
    "max_steps = 200000; save_steps = 5 # 200k steps.\n",
    "\n",
    "# use `dataset_dir` instead of `dataset_name` to specify `preprocessed_dir`\n",
    "# --dataset_name=pile \\\n",
    "\n",
    "## learning rate for pretraining, substituted with finetuning hyperparameters\n",
    "# --learning_rate 1e-3 \\\n",
    "# --lr_end 1e-4 \\\n",
    "# --adam_epsilon 1e-8 \\\n",
    "\n",
    "## don't need cosine scheduling for finetuning\n",
    "# --weight_decay 0.01 \\\n",
    "# --lr_scheduler_name linear_warmup_cosine \\\n",
    "# --warmup_ratio 0.06 \\\n",
    "\n",
    "## avoids grad scaling error\n",
    "# --fp16 \\\n",
    "## for training model from scratch\n",
    "# --config_overrides=\"n_positions=1024,n_embd=1024,n_layer=18,n_head=16\" \\\n",
    "\n",
    "## added the following\n",
    "# add_domain_id: for non-pile preprocessed dataset\n",
    "# do_padding: true for variable size sequences, as in instruction tuning datasets.\n",
    "\n",
    "\n",
    "cmd = f\"\"\"\n",
    "--model_type={model_name_or_path} \\\n",
    "--tokenizer_name=gpt2 \\\n",
    "--do_train \\\n",
    "--cache_dir={cache_dir} \\\n",
    "--dataset_dir={dataset_dir} \\\n",
    "--domain_config_path={domain_config_path} \\\n",
    "--output_dir={output_dir} \\\n",
    "--max_token_length=1024 \\\n",
    "--per_device_train_batch_size={per_device_train_batch_size} \\\n",
    "--gradient_accumulation_steps={gradient_accumulation_steps} \\\n",
    "--dataloader_num_workers=1 \\\n",
    "--learning_rate=2e-5 \\\n",
    "--lr_scheduler_type=linear \\\n",
    "--warmup_ratio=0.03 \\\n",
    "--weight_decay=0. \\\n",
    "--max_grad_norm=1.0 \\\n",
    "--max_steps={max_steps} \\\n",
    "--evaluation_strategy=no \\\n",
    "--save_strategy=steps \\\n",
    "--save_steps={save_steps} \\\n",
    "--save_total_limit=1 \\\n",
    "--run_name={job_name} \\\n",
    "--seed=1111 \\\n",
    "--logging_strategy=steps \\\n",
    "--logging_steps=10 \\\n",
    "--logging_first_step \\\n",
    "--report_to=tensorboard \\\n",
    "--optim=adamw_hf \\\n",
    "--adam_beta1=0.9 \\\n",
    "--adam_beta2=0.99 \\\n",
    "--add_domain_id True \\\n",
    "--do_padding True \\\n",
    "--overwrite_output_dir \\\n",
    "--max_train_samples 1000\n",
    "\"\"\"\n",
    "\n",
    "import shlex\n",
    "args = shlex.split(cmd)\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, FullTrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n",
    "model_args, data_args, training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d39f3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 23:22:54 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "08/01/2023 23:22:54 - INFO - __main__ - Training/evaluation parameters FullTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.99,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=1,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "domain_config_path=../configs/humanmix_baseline_50kvocab.json,\n",
      "doremi_optimizer=doremiv1,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=64,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/runs/Aug01_23-22-54_dcs002,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_end=0.001,\n",
      "lr_scheduler_name=None,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=200000,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "reference_model_name_or_path=.,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "reweight_domains=False,\n",
      "reweight_eps=0.0001,\n",
      "reweight_eta=1.0,\n",
      "run_name=train_baseline,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=5,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=1111,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_domain_weights_tmp_file=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "if training_args.should_log:\n",
    "    # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "    \n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8d2de45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "num_skip_examples = 0\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n",
    "        state = TrainerState.load_from_json(str(Path(last_checkpoint) / TRAINER_STATE_NAME))\n",
    "        global_batch_size = training_args.train_batch_size * training_args.gradient_accumulation_steps * training_args.world_size\n",
    "        num_skip_examples = state.global_step * global_batch_size\n",
    "        logger.info(f\"Skipping {num_skip_examples} examples\")\n",
    "        \n",
    "last_checkpoint, num_skip_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e28eed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0f9dc83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 23:23:00 - WARNING - __main__ - You are instantiating a new config instance from scratch.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "\n",
    "config_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "}\n",
    "if model_args.config_name:\n",
    "    config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
    "elif model_args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "    if model_args.model_type == 'gpt_neox_flash':\n",
    "        config = gpt_neox_config_to_gpt2_config(config)\n",
    "        config.use_flash_attn = True\n",
    "        config.fused_mlp = True\n",
    "        config.fused_bias_fc = True\n",
    "        config.fused_dropout_add_ln = True\n",
    "        config.pad_vocab_size_multiple = 8\n",
    "        config.activation_function = 'gelu_new'\n",
    "        config.n_inner = None\n",
    "        # disable absolute\n",
    "        config.max_position_embeddings = 0\n",
    "else:\n",
    "    if model_args.model_type == 'gpt_flash': \n",
    "        config = GPT2Config(\n",
    "                vocab_size=50257, n_positions=2048, n_embd=2048,\n",
    "                n_layer=24, n_head=16, \n",
    "                scale_attn_by_inverse_layer_idx=True, \n",
    "                rotary_emb_fraction=0.5,\n",
    "                use_flash_attn=True, fused_mlp=True,\n",
    "                fused_bias_fc=True, fused_dropout_add_ln=True, \n",
    "                pad_vocab_size_multiple=8)\n",
    "        # disable absolute\n",
    "        config.max_position_embeddings = 0\n",
    "    elif model_args.model_type == 'gpt_neox_flash':\n",
    "        # convert to GPT2 config\n",
    "        config = CONFIG_MAPPING['gpt_neox']() \n",
    "        config = gpt_neox_config_to_gpt2_config(config)\n",
    "        config.use_flash_attn = True\n",
    "        config.fused_mlp = True\n",
    "        config.fused_bias_fc = True\n",
    "        config.fused_dropout_add_ln = True\n",
    "        config.pad_vocab_size_multiple = 8\n",
    "        config.activation_function = 'gelu_new'\n",
    "        config.n_inner = None\n",
    "        # disable absolute\n",
    "        config.max_position_embeddings = 0\n",
    "    else:\n",
    "        config = CONFIG_MAPPING[model_args.model_type]()\n",
    "    logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "    if model_args.config_overrides is not None:\n",
    "        logger.info(f\"Overriding config: {model_args.config_overrides}\")\n",
    "        config.update_from_string(model_args.config_overrides)\n",
    "        logger.info(f\"New config: {config}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b29572f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_auto.py:512] 2023-08-01 23:23:01,811 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:712] 2023-08-01 23:23:01,835 >> loading configuration file config.json from cache at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-01 23:23:01,837 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.32.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1839] 2023-08-01 23:23:01,889 >> loading file vocab.json from cache at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1839] 2023-08-01 23:23:01,890 >> loading file merges.txt from cache at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1839] 2023-08-01 23:23:01,890 >> loading file tokenizer.json from cache at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1839] 2023-08-01 23:23:01,891 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1839] 2023-08-01 23:23:01,892 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1839] 2023-08-01 23:23:01,892 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:712] 2023-08-01 23:23:01,894 >> loading configuration file config.json from cache at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-01 23:23:01,896 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.32.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"use_fast\": model_args.use_fast_tokenizer,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "}\n",
    "\n",
    "if model_args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
    "\n",
    "elif model_args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )\n",
    "    \n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83c98bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:603] 2023-08-01 23:23:04,013 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.32.0.dev0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 23:23:12 - INFO - __main__ - Training new model from scratch - Total size=118.68M params\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if model_args.model_name_or_path:\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "    if model_args.model_type in {'gpt_flash', 'gpt_neox_flash'}:\n",
    "        model = doremi_models.GPTFlashAttnLMHeadModel.from_pretrained(model_args.model_name_or_path, config=config)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "            torch_dtype=torch_dtype,\n",
    "        )\n",
    "else:\n",
    "    if model_args.model_type in {'gpt_flash', 'gpt_neox_flash'}:\n",
    "        model = doremi_models.GPTFlashAttnLMHeadModel(config)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "    n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())\n",
    "    logger.info(f\"Training new model from scratch - Total size={n_params/2**20:.2f}M params\")\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e222fb26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6364e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"transformers_version\": \"4.32.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from doremi.models import CausalLMOutputWithDomainIDs\n",
    "\n",
    "class GPTLMHeadModelDoReMi(GPT2LMHeadModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.ignore_index = -100\n",
    "        self.loss_fct = CrossEntropyLoss(reduction='mean', ignore_index=self.ignore_index)\n",
    "        self.pertoken_loss_fct = CrossEntropyLoss(reduction='none', ignore_index=self.ignore_index)\n",
    "        self.reference_model = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        ## wpq: re-order to match transformers's gpt2 args order.\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        ## wpq: doremi specific\n",
    "        domain_ids: Optional[torch.LongTensor] = None,\n",
    "        return_pertoken_losses: Optional[bool] = False,\n",
    "        inference_params: Optional[dict] = None,\n",
    "        last_token_only: Optional[bool] = False,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithDomainIDs]:\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if not return_pertoken_losses:\n",
    "            lm_logits = super().forward(\n",
    "                    input_ids=input_ids,\n",
    "                    position_ids=position_ids,\n",
    "                    inference_params=inference_params,\n",
    "                    last_token_only=last_token_only).logits\n",
    "\n",
    "            if labels is not None:\n",
    "                # move labels to correct device to enable model parallelism\n",
    "                labels = labels.to(lm_logits.device)\n",
    "                # Shift so that tokens < n predict n\n",
    "                shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "                shift_labels = labels[:, 1:].contiguous()\n",
    "                # Flatten the tokens\n",
    "                loss = self.loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            else:\n",
    "                loss = None\n",
    "\n",
    "            if not return_dict:\n",
    "                output = (lm_logits, None, None, None, domain_ids, None, None, None) \n",
    "                return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "            return CausalLMOutputWithDomainIDs(\n",
    "                loss=loss,\n",
    "                logits=lm_logits,\n",
    "                past_key_values=None,\n",
    "                hidden_states=None,\n",
    "                attentions=None,\n",
    "                domain_ids=domain_ids)\n",
    "        else:\n",
    "            lm_logits = super().forward(\n",
    "                    input_ids=input_ids,\n",
    "                    position_ids=position_ids,\n",
    "                    inference_params=inference_params,\n",
    "                    last_token_only=last_token_only).logits\n",
    "\n",
    "            loss = None\n",
    "            pertoken_loss = None\n",
    "            reference_pertoken_loss = None\n",
    "            if labels is not None:\n",
    "                # move labels to correct device to enable model parallelism\n",
    "                labels = labels.to(lm_logits.device)\n",
    "                # Shift so that tokens < n predict n\n",
    "                shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "                shift_labels = labels[:, 1:].contiguous()\n",
    "                # Flatten the tokens\n",
    "                ignore_index = -100\n",
    "                pertoken_loss = self.pertoken_loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                pertoken_loss = pertoken_loss.view(shift_labels.size(0), shift_labels.size(1))\n",
    "                token_mask = shift_labels.ne(ignore_index).float()\n",
    "\n",
    "                loss = pertoken_loss.sum() / token_mask.sum()\n",
    "\n",
    "                # run reference model forward to get pertoken_loss\n",
    "                if self.reference_model is not None:\n",
    "                    self.reference_model.eval()\n",
    "                    reference_outputs = self.reference_model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        inputs_embeds=inputs_embeds,\n",
    "                        head_mask=head_mask,\n",
    "                        past_key_values=past_key_values,\n",
    "                        labels=labels,\n",
    "                        use_cache=use_cache,\n",
    "                        output_attentions=output_attentions,\n",
    "                        output_hidden_states=output_hidden_states,\n",
    "                        return_dict=return_dict,\n",
    "                        domain_ids=domain_ids,\n",
    "                        return_pertoken_losses=True,\n",
    "                        position_ids=position_ids,\n",
    "                        inference_params=inference_params,\n",
    "                        last_token_only=last_token_only,\n",
    "                    )\n",
    "                    reference_pertoken_loss = reference_outputs['pertoken_loss']\n",
    "\n",
    "            if not return_dict:\n",
    "                output = (lm_logits, None, None, None, domain_ids, pertoken_loss, reference_pertoken_loss, token_mask) \n",
    "                return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "            return CausalLMOutputWithDomainIDs(\n",
    "                loss=loss,\n",
    "                logits=lm_logits,\n",
    "                past_key_values=None,\n",
    "                hidden_states=None,\n",
    "                attentions=None,\n",
    "                domain_ids=domain_ids,\n",
    "                pertoken_loss=pertoken_loss,\n",
    "                reference_pertoken_loss=reference_pertoken_loss,\n",
    "                token_mask=token_mask)\n",
    "        \n",
    "        \n",
    "\n",
    "#             model_args.model_name_or_path,\n",
    "#             from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "#             config=config,\n",
    "#             cache_dir=model_args.cache_dir,\n",
    "#             revision=model_args.model_revision,\n",
    "#             use_auth_token=True if model_args.use_auth_token else None,\n",
    "#             torch_dtype=torch_dtype,\n",
    "\n",
    "model = doremi_models.GPTFlashAttnLMHeadModel.from_pretrained(\n",
    "    model_args.model_name_or_path, config=config)\n",
    "\n",
    "        \n",
    "GPTLMHeadModelDoReMi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c00f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32dd1e74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['cot', 'dolly', 'flan_v2', 'oasst1'],\n",
       " 4,\n",
       " {'cot': 0.5, 'flan_v2': 0.25, 'dolly': 0.12, 'oasst1': 0.13})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(training_args.domain_config_path, 'r') as f:\n",
    "    domain_config = json.load(f)\n",
    "\n",
    "train_domain_weights_dict = domain_config['train_domain_weights']\n",
    "eval_domain_weights_dict = domain_config['eval_domain_weights']\n",
    "# whenever we convert dict to array, we sort by key\n",
    "domain_list = list(sorted(train_domain_weights_dict.keys()))\n",
    "num_domains = len(domain_list)\n",
    "\n",
    "domain_list, num_domains, train_domain_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bf74f1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 23:14:21 - WARNING - doremi.dataloader - No split used or split directory not found: using same data for all splits.\n",
      "08/01/2023 23:14:21 - INFO - datasets.builder - Using custom data configuration default-20e8d8f5df875937\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/01/2023 23:14:21 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/01/2023 23:14:21 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a616428e2e6c4d9d9d58147517c4b437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dac4998a00f3727e_00000_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dac4998a00f3727e_00001_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dac4998a00f3727e_00002_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dac4998a00f3727e_00003_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dac4998a00f3727e_00004_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dac4998a00f3727e_00005_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dac4998a00f3727e_00006_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dac4998a00f3727e_00007_of_00008.arrow\n",
      "08/01/2023 23:14:21 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dac4998a00f3727e_*_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "08/01/2023 23:14:21 - INFO - datasets.builder - Using custom data configuration default-26e8261e5eec66e6\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/01/2023 23:14:21 - INFO - datasets.builder - Using custom data configuration default-68fd04897f8e942c\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/01/2023 23:14:21 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/01/2023 23:14:21 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9b14d8daea4482ae2a8ce58420a59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee2bcd7dac9daed9_00000_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee2bcd7dac9daed9_00001_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee2bcd7dac9daed9_00002_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee2bcd7dac9daed9_00003_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee2bcd7dac9daed9_00004_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee2bcd7dac9daed9_00005_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee2bcd7dac9daed9_00006_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee2bcd7dac9daed9_00007_of_00008.arrow\n",
      "08/01/2023 23:14:21 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee2bcd7dac9daed9_*_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "08/01/2023 23:14:21 - INFO - datasets.builder - Using custom data configuration default-8aa3f6a117085801\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/01/2023 23:14:21 - INFO - datasets.builder - Using custom data configuration default-a01381664fd2589b\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/01/2023 23:14:21 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/01/2023 23:14:21 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cf6c6cedf140e9b864601d44008d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dd3397b5949d01cc_00000_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dd3397b5949d01cc_00001_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dd3397b5949d01cc_00002_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dd3397b5949d01cc_00003_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dd3397b5949d01cc_00004_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dd3397b5949d01cc_00005_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dd3397b5949d01cc_00006_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dd3397b5949d01cc_00007_of_00008.arrow\n",
      "08/01/2023 23:14:21 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dd3397b5949d01cc_*_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "08/01/2023 23:14:21 - INFO - datasets.builder - Using custom data configuration default-3633299fdc416400\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/01/2023 23:14:21 - INFO - datasets.builder - Using custom data configuration default-d43591dedd2996a1\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/01/2023 23:14:21 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/01/2023 23:14:21 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324380333a3641f38351d2f1a1bb0b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9d43238af1e5752b_00000_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9d43238af1e5752b_00001_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9d43238af1e5752b_00002_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9d43238af1e5752b_00003_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9d43238af1e5752b_00004_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9d43238af1e5752b_00005_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9d43238af1e5752b_00006_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9d43238af1e5752b_00007_of_00008.arrow\n",
      "08/01/2023 23:14:21 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9d43238af1e5752b_*_of_00008.arrow\n",
      "08/01/2023 23:14:21 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "08/01/2023 23:14:21 - INFO - datasets.builder - Using custom data configuration default-550bfb9fe5e17787\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR|tokenization_utils_base.py:1056] 2023-08-01 23:14:21,926 >> Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 23:14:21 - INFO - datasets.builder - Using custom data configuration default-2cb43153e98ab920\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/01/2023 23:14:21 - INFO - datasets.builder - Using custom data configuration default-ecd9b34c777a15d8\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/01/2023 23:14:21 - INFO - datasets.builder - Using custom data configuration default-4984ce55f649fc18\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/01/2023 23:14:21 - INFO - datasets.builder - Using custom data configuration default-7f33c15e96487ac8\n",
      "08/01/2023 23:14:21 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/01/2023 23:14:22 - INFO - datasets.builder - Using custom data configuration default-705a17429105e7ef\n",
      "08/01/2023 23:14:22 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if training_args.do_train:\n",
    "    # data script could change tokenizer shape\n",
    "    train_dataset = data_utils.get_preprocessed_mixed_dataset(\n",
    "            preprocessed_dir=data_args.dataset_dir,\n",
    "            domain_weights_dict=train_domain_weights_dict,\n",
    "            dataset_name=data_args.dataset_name,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            split='train',\n",
    "            max_samples=data_args.max_train_samples,\n",
    "            add_domain_id=data_args.add_domain_id,\n",
    "            tmp_file=None,\n",
    "            seed=training_args.seed,\n",
    "            tokenizer=tokenizer,\n",
    "            shuffle=data_args.shuffle,\n",
    "            num_skip_examples=num_skip_examples,\n",
    "            shard_reversal=training_args.reweight_domains,\n",
    "            training_args=training_args,\n",
    "    )\n",
    "\n",
    "if training_args.do_eval:\n",
    "    eval_dataset = data_utils.get_preprocessed_mixed_dataset(\n",
    "            preprocessed_dir=data_args.dataset_dir,\n",
    "            domain_weights_dict=eval_domain_weights_dict,\n",
    "            dataset_name=data_args.dataset_name,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            split='validation',\n",
    "            add_domain_id=data_args.add_domain_id,\n",
    "            max_samples=data_args.max_eval_samples,\n",
    "            tokenizer=tokenizer,\n",
    "            no_interleave=True,\n",
    "            training_args=training_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23f4e911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# preprocessed_dir=data_args.dataset_dir\n",
    "# domain_weights_dict=train_domain_weights_dict\n",
    "# dataset_name=data_args.dataset_name\n",
    "# cache_dir=model_args.cache_dir\n",
    "# split='train'\n",
    "# max_samples=data_args.max_train_samples\n",
    "# add_domain_id=data_args.add_domain_id\n",
    "# tmp_file=None\n",
    "# seed=training_args.seed\n",
    "# tokenizer=tokenizer\n",
    "# shuffle=data_args.shuffle\n",
    "# num_skip_examples=num_skip_examples\n",
    "# shard_reversal=training_args.reweight_domains\n",
    "# no_interleave=False\n",
    "\n",
    "# print(preprocessed_dir)\n",
    "# print(domain_weights_dict)\n",
    "# print(dataset_name)\n",
    "# print(cache_dir)\n",
    "# print(split)\n",
    "# print(max_samples)\n",
    "# print(add_domain_id)\n",
    "# print(seed)\n",
    "# print(shuffle)\n",
    "# print(num_skip_examples)\n",
    "# print(shard_reversal)\n",
    "\n",
    "\n",
    "# domain_names = list(sorted(domain_weights_dict.keys()))\n",
    "# domain_to_idx = {domain_names[i]: i for i in range(len(domain_names))}\n",
    "# domain_weights = np.asarray([domain_weights_dict[domain_name] for domain_name in domain_names])\n",
    "# domain_weights = domain_weights / domain_weights.sum()\n",
    "\n",
    "# print()\n",
    "# print(json.dumps({'domain_names': domain_names, \n",
    "#                   'domain_to_idx': domain_to_idx, \n",
    "#                   'domain_weights': list(domain_weights)},\n",
    "#                 indent=4))\n",
    "\n",
    "\n",
    "# # write domain weights to file if tmp_file is set\n",
    "# if tmp_file is not None:\n",
    "#     probabilities_tmp_file = tmp_file\n",
    "\n",
    "#     with open(str(probabilities_tmp_file), 'wb') as f:\n",
    "#         pickle.dump(domain_weights, f)\n",
    "#     probabilities = None\n",
    "# else:\n",
    "#     probabilities = domain_weights\n",
    "#     probabilities_tmp_file = None\n",
    "\n",
    "\n",
    "# print()\n",
    "# print(json.dumps({'probabilities': list(probabilities)}, indent=4))\n",
    "\n",
    "# # from doremi.dataloader import get_perdomain_datasets\n",
    "# # all_ds = get_perdomain_datasets(\n",
    "# #     preprocessed_dir, \n",
    "# #     domain_weights_dict,\n",
    "# #     cache_dir=cache_dir,\n",
    "# #     split=split,\n",
    "# #     seed=seed,\n",
    "# #     domain_weights=domain_weights,\n",
    "# #     domain_names=domain_names,\n",
    "# #     num_skip_examples=num_skip_examples,\n",
    "# #     shuffle=shuffle,\n",
    "# #     shard_reversal=shard_reversal\n",
    "# # )\n",
    "\n",
    "# domain_name_to_skip_num = determine_skip_per_domain(num_skip_examples, seed, domain_weights, domain_names)\n",
    "\n",
    "# preprocessed_dir = Path(preprocessed_dir)\n",
    "# if split is not None and (preprocessed_dir / split).exists():\n",
    "#     preprocessed_dir = preprocessed_dir / split\n",
    "# else:\n",
    "#     logger.warn(f\"No split used or split directory not found: using same data for all splits.\")\n",
    "\n",
    "# domains = list(sorted(domain_weights_dict.keys()))\n",
    "\n",
    "# print(preprocessed_dir)\n",
    "# print(domain_name_to_skip_num)\n",
    "# print()\n",
    "# print(json.dumps({'preprocessed_dir': str(preprocessed_dir), \n",
    "#                   'domain_name_to_skip_num': domain_name_to_skip_num}, indent=4))\n",
    "\n",
    "\n",
    "# all_ds = {}\n",
    "# for domain in domains:\n",
    "#     domain_dir = preprocessed_dir / domain\n",
    "    \n",
    "#     ## wpq: read instruction tuning dataset off `jsonl` files\n",
    "#     if (domain_dir / f'{domain}_data.jsonl').exists():\n",
    "#         from datasets import load_dataset\n",
    "#         from functools import partial\n",
    "#         from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n",
    "#         from doremi.dataloader import skippable_data_gen_dataset\n",
    "\n",
    "#         data_files = {'train': str(domain_dir / f'{domain}_data.jsonl')}\n",
    "#         raw_datasets = load_dataset(\n",
    "#             \"json\",\n",
    "#             data_files=data_files,\n",
    "#             cache_dir=cache_dir,\n",
    "#             use_auth_token=True if model_args.use_auth_token else None,\n",
    "#         )\n",
    "#         # Preprocessing the datasets.\n",
    "#         if \"prompt\" in raw_datasets[\"train\"].column_names and \"completion\" in raw_datasets[\"train\"].column_names:\n",
    "#             encode_function = partial(\n",
    "#                 encode_with_prompt_completion_format,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 max_seq_length=1024,\n",
    "#             )\n",
    "#         elif \"messages\" in raw_datasets[\"train\"].column_names:\n",
    "#             encode_function = partial(\n",
    "#                 encode_with_messages_format,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 max_seq_length=1024,\n",
    "#             )\n",
    "#         else:\n",
    "#             raise ValueError(\"You need to have either 'prompt'&'completion' or 'messages' in your column names.\")\n",
    "\n",
    "#         with training_args.main_process_first(local=False, desc=\"Processing instruction data\"):\n",
    "#             lm_datasets = raw_datasets.map(\n",
    "#                 encode_function,\n",
    "#                 num_proc=16,\n",
    "#                 batched=False,\n",
    "#             )\n",
    "#             lm_datasets.set_format(type=\"pt\")\n",
    "#         ds = lm_datasets['train']\n",
    "#         ds = IterableDataset.from_generator(\n",
    "#                 skippable_data_gen_dataset,\n",
    "#                 gen_kwargs={'ds': ds,\n",
    "#                             'num_skip_examples': domain_name_to_skip_num[domain],\n",
    "#                             'loop': (split == 'train'),\n",
    "#                             'seed': seed,\n",
    "#                             'shuffle': shuffle}\n",
    "#                 )\n",
    "#         seed += 1\n",
    "#     elif (domain_dir / 'dataset_info.json').exists():\n",
    "#         ds = load_from_disk(dataset_path=str(domain_dir))\n",
    "#         logger.info(f\"Loaded {domain_dir}. Length: {len(ds)}\")\n",
    "#     else:\n",
    "#         curr_shards = list(domain_dir.iterdir())\n",
    "#         if shard_reversal:\n",
    "#             curr_shards = list(reversed(curr_shards))\n",
    "#         # shuffle shard order\n",
    "#         random.Random(seed).shuffle(curr_shards)\n",
    "#         ds = IterableDataset.from_generator(\n",
    "#                 skippable_data_gen,\n",
    "#                 gen_kwargs={'shards': curr_shards,\n",
    "#                             'num_skip_examples': domain_name_to_skip_num[domain],\n",
    "#                             'loop': (split == 'train'),\n",
    "#                             'seed': seed,\n",
    "#                             'shuffle': shuffle}\n",
    "#                 )\n",
    "#         seed += 1\n",
    "#     all_ds[domain] = ds\n",
    "    \n",
    "\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# def add_domain_id_generator(ds, domain_idx):\n",
    "#     for ex in ds:\n",
    "#         ex['domain_id'] = domain_idx\n",
    "#         yield ex\n",
    "        \n",
    "# domain_ds_ls = []\n",
    "# for domain_name in domain_names:\n",
    "#     domain_idx = domain_to_idx[domain_name]\n",
    "#     domain_ds = all_ds[domain_name]\n",
    "#     # add domain_id if necessary\n",
    "#     if add_domain_id:\n",
    "#         domain_ds = IterableDataset.from_generator(\n",
    "#             add_domain_id_generator, \n",
    "#             gen_kwargs={'ds': domain_ds, 'domain_idx': domain_idx})\n",
    "#     domain_ds_ls.append(domain_ds)\n",
    "\n",
    "# if no_interleave:\n",
    "#     # instead of interleaving, run through each dataset\n",
    "#     def data_generator(shards):\n",
    "#         for shard in shards:\n",
    "#             for ex in shard:\n",
    "#                 yield ex\n",
    "#     ds = IterableDataset.from_generator(data_generator, gen_kwargs={'shards': domain_ds_ls})\n",
    "#     logger.info(\"Not interleaving dataset - will not sample according to domain weights\")\n",
    "\n",
    "# else:\n",
    "#     ds = interleave_datasets(\n",
    "#             domain_ds_ls,\n",
    "#             probabilities=probabilities,\n",
    "#             probabilities_file=probabilities_tmp_file,\n",
    "#             seed=seed)\n",
    "    \n",
    "\n",
    "# def take_data_generator(ds, max_samples):\n",
    "#     idx = 0\n",
    "#     for ex in ds:\n",
    "#         yield ex\n",
    "#         idx += 1\n",
    "#         if max_samples is not None and idx >= max_samples:\n",
    "#             return\n",
    "\n",
    "# ds = IterableDataset.from_generator(take_data_generator, gen_kwargs={'ds': ds, 'max_samples': max_samples})\n",
    "# train_dataset = ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5426031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ds = load_dataset(\n",
    "#     \"json\",\n",
    "#     data_files='test.jsonl',\n",
    "#     cache_dir=model_args.cache_dir)['train']\n",
    "# test_ds[0]\n",
    "\n",
    "# for x in test_ds.to_iterable_dataset():\n",
    "#     print(x)\n",
    "# for i, v in enumerate(ds):\n",
    "#     if i == 10:\n",
    "#         break\n",
    "#     print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3789d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:565] 2023-08-01 23:14:23,845 >> max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# turn off find unused parameters\n",
    "training_args.ddp_find_unused_parameters = False\n",
    "\n",
    "# We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "# on a small vocab and want a smaller embedding size, remove this test.\n",
    "# embedding_size = model.get_input_embeddings.weight.shape[0]\n",
    "# if len(tokenizer) > embedding_size:\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = DoReMiTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_utils.get_data_collator(tokenizer, do_padding=data_args.do_padding),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29cbb17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 None auto False\n"
     ]
    }
   ],
   "source": [
    "print(trainer.args.max_grad_norm, \\\n",
    "    trainer.sharded_ddp, \\\n",
    "    trainer.args.half_precision_backend, \\\n",
    "    trainer.do_grad_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38a27bcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1682] 2023-08-01 23:14:24,401 >> ***** Running training *****\n",
      "[INFO|trainer.py:1683] 2023-08-01 23:14:24,401 >>   Num examples = 25,600,000\n",
      "[INFO|trainer.py:1684] 2023-08-01 23:14:24,402 >>   Num Epochs = 9,223,372,036,854,775,807\n",
      "[INFO|trainer.py:1685] 2023-08-01 23:14:24,403 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1688] 2023-08-01 23:14:24,404 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "[INFO|trainer.py:1689] 2023-08-01 23:14:24,404 >>   Gradient Accumulation steps = 32\n",
      "[INFO|trainer.py:1690] 2023-08-01 23:14:24,405 >>   Total optimization steps = 200,000\n",
      "[INFO|trainer.py:1691] 2023-08-01 23:14:24,406 >>   Number of trainable parameters = 124,439,808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 23:14:24 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9fd20ed78de90eca.arrow\n",
      "08/01/2023 23:14:24 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9659265e11252187.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer_utils.py:696] 2023-08-01 23:14:24,482 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: id, dataset, messages, domain_id. If id, dataset, messages, domain_id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "[WARNING|logging.py:280] 2023-08-01 23:14:24,483 >> You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 23:14:24 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-56427ff2ded4e54d.arrow\n",
      "08/01/2023 23:14:24 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e41ab1d4274cf168.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='200000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    14/200000 01:26 < 401:44:06, 0.14 it/s, Epoch 1.00/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.992700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10.994900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2807] 2023-08-01 23:15:00,299 >> Saving model checkpoint to /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/checkpoint-5\n",
      "[INFO|configuration_utils.py:458] 2023-08-01 23:15:00,303 >> Configuration saved in /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/checkpoint-5/config.json\n",
      "[INFO|configuration_utils.py:379] 2023-08-01 23:15:00,304 >> Configuration saved in /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/checkpoint-5/generation_config.json\n",
      "[INFO|modeling_utils.py:1855] 2023-08-01 23:15:00,760 >> Model weights saved in /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/checkpoint-5/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2210] 2023-08-01 23:15:00,763 >> tokenizer config file saved in /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/checkpoint-5/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2217] 2023-08-01 23:15:00,765 >> Special tokens file saved in /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/checkpoint-5/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 23:15:20 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9fd20ed78de90eca.arrow\n",
      "08/01/2023 23:15:20 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9659265e11252187.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer_utils.py:696] 2023-08-01 23:15:20,848 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: id, dataset, messages, domain_id. If id, dataset, messages, domain_id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "[WARNING|logging.py:280] 2023-08-01 23:15:20,849 >> You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 23:15:20 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-56427ff2ded4e54d.arrow\n",
      "08/01/2023 23:15:20 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e41ab1d4274cf168.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2807] 2023-08-01 23:15:36,394 >> Saving model checkpoint to /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/checkpoint-10\n",
      "[INFO|configuration_utils.py:458] 2023-08-01 23:15:36,397 >> Configuration saved in /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/checkpoint-10/config.json\n",
      "[INFO|configuration_utils.py:379] 2023-08-01 23:15:36,399 >> Configuration saved in /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/checkpoint-10/generation_config.json\n",
      "[INFO|modeling_utils.py:1855] 2023-08-01 23:15:36,865 >> Model weights saved in /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/checkpoint-10/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2210] 2023-08-01 23:15:36,868 >> tokenizer config file saved in /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/checkpoint-10/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2217] 2023-08-01 23:15:36,869 >> Special tokens file saved in /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/checkpoint-10/special_tokens_map.json\n",
      "[INFO|trainer.py:2894] 2023-08-01 23:15:37,869 >> Deleting older checkpoint [/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/checkpoint-5] due to args.save_total_limit\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "\n",
    "    if training_args.reweight_domains:\n",
    "        avg_domain_weights_dict = {}\n",
    "        for i in range(len(model.avg_domain_weights)):\n",
    "            domain_name = domain_list[i]\n",
    "            metrics[f'avg_domain_weight:{domain_name}'] = model.avg_domain_weights[i].item()\n",
    "            avg_domain_weights_dict[domain_name] = model.avg_domain_weights[i].item()\n",
    "\n",
    "        # save avg domain weights to json\n",
    "        avg_domain_weights_file = Path(training_args.output_dir) / 'avg_domain_weights.json'\n",
    "        with open(avg_domain_weights_file, 'w') as f:\n",
    "            json.dump(avg_domain_weights_dict, f, indent=2)\n",
    "\n",
    "        # also save to configs dir\n",
    "        config_dict = {\"train_domain_weights\": avg_domain_weights_dict,\n",
    "                       \"eval_domain_weights\": avg_domain_weights_dict}\n",
    "        config_dict_file = Path(__file__).parent.parent / 'configs' / f\"{Path(training_args.output_dir).name}.json\"\n",
    "        with open(config_dict_file, 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "256b7ca6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT2LMHeadModel' object has no attribute 'perdomain_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperdomain_scores\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2LMHeadModel' object has no attribute 'perdomain_scores'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584e6abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
