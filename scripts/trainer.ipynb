{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156e90ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_setup; jpt_setup()\n",
    "import os; os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb018c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-01 14:56:14,094] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import logging\n",
    "import math\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import chain\n",
    "from typing import Optional\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import pyarrow\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset, IterableDataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.trainer_callback import TrainerState\n",
    "from transformers.trainer import TRAINER_STATE_NAME\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "\n",
    "from doremi.training_args import ModelArguments, DataTrainingArguments, FullTrainingArguments\n",
    "import doremi.dataloader as data_utils\n",
    "from doremi.trainer import DoReMiTrainer\n",
    "try:\n",
    "    import doremi.models as doremi_models\n",
    "except Exception:\n",
    "    \n",
    "    pass\n",
    "try:\n",
    "    from flash_attn.models.gpt_neox import gpt_neox_config_to_gpt2_config\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.27.0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/language-modeling/requirements.txt\")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43447b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_dir = \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi\"\n",
    "cache_dir = '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache'\n",
    "preprocessed_data = \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed\"\n",
    "\n",
    "envs = {\n",
    "    \"CACHE\": cache_dir,\n",
    "    \"DOREMI_DIR\": package_dir,\n",
    "    \"PILE_DIR\": os.path.join(package_dir, \"data\", 'raw'),\n",
    "    \"PREPROCESSED_PILE_DIR\": preprocessed_data,\n",
    "    \"MODEL_OUTPUT_DIR\": os.path.join(package_dir, 'results'),\n",
    "    \"PARTITION\": \"el8\",\n",
    "    \"HF_HOME\": cache_dir,\n",
    "    \"TRANSFORMERS_CACHE\": cache_dir,\n",
    "    \"HF_DATASETS_CACHE\": cache_dir,\n",
    "    \"HF_DATASETS_IN_MEMORY_MAX_SIZE\": \"0\",\n",
    "    \"TORCH_EXTENSIONS_DIR\": cache_dir,\n",
    "    \"TMPDIR\": cache_dir,\n",
    "    \"WANDB_DIR\": os.path.join(cache_dir, \"wandb\"),\n",
    "    \"PREPROCESSED_DATA\": preprocessed_data,\n",
    "    'PREPROCESSED_CACHE': os.path.join(cache_dir, 'preprocessed_cache', 'perdomain_pile_preprocessed'),\n",
    "\n",
    "}\n",
    "\n",
    "for k, v in envs.items():\n",
    "    os.environ[k] = v\n",
    "    \n",
    "os.makedirs(cache_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07cce8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CACHE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache;export DOREMI_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi;export PILE_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/raw;export PREPROCESSED_PILE_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed;export MODEL_OUTPUT_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results;export PARTITION=el8;export HF_HOME=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache;export TRANSFORMERS_CACHE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache;export HF_DATASETS_CACHE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache;export HF_DATASETS_IN_MEMORY_MAX_SIZE=0;export TORCH_EXTENSIONS_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache;export TMPDIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache;export WANDB_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/wandb;export PREPROCESSED_DATA=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed;export PREPROCESSED_CACHE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/preprocessed_cache/perdomain_pile_preprocessed\n"
     ]
    }
   ],
   "source": [
    "print(';'.join([f'export {k}={v}' for k, v in envs.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef8ee5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate domain weight config\n",
    "import json\n",
    "domain_config_path = '../configs/humanmix_baseline_50kvocab.json'\n",
    "domain_weights = {\"cot\": .25, \"flan_v2\": .25, \"dolly\": .25, \"oasst1\": .25}\n",
    "domain_config = {\"train_domain_weights\": domain_weights, \"eval_domain_weights\": domain_weights}\n",
    "with open(domain_config_path, 'w') as f:\n",
    "    json.dump(domain_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbc7aa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/data/processed/cot True\n",
      "/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/data/processed/flan_v2 True\n",
      "/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/data/processed/dolly True\n",
      "/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/data/processed/oasst1 True\n"
     ]
    }
   ],
   "source": [
    "# # move domain datasets to the right folder.\n",
    "# src_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/data/processed'\n",
    "# tgt_dir = preprocessed_data\n",
    "\n",
    "# def copyanything(src, dst):\n",
    "#     import shutil, errno\n",
    "#     try:\n",
    "#         shutil.copytree(src, dst)\n",
    "#     except OSError as exc: # python >2.5\n",
    "#         if exc.errno in (errno.ENOTDIR, errno.EINVAL):\n",
    "#             shutil.copy(src, dst)\n",
    "#         else: raise\n",
    "\n",
    "# for domain in  domain_weights.keys():\n",
    "#     src_domain_dir = os.path.join(src_dir, domain)\n",
    "#     tgt_domain_dir = os.path.join(tgt_dir, domain)\n",
    "#     print(src_domain_dir, os.path.isdir(src_domain_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "142128f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1301] 2023-08-01 17:24:18,841 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "[INFO|training_args.py:1716] 2023-08-01 17:24:18,842 >> PyTorch: setting up devices\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ModelArguments(model_name_or_path=None, model_type='gpt2', config_overrides='n_positions=1024,n_embd=1024,n_layer=18,n_head=16', config_name=None, tokenizer_name='gpt2', cache_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache', use_fast_tokenizer=True, model_revision='main', use_auth_token=False, torch_dtype=None),\n",
       " DataTrainingArguments(dataset_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed', dataset_name='pile', max_train_samples=None, max_eval_samples=None, max_token_length=1024, block_size=None, overwrite_cache=False, do_padding=True, add_domain_id=True, preprocessing_num_workers=None, shuffle=True),\n",
       " FullTrainingArguments(output_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.99, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=200000, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/runs/Aug01_17-24-18_dcs002', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=True, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=10000, save_total_limit=1, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=1111, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=True, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='train_baseline', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_HF: 'adamw_hf'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, domain_config_path='../configs/humanmix_baseline_50kvocab.json', lr_end=0.001, reweight_domains=False, reweight_eta=1.0, reweight_eps=0.0001, doremi_optimizer='doremiv1', reference_model_name_or_path='.', lr_scheduler_name=None, train_domain_weights_tmp_file=None))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_name = 'train_baseline'\n",
    "\n",
    "nodes = 1\n",
    "num_gpus = 1\n",
    "\n",
    "model_name_or_path = 'gpt2'\n",
    "cache_dir = envs['CACHE']\n",
    "domain_config_path = '../configs/humanmix_baseline_50kvocab.json'\n",
    "output_dir = os.path.join(envs['MODEL_OUTPUT_DIR'], job_name)\n",
    "dataset_dir = envs['PREPROCESSED_CACHE']\n",
    "dataset_dir = preprocessed_data\n",
    " \n",
    "total_batch_size = 2 # # 64*8=512\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_accumulation_steps = int(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "\n",
    "max_steps = 200000; save_steps = 10000\n",
    "\n",
    "# --learning_rate 1e-3 \\\n",
    "# --lr_end 1e-4 \\\n",
    "# --weight_decay 0.01 \\\n",
    "# --max_grad_norm 1.0 \\\n",
    "# --adam_epsilon 1e-8 \\\n",
    "# --lr_scheduler_name linear_warmup_cosine \\\n",
    "# --warmup_ratio 0.06 \\\n",
    "\n",
    "cmd = f\"\"\"\n",
    "--dataset_name=pile \\\n",
    "--model_type={model_name_or_path} \\\n",
    "--tokenizer_name=gpt2 \\\n",
    "--do_train \\\n",
    "--cache_dir={cache_dir} \\\n",
    "--dataset_dir={dataset_dir} \\\n",
    "--domain_config_path={domain_config_path} \\\n",
    "--output_dir={output_dir} \\\n",
    "--max_token_length=1024 \\\n",
    "--per_device_train_batch_size={per_device_train_batch_size} \\\n",
    "--gradient_accumulation_steps={gradient_accumulation_steps} \\\n",
    "--dataloader_num_workers=1 \\\n",
    "--learning_rate=2e-5 \\\n",
    "--lr_scheduler_type=linear \\\n",
    "--warmup_ratio=0.03 \\\n",
    "--weight_decay=0. \\\n",
    "--max_grad_norm=1.0 \\\n",
    "--max_steps={max_steps} \\\n",
    "--evaluation_strategy=no \\\n",
    "--save_strategy=steps \\\n",
    "--save_steps={save_steps} \\\n",
    "--save_total_limit=1 \\\n",
    "--run_name={job_name} \\\n",
    "--seed=1111 \\\n",
    "--logging_strategy=steps \\\n",
    "--logging_steps=10 \\\n",
    "--logging_first_step \\\n",
    "--report_to=tensorboard \\\n",
    "--optim=adamw_hf \\\n",
    "--adam_beta1=0.9 \\\n",
    "--adam_beta2=0.99 \\\n",
    "--add_domain_id True \\\n",
    "--do_padding True \\\n",
    "--fp16 \\\n",
    "--config_overrides=\"n_positions=1024,n_embd=1024,n_layer=18,n_head=16\" \\\n",
    "\"\"\"\n",
    "# add_domain_id: for non-pile preprocessed dataset\n",
    "# do_padding: true for variable size sequences, as in instruction tuning datasets.\n",
    "\n",
    "\n",
    "import shlex\n",
    "args = shlex.split(cmd)\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, FullTrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n",
    "model_args, data_args, training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "644c28f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 17:24:19 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "08/01/2023 17:24:19 - INFO - __main__ - Training/evaluation parameters FullTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.99,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=1,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "domain_config_path=../configs/humanmix_baseline_50kvocab.json,\n",
      "doremi_optimizer=doremiv1,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/runs/Aug01_17-24-18_dcs002,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_end=0.001,\n",
      "lr_scheduler_name=None,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=200000,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "reference_model_name_or_path=.,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "reweight_domains=False,\n",
      "reweight_eps=0.0001,\n",
      "reweight_eta=1.0,\n",
      "run_name=train_baseline,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=10000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=1111,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_domain_weights_tmp_file=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "if training_args.should_log:\n",
    "    # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "    \n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f9a048da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Output directory (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline) already exists and is not empty. Use --overwrite_output_dir to overcome.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[205], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m last_checkpoint \u001b[38;5;241m=\u001b[39m get_last_checkpoint(training_args\u001b[38;5;241m.\u001b[39moutput_dir)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m last_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(training_args\u001b[38;5;241m.\u001b[39moutput_dir)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput directory (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_args\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) already exists and is not empty. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse --overwrite_output_dir to overcome.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m last_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m training_args\u001b[38;5;241m.\u001b[39mresume_from_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint detected, resuming training at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_checkpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. To avoid this behavior, change \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe `--output_dir` or add `--overwrite_output_dir` to train from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Output directory (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline) already exists and is not empty. Use --overwrite_output_dir to overcome."
     ]
    }
   ],
   "source": [
    "\n",
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "num_skip_examples = 0\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n",
    "        state = TrainerState.load_from_json(str(Path(last_checkpoint) / TRAINER_STATE_NAME))\n",
    "        global_batch_size = training_args.train_batch_size * training_args.gradient_accumulation_steps * training_args.world_size\n",
    "        num_skip_examples = state.global_step * global_batch_size\n",
    "        logger.info(f\"Skipping {num_skip_examples} examples\")\n",
    "        \n",
    "last_checkpoint, num_skip_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a00590",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "\n",
    "config_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "}\n",
    "if model_args.config_name:\n",
    "    config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
    "elif model_args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "    if model_args.model_type == 'gpt_neox_flash':\n",
    "        config = gpt_neox_config_to_gpt2_config(config)\n",
    "        config.use_flash_attn = True\n",
    "        config.fused_mlp = True\n",
    "        config.fused_bias_fc = True\n",
    "        config.fused_dropout_add_ln = True\n",
    "        config.pad_vocab_size_multiple = 8\n",
    "        config.activation_function = 'gelu_new'\n",
    "        config.n_inner = None\n",
    "        # disable absolute\n",
    "        config.max_position_embeddings = 0\n",
    "else:\n",
    "    if model_args.model_type == 'gpt_flash': \n",
    "        config = GPT2Config(\n",
    "                vocab_size=50257, n_positions=2048, n_embd=2048,\n",
    "                n_layer=24, n_head=16, \n",
    "                scale_attn_by_inverse_layer_idx=True, \n",
    "                rotary_emb_fraction=0.5,\n",
    "                use_flash_attn=True, fused_mlp=True,\n",
    "                fused_bias_fc=True, fused_dropout_add_ln=True, \n",
    "                pad_vocab_size_multiple=8)\n",
    "        # disable absolute\n",
    "        config.max_position_embeddings = 0\n",
    "    elif model_args.model_type == 'gpt_neox_flash':\n",
    "        # convert to GPT2 config\n",
    "        config = CONFIG_MAPPING['gpt_neox']() \n",
    "        config = gpt_neox_config_to_gpt2_config(config)\n",
    "        config.use_flash_attn = True\n",
    "        config.fused_mlp = True\n",
    "        config.fused_bias_fc = True\n",
    "        config.fused_dropout_add_ln = True\n",
    "        config.pad_vocab_size_multiple = 8\n",
    "        config.activation_function = 'gelu_new'\n",
    "        config.n_inner = None\n",
    "        # disable absolute\n",
    "        config.max_position_embeddings = 0\n",
    "    else:\n",
    "        config = CONFIG_MAPPING[model_args.model_type]()\n",
    "    logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "    if model_args.config_overrides is not None:\n",
    "        logger.info(f\"Overriding config: {model_args.config_overrides}\")\n",
    "        config.update_from_string(model_args.config_overrides)\n",
    "        logger.info(f\"New config: {config}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1711ed3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"use_fast\": model_args.use_fast_tokenizer,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "}\n",
    "\n",
    "if model_args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
    "\n",
    "elif model_args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )\n",
    "    \n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if model_args.model_name_or_path:\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "    if model_args.model_type in {'gpt_flash', 'gpt_neox_flash'}:\n",
    "        model = doremi_models.GPTFlashAttnLMHeadModel.from_pretrained(model_args.model_name_or_path, config=config)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "            torch_dtype=torch_dtype,\n",
    "        )\n",
    "else:\n",
    "    if model_args.model_type in {'gpt_flash', 'gpt_neox_flash'}:\n",
    "        model = doremi_models.GPTFlashAttnLMHeadModel(config)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "    n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())\n",
    "    logger.info(f\"Training new model from scratch - Total size={n_params/2**20:.2f}M params\")\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "cd4a024e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['cot', 'dolly', 'flan_v2', 'oasst1'],\n",
       " 4,\n",
       " {'cot': 0.5, 'flan_v2': 0.25, 'dolly': 0.12, 'oasst1': 0.13})"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(training_args.domain_config_path, 'r') as f:\n",
    "    domain_config = json.load(f)\n",
    "    \n",
    "domain_config['train_domain_weights'] =  {'cot': 0.5, 'flan_v2': 0.25, 'dolly': 0.12, 'oasst1': 0.13}\n",
    "\n",
    "\n",
    "train_domain_weights_dict = domain_config['train_domain_weights']\n",
    "eval_domain_weights_dict = domain_config['eval_domain_weights']\n",
    "# whenever we convert dict to array, we sort by key\n",
    "domain_list = list(sorted(train_domain_weights_dict.keys()))\n",
    "num_domains = len(domain_list)\n",
    "\n",
    "\n",
    "domain_list, num_domains, train_domain_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "0b283eee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[207], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_args\u001b[38;5;241m.\u001b[39mdo_train:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# data script could change tokenizer shape\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdata_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_preprocessed_mixed_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreprocessed_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdomain_weights_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_domain_weights_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_train_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43madd_domain_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_domain_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtmp_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_skip_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_skip_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_reversal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreweight_domains\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# if training_args.do_eval:\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     eval_dataset = data_utils.get_preprocessed_mixed_dataset(\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#             preprocessed_dir=data_args.dataset_dir,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#             tokenizer=tokenizer,\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#             no_interleave=True)\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/doremi/dataloader.py:349\u001b[0m, in \u001b[0;36mget_preprocessed_mixed_dataset\u001b[0;34m(preprocessed_dir, domain_weights_dict, dataset_name, cache_dir, split, seed, max_samples, add_domain_id, tmp_file, tokenizer, no_interleave, shuffle, num_skip_examples, shard_reversal)\u001b[0m\n\u001b[1;32m    346\u001b[0m     probabilities_tmp_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpile\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 349\u001b[0m     all_ds \u001b[38;5;241m=\u001b[39m \u001b[43mget_pile_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreprocessed_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m            \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdomain_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdomain_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdomain_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdomain_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_skip_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_skip_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_reversal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshard_reversal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/doremi/dataloader.py:228\u001b[0m, in \u001b[0;36mget_pile_datasets\u001b[0;34m(preprocessed_dir, cache_dir, split, seed, domain_weights, domain_names, num_skip_examples, shuffle, shard_reversal)\u001b[0m\n\u001b[1;32m    225\u001b[0m preprocessed_dir \u001b[38;5;241m=\u001b[39m Path(preprocessed_dir) \u001b[38;5;241m/\u001b[39m split\n\u001b[1;32m    227\u001b[0m all_ds \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m domain_dir \u001b[38;5;129;01min\u001b[39;00m preprocessed_dir\u001b[38;5;241m.\u001b[39miterdir():\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    230\u001b[0m         shards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(domain_dir\u001b[38;5;241m.\u001b[39miterdir())\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/pathlib.py:1017\u001b[0m, in \u001b[0;36mPath.iterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Iterate over the files in this directory.  Does not yield any\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;124;03m    result for the special paths '.' and '..'.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m}:\n\u001b[1;32m   1019\u001b[0m             \u001b[38;5;66;03m# Yielding a path object for these makes little sense\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed/train'"
     ]
    }
   ],
   "source": [
    "\n",
    "if training_args.do_train:\n",
    "    # data script could change tokenizer shape\n",
    "    train_dataset = data_utils.get_preprocessed_mixed_dataset(\n",
    "            preprocessed_dir=data_args.dataset_dir,\n",
    "            domain_weights_dict=train_domain_weights_dict,\n",
    "            dataset_name=data_args.dataset_name,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            split='train',\n",
    "            max_samples=data_args.max_train_samples,\n",
    "            add_domain_id=data_args.add_domain_id,\n",
    "            tmp_file=None,\n",
    "            seed=training_args.seed,\n",
    "            tokenizer=tokenizer,\n",
    "            shuffle=data_args.shuffle,\n",
    "            num_skip_examples=num_skip_examples,\n",
    "            shard_reversal=training_args.reweight_domains\n",
    "    )\n",
    "\n",
    "# if training_args.do_eval:\n",
    "#     eval_dataset = data_utils.get_preprocessed_mixed_dataset(\n",
    "#             preprocessed_dir=data_args.dataset_dir,\n",
    "#             domain_weights_dict=eval_domain_weights_dict,\n",
    "#             dataset_name=data_args.dataset_name,\n",
    "#             cache_dir=model_args.cache_dir,\n",
    "#             split='validation',\n",
    "#             add_domain_id=data_args.add_domain_id,\n",
    "#             max_samples=data_args.max_eval_samples,\n",
    "#             tokenizer=tokenizer,\n",
    "#             no_interleave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ff99a4e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed\n",
      "{'cot': 0.5, 'flan_v2': 0.25, 'dolly': 0.12, 'oasst1': 0.13}\n",
      "pile\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache\n",
      "train\n",
      "None\n",
      "True\n",
      "1111\n",
      "True\n",
      "0\n",
      "False\n",
      "\n",
      "{\n",
      "    \"domain_names\": [\n",
      "        \"cot\",\n",
      "        \"dolly\",\n",
      "        \"flan_v2\",\n",
      "        \"oasst1\"\n",
      "    ],\n",
      "    \"domain_to_idx\": {\n",
      "        \"cot\": 0,\n",
      "        \"dolly\": 1,\n",
      "        \"flan_v2\": 2,\n",
      "        \"oasst1\": 3\n",
      "    },\n",
      "    \"domain_weights\": [\n",
      "        0.5,\n",
      "        0.12,\n",
      "        0.25,\n",
      "        0.13\n",
      "    ]\n",
      "}\n",
      "\n",
      "{\n",
      "    \"probabilities\": [\n",
      "        0.5,\n",
      "        0.12,\n",
      "        0.25,\n",
      "        0.13\n",
      "    ]\n",
      "}\n",
      "08/01/2023 17:30:58 - WARNING - __main__ - No split used or split directory not found: using same data for all splits.\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed\n",
      "{'cot': 0, 'dolly': 0, 'flan_v2': 0, 'oasst1': 0}\n",
      "\n",
      "{\n",
      "    \"preprocessed_dir\": \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed\",\n",
      "    \"domain_name_to_skip_num\": {\n",
      "        \"cot\": 0,\n",
      "        \"dolly\": 0,\n",
      "        \"flan_v2\": 0,\n",
      "        \"oasst1\": 0\n",
      "    }\n",
      "}\n",
      "08/01/2023 17:30:58 - INFO - datasets.builder - Using custom data configuration default-20e8d8f5df875937\n",
      "08/01/2023 17:30:58 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/01/2023 17:30:58 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/01/2023 17:30:58 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/01/2023 17:30:58 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/01/2023 17:30:58 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64564/969478157.py:79: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(f\"No split used or split directory not found: using same data for all splits.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c1de97842b4348b66e5b9553687422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_00000_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_00001_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_00002_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_00003_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_00004_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_00005_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_00006_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_00007_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #8 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_00008_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #9 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_00009_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #10 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_00010_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #11 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_00011_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #12 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_00012_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #13 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_00013_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #14 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_00014_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #15 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_00015_of_00016.arrow\n",
      "08/01/2023 17:30:58 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b9a63c60434c0f7_*_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Concatenating 16 shards\n",
      "08/01/2023 17:30:58 - INFO - datasets.builder - Using custom data configuration default-5a40afd3f0f8b62f\n",
      "08/01/2023 17:30:58 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/01/2023 17:30:58 - INFO - datasets.builder - Using custom data configuration default-68fd04897f8e942c\n",
      "08/01/2023 17:30:58 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/01/2023 17:30:58 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/01/2023 17:30:58 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/01/2023 17:30:58 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/01/2023 17:30:58 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5757de385da243ae87a9f01013f7bca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_00000_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_00001_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_00002_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_00003_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_00004_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_00005_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_00006_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_00007_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #8 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_00008_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #9 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_00009_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #10 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_00010_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #11 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_00011_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #12 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_00012_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #13 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_00013_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #14 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_00014_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #15 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_00015_of_00016.arrow\n",
      "08/01/2023 17:30:58 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-273c81b3f5a0e1d5_*_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Concatenating 16 shards\n",
      "08/01/2023 17:30:58 - INFO - datasets.builder - Using custom data configuration default-b31cb0da24b51be5\n",
      "08/01/2023 17:30:58 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/01/2023 17:30:58 - INFO - datasets.builder - Using custom data configuration default-a01381664fd2589b\n",
      "08/01/2023 17:30:58 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/01/2023 17:30:58 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/01/2023 17:30:58 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/01/2023 17:30:58 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/01/2023 17:30:58 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9b513fc2154635b9fd6e4c59039a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_00000_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_00001_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_00002_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_00003_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_00004_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_00005_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_00006_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_00007_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #8 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_00008_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #9 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_00009_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #10 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_00010_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #11 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_00011_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #12 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_00012_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #13 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_00013_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #14 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_00014_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #15 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_00015_of_00016.arrow\n",
      "08/01/2023 17:30:58 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-216e0143932edb57_*_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Concatenating 16 shards\n",
      "08/01/2023 17:30:58 - INFO - datasets.builder - Using custom data configuration default-580b256e2fb5a83e\n",
      "08/01/2023 17:30:58 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/01/2023 17:30:58 - INFO - datasets.builder - Using custom data configuration default-d43591dedd2996a1\n",
      "08/01/2023 17:30:58 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/01/2023 17:30:58 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/01/2023 17:30:58 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/01/2023 17:30:58 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/01/2023 17:30:58 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd54ebe149ed4287a03337982b06c39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_00000_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_00001_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_00002_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_00003_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_00004_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_00005_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_00006_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_00007_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #8 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_00008_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #9 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_00009_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #10 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_00010_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #11 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_00011_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #12 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_00012_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #13 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_00013_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #14 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_00014_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Process #15 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_00015_of_00016.arrow\n",
      "08/01/2023 17:30:58 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d828a53418d0df31_*_of_00016.arrow\n",
      "08/01/2023 17:30:58 - INFO - datasets.arrow_dataset - Concatenating 16 shards\n",
      "08/01/2023 17:30:58 - INFO - datasets.builder - Using custom data configuration default-8427041964f2d408\n",
      "08/01/2023 17:30:58 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n"
     ]
    }
   ],
   "source": [
    "from doremi.dataloader import determine_skip_per_domain\n",
    "\n",
    "\n",
    "preprocessed_dir=data_args.dataset_dir\n",
    "domain_weights_dict=train_domain_weights_dict\n",
    "dataset_name=data_args.dataset_name\n",
    "cache_dir=model_args.cache_dir\n",
    "split='train'\n",
    "max_samples=data_args.max_train_samples\n",
    "add_domain_id=data_args.add_domain_id\n",
    "tmp_file=None\n",
    "seed=training_args.seed\n",
    "tokenizer=tokenizer\n",
    "shuffle=data_args.shuffle\n",
    "num_skip_examples=num_skip_examples\n",
    "shard_reversal=training_args.reweight_domains\n",
    "no_interleave=False\n",
    "\n",
    "print(preprocessed_dir)\n",
    "print(domain_weights_dict)\n",
    "print(dataset_name)\n",
    "print(cache_dir)\n",
    "print(split)\n",
    "print(max_samples)\n",
    "print(add_domain_id)\n",
    "print(seed)\n",
    "print(shuffle)\n",
    "print(num_skip_examples)\n",
    "print(shard_reversal)\n",
    "\n",
    "\n",
    "domain_names = list(sorted(domain_weights_dict.keys()))\n",
    "domain_to_idx = {domain_names[i]: i for i in range(len(domain_names))}\n",
    "domain_weights = np.asarray([domain_weights_dict[domain_name] for domain_name in domain_names])\n",
    "domain_weights = domain_weights / domain_weights.sum()\n",
    "\n",
    "print()\n",
    "print(json.dumps({'domain_names': domain_names, \n",
    "                  'domain_to_idx': domain_to_idx, \n",
    "                  'domain_weights': list(domain_weights)},\n",
    "                indent=4))\n",
    "\n",
    "\n",
    "# write domain weights to file if tmp_file is set\n",
    "if tmp_file is not None:\n",
    "    probabilities_tmp_file = tmp_file\n",
    "\n",
    "    with open(str(probabilities_tmp_file), 'wb') as f:\n",
    "        pickle.dump(domain_weights, f)\n",
    "    probabilities = None\n",
    "else:\n",
    "    probabilities = domain_weights\n",
    "    probabilities_tmp_file = None\n",
    "\n",
    "\n",
    "print()\n",
    "print(json.dumps({'probabilities': list(probabilities)}, indent=4))\n",
    "\n",
    "# from doremi.dataloader import get_perdomain_datasets\n",
    "# all_ds = get_perdomain_datasets(\n",
    "#     preprocessed_dir, \n",
    "#     domain_weights_dict,\n",
    "#     cache_dir=cache_dir,\n",
    "#     split=split,\n",
    "#     seed=seed,\n",
    "#     domain_weights=domain_weights,\n",
    "#     domain_names=domain_names,\n",
    "#     num_skip_examples=num_skip_examples,\n",
    "#     shuffle=shuffle,\n",
    "#     shard_reversal=shard_reversal\n",
    "# )\n",
    "\n",
    "domain_name_to_skip_num = determine_skip_per_domain(num_skip_examples, seed, domain_weights, domain_names)\n",
    "\n",
    "preprocessed_dir = Path(preprocessed_dir)\n",
    "if split is not None and (preprocessed_dir / split).exists():\n",
    "    preprocessed_dir = preprocessed_dir / split\n",
    "else:\n",
    "    logger.warn(f\"No split used or split directory not found: using same data for all splits.\")\n",
    "\n",
    "domains = list(sorted(domain_weights_dict.keys()))\n",
    "\n",
    "print(preprocessed_dir)\n",
    "print(domain_name_to_skip_num)\n",
    "print()\n",
    "print(json.dumps({'preprocessed_dir': str(preprocessed_dir), \n",
    "                  'domain_name_to_skip_num': domain_name_to_skip_num}, indent=4))\n",
    "\n",
    "\n",
    "all_ds = {}\n",
    "for domain in domains:\n",
    "    domain_dir = preprocessed_dir / domain\n",
    "    if (domain_dir / f'{domain}_data.jsonl').exists():\n",
    "        from datasets import load_dataset\n",
    "        from functools import partial\n",
    "        from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n",
    "        from doremi.dataloader import skippable_data_gen_dataset\n",
    "\n",
    "        data_files = {'train': str(domain_dir / f'{domain}_data.jsonl')}\n",
    "        raw_datasets = load_dataset(\n",
    "            \"json\",\n",
    "            data_files=data_files,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "        )\n",
    "        # Preprocessing the datasets.\n",
    "        if \"prompt\" in raw_datasets[\"train\"].column_names and \"completion\" in raw_datasets[\"train\"].column_names:\n",
    "            encode_function = partial(\n",
    "                encode_with_prompt_completion_format,\n",
    "                tokenizer=tokenizer,\n",
    "                max_seq_length=1024,\n",
    "            )\n",
    "        elif \"messages\" in raw_datasets[\"train\"].column_names:\n",
    "            encode_function = partial(\n",
    "                encode_with_messages_format,\n",
    "                tokenizer=tokenizer,\n",
    "                max_seq_length=1024,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"You need to have either 'prompt'&'completion' or 'messages' in your column names.\")\n",
    "\n",
    "        with training_args.main_process_first(local=False, desc=\"Processing instruction data\"):\n",
    "            lm_datasets = raw_datasets.map(\n",
    "                encode_function,\n",
    "                num_proc=16,\n",
    "                batched=False,\n",
    "            )\n",
    "            lm_datasets.set_format(type=\"pt\")\n",
    "        ds = lm_datasets['train']\n",
    "        ds = IterableDataset.from_generator(\n",
    "                skippable_data_gen_dataset,\n",
    "                gen_kwargs={'ds': ds,\n",
    "                            'num_skip_examples': domain_name_to_skip_num[domain],\n",
    "                            'loop': (split == 'train'),\n",
    "                            'seed': seed,\n",
    "                            'shuffle': shuffle}\n",
    "                )\n",
    "        seed += 1\n",
    "    elif (domain_dir / 'dataset_info.json').exists():\n",
    "        ds = load_from_disk(dataset_path=str(domain_dir))\n",
    "        logger.info(f\"Loaded {domain_dir}. Length: {len(ds)}\")\n",
    "    else:\n",
    "        curr_shards = list(domain_dir.iterdir())\n",
    "        if shard_reversal:\n",
    "            curr_shards = list(reversed(curr_shards))\n",
    "        # shuffle shard order\n",
    "        random.Random(seed).shuffle(curr_shards)\n",
    "        ds = IterableDataset.from_generator(\n",
    "                skippable_data_gen,\n",
    "                gen_kwargs={'shards': curr_shards,\n",
    "                            'num_skip_examples': domain_name_to_skip_num[domain],\n",
    "                            'loop': (split == 'train'),\n",
    "                            'seed': seed,\n",
    "                            'shuffle': shuffle}\n",
    "                )\n",
    "        seed += 1\n",
    "    all_ds[domain] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a92a59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ds = load_dataset(\n",
    "#     \"json\",\n",
    "#     data_files='test.jsonl',\n",
    "#     cache_dir=model_args.cache_dir)['train']\n",
    "# test_ds[0]\n",
    "\n",
    "# for x in test_ds.to_iterable_dataset():\n",
    "#     print(x)\n",
    "# for i, v in enumerate(ds):\n",
    "#     if i == 10:\n",
    "#         break\n",
    "#     print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "07f33784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 17:30:59 - INFO - datasets.builder - Using custom data configuration default-054ef3928e56a804\n",
      "08/01/2023 17:30:59 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/01/2023 17:30:59 - INFO - datasets.builder - Using custom data configuration default-ad2f540c72bd8fb9\n",
      "08/01/2023 17:30:59 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/01/2023 17:30:59 - INFO - datasets.builder - Using custom data configuration default-dfe585457d1ad863\n",
      "08/01/2023 17:30:59 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/01/2023 17:30:59 - INFO - datasets.builder - Using custom data configuration default-253e201ad2e5dd82\n",
      "08/01/2023 17:30:59 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def add_domain_id_generator(ds, domain_idx):\n",
    "    for ex in ds:\n",
    "        print(ex)\n",
    "        ex['domain_id'] = domain_idx\n",
    "        yield ex\n",
    "\n",
    "        \n",
    "domain_ds_ls = []\n",
    "for domain_name in domain_names:\n",
    "    domain_idx = domain_to_idx[domain_name]\n",
    "    domain_ds = all_ds[domain_name]\n",
    "    # add domain_id if necessary\n",
    "    if add_domain_id:\n",
    "        domain_ds = IterableDataset.from_generator(\n",
    "            add_domain_id_generator, \n",
    "            gen_kwargs={'ds': domain_ds, 'domain_idx': domain_idx})\n",
    "    domain_ds_ls.append(domain_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "805b7cad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 17:30:59 - INFO - datasets.builder - Using custom data configuration default-d63725ddb00f354b\n",
      "08/01/2023 17:30:59 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n"
     ]
    }
   ],
   "source": [
    "from doremi.dataloader import interleave_datasets\n",
    "no_interleave = False\n",
    "\n",
    "if no_interleave:\n",
    "    # instead of interleaving, run through each dataset\n",
    "    def data_generator(shards):\n",
    "        for shard in shards:\n",
    "            for ex in shard:\n",
    "                yield ex\n",
    "    ds = IterableDataset.from_generator(data_generator, gen_kwargs={'shards': domain_ds_ls})\n",
    "    logger.info(\"Not interleaving dataset - will not sample according to domain weights\")\n",
    "\n",
    "else:\n",
    "    ds = interleave_datasets(\n",
    "            domain_ds_ls,\n",
    "            probabilities=probabilities,\n",
    "            probabilities_file=probabilities_tmp_file,\n",
    "            seed=seed)\n",
    "    \n",
    "\n",
    "def take_data_generator(ds, max_samples):\n",
    "    idx = 0\n",
    "    for ex in ds:\n",
    "        yield ex\n",
    "        idx += 1\n",
    "        if max_samples is not None and idx >= max_samples:\n",
    "            return\n",
    "\n",
    "ds = IterableDataset.from_generator(take_data_generator, gen_kwargs={'ds': ds, 'max_samples': max_samples})\n",
    "train_dataset = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "892ead90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ds_iter = iter(ds)\n",
    "\n",
    "# l = []\n",
    "\n",
    "# for i, x in enumerate(ds_iter):\n",
    "# #     print(x.keys())\n",
    "#     l.append(x['domain_id'])\n",
    "#     if i == 2000:\n",
    "#         break\n",
    "        \n",
    "# from collections import Counter\n",
    "# Counter(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7e35ed26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:565] 2023-08-01 17:31:00,383 >> max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# turn off find unused parameters\n",
    "training_args.ddp_find_unused_parameters = False\n",
    "\n",
    "# We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "# on a small vocab and want a smaller embedding size, remove this test.\n",
    "# embedding_size = model.get_input_embeddings.weight.shape[0]\n",
    "# if len(tokenizer) > embedding_size:\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = DoReMiTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_utils.get_data_collator(tokenizer, do_padding=data_args.do_padding),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2a3f80bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1682] 2023-08-01 17:31:01,721 >> ***** Running training *****\n",
      "[INFO|trainer.py:1683] 2023-08-01 17:31:01,722 >>   Num examples = 400,000\n",
      "[INFO|trainer.py:1684] 2023-08-01 17:31:01,723 >>   Num Epochs = 9,223,372,036,854,775,807\n",
      "[INFO|trainer.py:1685] 2023-08-01 17:31:01,723 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:1688] 2023-08-01 17:31:01,724 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "[INFO|trainer.py:1689] 2023-08-01 17:31:01,724 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1690] 2023-08-01 17:31:01,725 >>   Total optimization steps = 200,000\n",
      "[INFO|trainer.py:1691] 2023-08-01 17:31:01,727 >>   Number of trainable parameters = 279,245,824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 17:31:01 - INFO - datasets.arrow_dataset - Caching indices mapping at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1d505b89c64f68c7.arrow\n",
      "{'dataset': 'cot', 'id': 'cot_60290', 'messages': [{'role': 'user', 'content': 'QUESTION: Premise: \"A man is squatting down while painting his artwork on a large metal garage door.\"\\nBased on this premise, can we conclude that the hypothesis \"A man paints art on garage door.\" is true?\\nOptions:\\n- yes\\n- it is not possible to tell\\n- no\\n\\nLet\\'s solve it slowly: A man is squatting down while paints art on metal garage door.\\nThe answer is yes.\\n[QUESTION] Premise: \"Woman hiking outside in a vineyard.\"\\nBased on this premise, can we conclude that the hypothesis \"The vines are covered with ripe grapes.\" is true?\\nOptions:\\n- yes\\n- it is not possible to tell\\n- no\\nHiking in a vineyard does not imply the vines are covered with ripe grapes.\\nThe answer is it is not possible to tell.\\nQ: Test for natural language inference.\\nPremise: \"A woman walks through the drizzling rain while shielding herself with her red umbrella.\"\\nHypothesis: \"A woman is out in the rain.\"\\nIs the hypothesis entailed by the premise?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell\\nA: A woman walks through rain means she is out in the rain.\\nThe answer is yes.\\nQUESTION: Premise: \"Two people are waiting for their food.\"\\nHypothesis: \"People are waiting on food.\"\\nDo we know that the hypothesis entailed by the premise?\\n\\nLet\\'s solve it slowly: People who is plural for two people\\'s waiting on their food.\\nThe answer is yes.\\n[QUESTION] Test for natural language inference.\\nPremise: \"A dog swimming though water holding a stick.\"\\nHypothesis: \"A dog is swimming while holding onto a stick in his mouth to bring it back to its owner.\"\\nIs the hypothesis entailed by the premise?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell\\nA dog holding a stick will not always bring it back to its owner.\\nThe answer is it is not possible to tell.\\nQ: Premise: \"A woman sitting outside a small restaurant suddenly throws up her hands in excitement.\"\\nBased on this premise, can we conclude that the hypothesis \"A lady throws up her hands in excitement because she spots her friend.\" is true?\\nOptions:\\n- yes\\n- it is not possible to tell\\n- no\\nA:'}, {'role': 'assistant', 'content': 'A woman sitting outside a small restaurant suddenly throws up her hands in excitement does not indicate that she spots her friend.\\nThe answer is it is not possible to tell.'}], 'input_ids': tensor([   27,    91,  7220,    91,    29,   198, 35780,  2849,    25,  6929,\n",
      "          786,    25,   366,    32,   582,   318, 22713,   889,   866,   981,\n",
      "        12036,   465, 16257,   319,   257,  1588,  6147, 15591,  3420,   526,\n",
      "          198, 15001,   319,   428, 18659,    11,   460,   356, 13796,   326,\n",
      "          262, 14078,   366,    32,   582, 32281,  1242,   319, 15591,  3420,\n",
      "          526,   318,  2081,    30,   198, 29046,    25,   198,    12,  3763,\n",
      "          198,    12,   340,   318,   407,  1744,   284,  1560,   198,    12,\n",
      "          645,   198,   198,  5756,   338,  8494,   340,  6364,    25,   317,\n",
      "          582,   318, 22713,   889,   866,   981, 32281,  1242,   319,  6147,\n",
      "        15591,  3420,    13,   198,   464,  3280,   318,  3763,    13,   198,\n",
      "           58, 35780,  2849,    60,  6929,   786,    25,   366, 48081, 24522,\n",
      "         2354,   287,   257, 17793,  9413,   526,   198, 15001,   319,   428,\n",
      "        18659,    11,   460,   356, 13796,   326,   262, 14078,   366,   464,\n",
      "        44439,   389,  5017,   351, 29036, 36656,   526,   318,  2081,    30,\n",
      "          198, 29046,    25,   198,    12,  3763,   198,    12,   340,   318,\n",
      "          407,  1744,   284,  1560,   198,    12,   645,   198,    39, 14132,\n",
      "          287,   257, 17793,  9413,   857,   407, 20135,   262, 44439,   389,\n",
      "         5017,   351, 29036, 36656,    13,   198,   464,  3280,   318,   340,\n",
      "          318,   407,  1744,   284,  1560,    13,   198,    48,    25,  6208,\n",
      "          329,  3288,  3303, 32278,    13,   198, 24914,   786,    25,   366,\n",
      "           32,  2415, 11114,   832,   262,  1454,  3019,  1359,  6290,   981,\n",
      "        43539,  5223,   351,   607,  2266, 25510,   526,   198, 49926,   313,\n",
      "         8497,    25,   366,    32,  2415,   318,   503,   287,   262,  6290,\n",
      "          526,   198,  3792,   262, 14078,   920,  6255,   416,   262, 18659,\n",
      "           30,   198, 29046,    25,   198,    12,  3763,   198,    12,   645,\n",
      "          198,    12,   340,   318,   407,  1744,   284,  1560,   198,    32,\n",
      "           25,   317,  2415, 11114,   832,  6290,  1724,   673,   318,   503,\n",
      "          287,   262,  6290,    13,   198,   464,  3280,   318,  3763,    13,\n",
      "          198, 35780,  2849,    25,  6929,   786,    25,   366,  7571,   661,\n",
      "          389,  4953,   329,   511,  2057,   526,   198, 49926,   313,  8497,\n",
      "           25,   366,  8061,   389,  4953,   319,  2057,   526,   198,  5211,\n",
      "          356,   760,   326,   262, 14078,   920,  6255,   416,   262, 18659,\n",
      "           30,   198,   198,  5756,   338,  8494,   340,  6364,    25,  4380,\n",
      "          508,   318, 22801,   329,   734,   661,   338,  4953,   319,   511,\n",
      "         2057,    13,   198,   464,  3280,   318,  3763,    13,   198,    58,\n",
      "        35780,  2849,    60,  6208,   329,  3288,  3303, 32278,    13,   198,\n",
      "        24914,   786,    25,   366,    32,  3290, 14899,   996,  1660,  4769,\n",
      "          257,  4859,   526,   198, 49926,   313,  8497,    25,   366,    32,\n",
      "         3290,   318, 14899,   981,  4769,  4291,   257,  4859,   287,   465,\n",
      "         5422,   284,  2222,   340,   736,   284,   663,  4870,   526,   198,\n",
      "         3792,   262, 14078,   920,  6255,   416,   262, 18659,    30,   198,\n",
      "        29046,    25,   198,    12,  3763,   198,    12,   645,   198,    12,\n",
      "          340,   318,   407,  1744,   284,  1560,   198,    32,  3290,  4769,\n",
      "          257,  4859,   481,   407,  1464,  2222,   340,   736,   284,   663,\n",
      "         4870,    13,   198,   464,  3280,   318,   340,   318,   407,  1744,\n",
      "          284,  1560,    13,   198,    48,    25,  6929,   786,    25,   366,\n",
      "           32,  2415,  5586,  2354,   257,  1402,  7072,  6451, 12542,   510,\n",
      "          607,  2832,   287, 14067,   526,   198, 15001,   319,   428, 18659,\n",
      "           11,   460,   356, 13796,   326,   262, 14078,   366,    32, 10846,\n",
      "        12542,   510,   607,  2832,   287, 14067,   780,   673, 10222,   607,\n",
      "         1545,   526,   318,  2081,    30,   198, 29046,    25,   198,    12,\n",
      "         3763,   198,    12,   340,   318,   407,  1744,   284,  1560,   198,\n",
      "           12,   645,   198,    32,    25,   198,    27,    91,   562, 10167,\n",
      "           91,    29,   198,    32,  2415,  5586,  2354,   257,  1402,  7072,\n",
      "         6451, 12542,   510,   607,  2832,   287, 14067,   857,   407,  7603,\n",
      "          326,   673, 10222,   607,  1545,    13,   198,   464,  3280,   318,\n",
      "          340,   318,   407,  1744,   284,  1560,    13, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,    32,  2415,  5586,  2354,   257,  1402,  7072,\n",
      "         6451, 12542,   510,   607,  2832,   287, 14067,   857,   407,  7603,\n",
      "          326,   673, 10222,   607,  1545,    13,   198,   464,  3280,   318,\n",
      "          340,   318,   407,  1744,   284,  1560,    13, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'dataset': 'cot', 'id': 'cot_69782', 'messages': [{'role': 'user', 'content': 'People dancing outside does not mean that those people had attended a wedding.. So what could be the question?\\nQuestion followed by answer: If \"A group of men and women dancing outside.\" does that mean that \"People dancing outside after a wedding.\"?\\nOptions:\\n- yes\\n- it is not possible to tell\\n- no\\nit is not possible to tell\\n\\nOne can not hold an infant and eat food at the same time.\\nThe question and answer are below.\\nPremise: \"A man holds an infant while leaning against a building.\"\\nHypothesis: \"A man is eating food.\"\\nDo we know that the hypothesis entailed by the premise?\\nno\\n\\nThe monster truck leaping over crushed cars is just a past tense action of how the truck leaps over crushed cars.\\nThe question and answer are below.\\nCan we conclude from \"A giant monster truck leaps over crushed cars and into the air inside the arena.\" that \"Monster truck leaping over crushed cars.\"?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell\\nyes\\n\\nPower Wheels motorcycle is a motorcycle. One who rides a motorcycle also drives a motorcycle.. So what could be the question?\\nQuestion followed by answer: Can we conclude from \"A young boy drives a power wheels motorcycle with an american flag attached to the back.\" that \"A young boy rides a toy motorcycle.\"?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell\\nyes\\n\\nA man on a horse who tries to throw him off is on a bucking bronco.\\nThe question and answer are below.\\nTest for natural language inference.\\nPremise: \"A man in a red plaid shirt holds on for dear life as the horse tries to throw him off.\"\\nHypothesis: \"A man on a bucking bronco.\"\\nIs the hypothesis entailed by the premise?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell\\nyes\\n\\nSitting up high does not imply they are sitting on a bridge.\\nThe question and answer are below.\\n'}, {'role': 'assistant', 'content': 'Given the sentence \"A person is sitting up high over a street.\" is it true that \"A person sits on a bridge over a freeway.\"?\\nit is not possible to tell'}], 'input_ids': tensor([   27,    91,  7220,    91,    29,   198,  8061, 15360,  2354,   857,\n",
      "          407,  1612,   326,   883,   661,   550,  9141,   257, 10614,   492,\n",
      "         1406,   644,   714,   307,   262,  1808,    30,   198, 24361,  3940,\n",
      "          416,  3280,    25,  1002,   366,    32,  1448,   286,  1450,   290,\n",
      "         1466, 15360,  2354,   526,   857,   326,  1612,   326,   366,  8061,\n",
      "        15360,  2354,   706,   257, 10614,   526,    30,   198, 29046,    25,\n",
      "          198,    12,  3763,   198,    12,   340,   318,   407,  1744,   284,\n",
      "         1560,   198,    12,   645,   198,   270,   318,   407,  1744,   284,\n",
      "         1560,   198,   198,  3198,   460,   407,  1745,   281, 11212,   290,\n",
      "         4483,  2057,   379,   262,   976,   640,    13,   198,   464,  1808,\n",
      "          290,  3280,   389,  2174,    13,   198, 24914,   786,    25,   366,\n",
      "           32,   582,  6622,   281, 11212,   981, 21804,  1028,   257,  2615,\n",
      "          526,   198, 49926,   313,  8497,    25,   366,    32,   582,   318,\n",
      "         6600,  2057,   526,   198,  5211,   356,   760,   326,   262, 14078,\n",
      "          920,  6255,   416,   262, 18659,    30,   198,  3919,   198,   198,\n",
      "          464,  9234,  7779, 45583,   625, 18577,  5006,   318,   655,   257,\n",
      "         1613, 20170,  2223,   286,   703,   262,  7779, 37312,   625, 18577,\n",
      "         5006,    13,   198,   464,  1808,   290,  3280,   389,  2174,    13,\n",
      "          198,  6090,   356, 13796,   422,   366,    32,  6175,  9234,  7779,\n",
      "        37312,   625, 18577,  5006,   290,   656,   262,  1633,  2641,   262,\n",
      "        13478,   526,   326,   366, 40872,  7779, 45583,   625, 18577,  5006,\n",
      "          526,    30,   198, 29046,    25,   198,    12,  3763,   198,    12,\n",
      "          645,   198,    12,   340,   318,   407,  1744,   284,  1560,   198,\n",
      "         8505,   198,   198, 13434, 37416, 18757,   318,   257, 18757,    13,\n",
      "         1881,   508, 17445,   257, 18757,   635, 10182,   257, 18757,   492,\n",
      "         1406,   644,   714,   307,   262,  1808,    30,   198, 24361,  3940,\n",
      "          416,  3280,    25,  1680,   356, 13796,   422,   366,    32,  1862,\n",
      "         2933, 10182,   257,  1176, 13666, 18757,   351,   281, 45630,   272,\n",
      "         6056,  7223,   284,   262,   736,   526,   326,   366,    32,  1862,\n",
      "         2933, 17445,   257, 13373, 18757,   526,    30,   198, 29046,    25,\n",
      "          198,    12,  3763,   198,    12,   645,   198,    12,   340,   318,\n",
      "          407,  1744,   284,  1560,   198,  8505,   198,   198,    32,   582,\n",
      "          319,   257,  8223,   508,  8404,   284,  3714,   683,   572,   318,\n",
      "          319,   257, 10230,   278, 18443,  1073,    13,   198,   464,  1808,\n",
      "          290,  3280,   389,  2174,    13,   198, 14402,   329,  3288,  3303,\n",
      "        32278,    13,   198, 24914,   786,    25,   366,    32,   582,   287,\n",
      "          257,  2266,   458,  1698, 10147,  6622,   319,   329, 13674,  1204,\n",
      "          355,   262,  8223,  8404,   284,  3714,   683,   572,   526,   198,\n",
      "        49926,   313,  8497,    25,   366,    32,   582,   319,   257, 10230,\n",
      "          278, 18443,  1073,   526,   198,  3792,   262, 14078,   920,  6255,\n",
      "          416,   262, 18659,    30,   198, 29046,    25,   198,    12,  3763,\n",
      "          198,    12,   645,   198,    12,   340,   318,   407,  1744,   284,\n",
      "         1560,   198,  8505,   198,   198,    50,  2535,   510,  1029,   857,\n",
      "          407, 20135,   484,   389,  5586,   319,   257,  7696,    13,   198,\n",
      "          464,  1808,   290,  3280,   389,  2174,    13,   198,    27,    91,\n",
      "          562, 10167,    91,    29,   198, 15056,   262,  6827,   366,    32,\n",
      "         1048,   318,  5586,   510,  1029,   625,   257,  4675,   526,   318,\n",
      "          340,  2081,   326,   366,    32,  1048, 10718,   319,   257,  7696,\n",
      "          625,   257, 39042,   526,    30,   198,   270,   318,   407,  1744,\n",
      "          284,  1560, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100, 15056,   262,  6827,   366,    32,\n",
      "         1048,   318,  5586,   510,  1029,   625,   257,  4675,   526,   318,\n",
      "          340,  2081,   326,   366,    32,  1048, 10718,   319,   257,  7696,\n",
      "          625,   257, 39042,   526,    30,   198,   270,   318,   407,  1744,\n",
      "          284,  1560, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "08/01/2023 17:31:01 - INFO - datasets.arrow_dataset - Caching indices mapping at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ef242118c55c9ad.arrow\n",
      "{'dataset': 'flan_v2', 'id': 'flan_v2_70780', 'messages': [{'role': 'user', 'content': \"(Question)\\nInside Recordings is a Los Angeles California based independent record label founded by singer-songwriter Jackson Browne in 1999. Browne has stated that the mission of the label is to create a haven for music that might not find a home in the mainstream. In 2005 Inside Recordings signed a nationwide direct distribution deal with independent music distributor Alternative Distribution Alliance. The label's inaugural release under the agreement was Browne's own Solo Acoustic Vol. Given a list of categories: company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film or written work, what category does the paragraph belong to?\\n(Answer)\\nCompany\\n\\n\\n(Question)\\nId Rather Go Blonde is the eighth album in a twenty-year career that has seen Eleanor McEvoy establish herself as one of Irelands most accomplished singer / songwriters. Given a list of categories: company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film or written work, what category does the paragraph belong to?\\n(Answer)\\nAlbum\\n\\n\\n(Question)\\nMarcin Kamiski (Polish pronunciation: [martin kamiski]; born 15 January 1992 in Konin) is a left footed Polish footballer who plays for Lech Pozna as a central defender. Given a list of categories: company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film or written work, what category does the paragraph belong to?\\n(Answer)\\n\"}, {'role': 'assistant', 'content': 'Athlete'}], 'input_ids': tensor([   27,    91,  7220,    91,    29,   198,     7, 24361,     8,   198,\n",
      "        24441, 13266,   654,   318,   257,  5401,  5652,  3442,  1912,  4795,\n",
      "         1700,  6167,  9393,   416, 14015,    12, 34050, 16002,  6612, 46760,\n",
      "          287,  7358,    13, 46760,   468,  5081,   326,   262,  4365,   286,\n",
      "          262,  6167,   318,   284,  2251,   257,  4398,   329,  2647,   326,\n",
      "         1244,   407,  1064,   257,  1363,   287,   262,  8661,    13,   554,\n",
      "         5075, 14384, 13266,   654,  4488,   257, 13673,  1277,  6082,  1730,\n",
      "          351,  4795,  2647, 32137, 27182, 27484, 10302,    13,   383,  6167,\n",
      "          338, 26851,  2650,   739,   262,  4381,   373, 46760,   338,   898,\n",
      "        20284,  4013, 21618,  4709,    13, 11259,   257,  1351,   286,  9376,\n",
      "           25,  1664,    11,  9856,  9901,    11,  6802,    11, 16076,    11,\n",
      "         2607, 15762,    11,  1612,   286,  9358,    11,  2615,    11,  3288,\n",
      "         1295,    11,  7404,    11,  5044,    11,  4618,    11,  5062,    11,\n",
      "         2646,   393,  3194,   670,    11,   644,  6536,   857,   262,  7322,\n",
      "         5594,   284,    30,   198,     7, 33706,     8,   198, 39154,   628,\n",
      "          198,     7, 24361,     8,   198,    40,   447,   247,    67, 11317,\n",
      "         1514,  1086, 14378,   318,   262, 16974,  5062,   287,   257,  8208,\n",
      "           12,  1941,  3451,   326,   468,  1775, 45566,  1982, 15200,   726,\n",
      "         4474,  5223,   355,   530,   286,  7517,   447,   247,    82,   749,\n",
      "        13013, 14015,  1220,  3496, 34422,    13, 11259,   257,  1351,   286,\n",
      "         9376,    25,  1664,    11,  9856,  9901,    11,  6802,    11, 16076,\n",
      "           11,  2607, 15762,    11,  1612,   286,  9358,    11,  2615,    11,\n",
      "         3288,  1295,    11,  7404,    11,  5044,    11,  4618,    11,  5062,\n",
      "           11,  2646,   393,  3194,   670,    11,   644,  6536,   857,   262,\n",
      "         7322,  5594,   284,    30,   198,     7, 33706,     8,   198,  2348,\n",
      "         4435,   628,   198,     7, 24361,     8,   198, 22697,   259,   509,\n",
      "         6277,   129,   226, 20545,   357,  8017,   680, 41694,    25,   685,\n",
      "        45990, 13822,   137,    94,   133,   243,   259, 38387, 45990, 11632,\n",
      "          133,   110, 20545, 11208,  4642,  1315,  3269,  9768,   287, 17431,\n",
      "          259,     8,   318,   257,  1364,  2366,   276, 15945, 44185,   508,\n",
      "         5341,   329,  1004,   354,  7695,    89,  2616,   129,   226,   355,\n",
      "          257,  4318, 13191,    13, 11259,   257,  1351,   286,  9376,    25,\n",
      "         1664,    11,  9856,  9901,    11,  6802,    11, 16076,    11,  2607,\n",
      "        15762,    11,  1612,   286,  9358,    11,  2615,    11,  3288,  1295,\n",
      "           11,  7404,    11,  5044,    11,  4618,    11,  5062,    11,  2646,\n",
      "          393,  3194,   670,    11,   644,  6536,   857,   262,  7322,  5594,\n",
      "          284,    30,   198,     7, 33706,     8,   198,    27,    91,   562,\n",
      "        10167,    91,    29,   198,    32,   400,  5807, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,    32,   400,  5807, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer_utils.py:696] 2023-08-01 17:31:01,908 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: dataset, id, messages, domain_id. If dataset, id, messages, domain_id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'flan_v2', 'id': 'flan_v2_90368', 'messages': [{'role': 'user', 'content': \"Part 1. Definition\\nGenerate the answer for a given question. The question requires common knowledge of English. Make sure that the answer is in detail, long, consistent with the context, and requires general knowledge of English. To be less repetitive, please vary your language for each question. Give a proper explanation to answer, and if necessary, provide the definition of various terms. And conclude your answer if it's long. If you have used other resources (e.g., Merriam Webster's.) to answer the question, include citations in your answer. You don't need to paraphrase the context of your resources, but make sure that your answer is consistent and complete.\\nPart 2. Example\\nHow are i.e. and e.g. pronounced?\\nAnswer: i.e. stands for id est (Latin), which means 'that is'. You use it to link in a deeper explanation about something. Pronounce it 'eye - ee'. e.g. stands for exempli gratia (also Latin), which means 'for example'. You use it to link in an example of a more generic term. Pronounce it 'ee - jee'\\nExplanation: Perfect answer because it's consistent with context uses required information from the question to provide the detailed answer with an example.\\nPart 3. Exercise\\nIn the book I am reading (Patrick Fairbairn, The Interpretation of Prophecy, 2nd ed., 1865), the abbreviation 'sq.' is used sometimes in referencing both Scripture and other books (as 'Typology of Scripture, vol. i. p. 100 sq.' or 'Ezek xxvi. 7, sq.'). What does 'sq.' mean in this context? (Instances of this abbreviation's usage may be found at Google Books here.).\\nAnswer:\"}, {'role': 'assistant', 'content': \"Sq. is short for the Latin word sequiturque, which means the following. So p. 100 sq. means page 100 and those following pages. Here's the source.\"}], 'input_ids': tensor([   27,    91,  7220,    91,    29,   198,  7841,   352,    13, 30396,\n",
      "          198,  8645,   378,   262,  3280,   329,   257,  1813,  1808,    13,\n",
      "          383,  1808,  4433,  2219,  3725,   286,  3594,    13,  6889,  1654,\n",
      "          326,   262,  3280,   318,   287,  3703,    11,   890,    11,  6414,\n",
      "          351,   262,  4732,    11,   290,  4433,  2276,  3725,   286,  3594,\n",
      "           13,  1675,   307,  1342, 28585,    11,  3387,  7565,   534,  3303,\n",
      "          329,  1123,  1808,    13, 13786,   257,  1774,  7468,   284,  3280,\n",
      "           11,   290,   611,  3306,    11,  2148,   262,  6770,   286,  2972,\n",
      "         2846,    13,   843, 13796,   534,  3280,   611,   340,   338,   890,\n",
      "           13,  1002,   345,   423,   973,   584,  4133,   357,    68,    13,\n",
      "           70,  1539,  4638,   380,   321, 31890,   338,  2014,   284,  3280,\n",
      "          262,  1808,    11,  2291, 33499,   287,   534,  3280,    13,   921,\n",
      "          836,   470,   761,   284, 38624, 22789,   262,  4732,   286,   534,\n",
      "         4133,    11,   475,   787,  1654,   326,   534,  3280,   318,  6414,\n",
      "          290,  1844,    13,   198,  7841,   362,    13, 17934,   198,  2437,\n",
      "          389,  1312,    13,    68,    13,   290,   304,    13,    70,    13,\n",
      "        16293,    30,   198, 33706,    25,  1312,    13,    68,    13,  6296,\n",
      "          329,  4686,  1556,   357, 49022,   828,   543,  1724,   705,  5562,\n",
      "          318,  4458,   921,   779,   340,   284,  2792,   287,   257,  9211,\n",
      "         7468,   546,  1223,    13,   350,  1313,  8652,   340,   705, 25379,\n",
      "          532,   304,    68,  4458,   304,    13,    70,    13,  6296,   329,\n",
      "        21433,    72, 14586,   544,   357, 14508,  9133,   828,   543,  1724,\n",
      "          705,  1640,  1672,  4458,   921,   779,   340,   284,  2792,   287,\n",
      "          281,  1672,   286,   257,   517, 14276,  3381,    13,   350,  1313,\n",
      "         8652,   340,   705,  1453,   532,   474,  1453,     6,   198,  3109,\n",
      "        11578,   341,    25, 16374,  3280,   780,   340,   338,  6414,   351,\n",
      "         4732,  3544,  2672,  1321,   422,   262,  1808,   284,  2148,   262,\n",
      "         6496,  3280,   351,   281,  1672,    13,   198,  7841,   513,    13,\n",
      "        32900,   198,   818,   262,  1492,   314,   716,  3555,   357, 32718,\n",
      "         7011,    65,   958,    77,    11,   383, 48907,   341,   286, 33360,\n",
      "          948,    11,   362,   358,  1225,  1539, 47801,   828,   262, 28873,\n",
      "        47625,   705, 31166,  2637,   318,   973,  3360,   287, 32578,  1111,\n",
      "        25595,   290,   584,  3835,   357,   292,   705, 31467,  1435,   286,\n",
      "        25595,    11,  2322,    13,  1312,    13,   279,    13,  1802, 19862,\n",
      "         2637,   393,   705,    36, 43130, 31383,  8903,    13,   767,    11,\n",
      "        19862,  2637,   737,  1867,   857,   705, 31166,  2637,  1612,   287,\n",
      "          428,  4732,    30,   357,  6310,  1817,   286,   428, 28873, 47625,\n",
      "          338,  8748,   743,   307,  1043,   379,  3012, 13661,   994, 15729,\n",
      "          198, 33706,    25,   198,    27,    91,   562, 10167,    91,    29,\n",
      "          198,    50,    80,    13,   318,  1790,   329,   262,  9133,  1573,\n",
      "         4726,   270,   333,  4188,    11,   543,  1724,   262,  1708,    13,\n",
      "         1406,   279,    13,  1802, 19862,    13,  1724,  2443,  1802,   290,\n",
      "          883,  1708,  5468,    13,  3423,   338,   262,  2723,    13, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,    50,    80,    13,   318,  1790,   329,   262,  9133,  1573,\n",
      "         4726,   270,   333,  4188,    11,   543,  1724,   262,  1708,    13,\n",
      "         1406,   279,    13,  1802, 19862,    13,  1724,  2443,  1802,   290,\n",
      "          883,  1708,  5468,    13,  3423,   338,   262,  2723,    13, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'dataset': 'cot', 'id': 'cot_34935', 'messages': [{'role': 'user', 'content': 'Come up with a question and stream of consciousness reasoning that would justify this answer: it is not possible to tell\\n'}, {'role': 'assistant', 'content': 'The question is: Test for natural language inference.\\nPremise: \"A guy playing a violin on the street and four women are listening to him.\"\\nHypothesis: \"Women tip a street performer.\"\\nIs the hypothesis entailed by the premise?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell\\nStream of consciousness: Guy playing a violin on the street is not necessarily a street performer.\\n'}], 'input_ids': tensor([   27,    91,  7220,    91,    29,   198, 16773,   510,   351,   257,\n",
      "         1808,   290,  4269,   286, 10510, 14607,   326,   561, 12051,   428,\n",
      "         3280,    25,   340,   318,   407,  1744,   284,  1560,   198,    27,\n",
      "           91,   562, 10167,    91,    29,   198,   464,  1808,   318,    25,\n",
      "         6208,   329,  3288,  3303, 32278,    13,   198, 24914,   786,    25,\n",
      "          366,    32,  3516,  2712,   257, 38283,   319,   262,  4675,   290,\n",
      "         1440,  1466,   389,  8680,   284,   683,   526,   198, 49926,   313,\n",
      "         8497,    25,   366, 18495,  8171,   257,  4675, 26960,   526,   198,\n",
      "         3792,   262, 14078,   920,  6255,   416,   262, 18659,    30,   198,\n",
      "        29046,    25,   198,    12,  3763,   198,    12,   645,   198,    12,\n",
      "          340,   318,   407,  1744,   284,  1560,   198, 12124,   286, 10510,\n",
      "           25, 13145,  2712,   257, 38283,   319,   262,  4675,   318,   407,\n",
      "         6646,   257,  4675, 26960,    13, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,   464,  1808,   318,    25,\n",
      "         6208,   329,  3288,  3303, 32278,    13,   198, 24914,   786,    25,\n",
      "          366,    32,  3516,  2712,   257, 38283,   319,   262,  4675,   290,\n",
      "         1440,  1466,   389,  8680,   284,   683,   526,   198, 49926,   313,\n",
      "         8497,    25,   366, 18495,  8171,   257,  4675, 26960,   526,   198,\n",
      "         3792,   262, 14078,   920,  6255,   416,   262, 18659,    30,   198,\n",
      "        29046,    25,   198,    12,  3763,   198,    12,   645,   198,    12,\n",
      "          340,   318,   407,  1744,   284,  1560,   198, 12124,   286, 10510,\n",
      "           25, 13145,  2712,   257, 38283,   319,   262,  4675,   318,   407,\n",
      "         6646,   257,  4675, 26960,    13, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1])}\n",
      "{'dataset': 'cot', 'id': 'cot_63024', 'messages': [{'role': 'user', 'content': \"Detailed logic: A drug store is a place that sells condoms, toothpaste and notepads. One can buy from a drug store.\\ndrug store\\nQuestion for this logic: Where would you buy a notepad along side condoms or toothpaste?\\nOptions:\\n- back pocket\\n- meeting\\n- street seller\\n- drug store\\n- school\\n\\n\\nLogic ==> Hiking means the activity of going for long walks especially across country for exercise and burning lot more calories per hour than walking. Exercises keep body fit and so hiking is the reason for exercise.\\nexercise\\nQuestion ==> What is a reason for hiking?\\nOptions:\\n- adventure\\n- walking\\n- becoming tired\\n- fatigue\\n- exercise\\n\\n\\n** Being against a decision means holding an opposite position of the decision. Being opposite of opposite means being for.\\nbeing for\\n** Sam was against Allison's decision. Joe was the opposite. What was Joe, regarding that decision?\\nOptions:\\n- being for\\n- away from\\n- combined\\n- for or\\n- conjunction\\n\\n\\nDetailed logic: Orchestra is a group of instrumentalists. Bass clarinet and other wood wind instruments come under instruments.\\norchestra\\nQuestion for this logic: Where would your hear a bass clarinet along side other wood wind instruments?\\nOptions:\\n- opera house\\n- school band\\n- music store\\n- orchestra\\n- bathroom stall\\n\\n\\nLogic ==> Aloof people do not enjoy company of other people and get bored.\\nbeing bored\\nQuestion ==> If someone is aloof and self-important, what is their experience when meeting people?\\nOptions:\\n- stress\\n- nervousness\\n- smile\\n- being bored\\n- making new friends\\n\\n\\nLogic ==> When person don't have knowledge of music, he just fiddle the music book instead of playing music from it. To read the notation of music theory book person must have knowledge of music.\\nknowledge of music\\nQuestion ==>\\n\"}, {'role': 'assistant', 'content': 'If you are fiddling through a music book when you should be playing, you probably dont have?\\nOptions:\\n- practice\\n- dexterity\\n- idle time\\n- knowledge of music\\n- conductor'}], 'input_ids': tensor([   27,    91,  7220,    91,    29,   198, 32080,  9156,    25,   317,\n",
      "         2563,  3650,   318,   257,  1295,   326, 16015, 33298,    11, 16162,\n",
      "        34274,   290,   407,   538,  5643,    13,  1881,   460,  2822,   422,\n",
      "          257,  2563,  3650,    13,   198, 30349,  3650,   198, 24361,   329,\n",
      "          428,  9156,    25,  6350,   561,   345,  2822,   257,   407, 47852,\n",
      "         1863,  1735, 33298,   393, 16162, 34274,    30,   198, 29046,    25,\n",
      "          198,    12,   736, 10000,   198,    12,  3249,   198,    12,  4675,\n",
      "        18583,   198,    12,  2563,  3650,   198,    12,  1524,   628,   198,\n",
      "        11187,   291,  6624,    29,   367, 14132,  1724,   262,  3842,   286,\n",
      "         1016,   329,   890, 11114,  2592,  1973,  1499,   329,  5517,   290,\n",
      "         9482,  1256,   517, 14653,   583,  1711,   621,  6155,    13,  1475,\n",
      "         2798,  2696,  1394,  1767,  4197,   290,   523, 24522,   318,   262,\n",
      "         1738,   329,  5517,    13,   198,  1069, 23697,   198, 24361,  6624,\n",
      "           29,  1867,   318,   257,  1738,   329, 24522,    30,   198, 29046,\n",
      "           25,   198,    12,  8855,   198,    12,  6155,   198,    12,  5033,\n",
      "        10032,   198,    12, 18787,   198,    12,  5517,   628,   198,  1174,\n",
      "        11204,  1028,   257,  2551,  1724,  4769,   281,  6697,  2292,   286,\n",
      "          262,  2551,    13, 11204,  6697,   286,  6697,  1724,   852,   329,\n",
      "           13,   198, 11873,   329,   198,  1174,  3409,   373,  1028, 33461,\n",
      "          338,  2551,    13,  5689,   373,   262,  6697,    13,  1867,   373,\n",
      "         5689,    11,  5115,   326,  2551,    30,   198, 29046,    25,   198,\n",
      "           12,   852,   329,   198,    12,  1497,   422,   198,    12,  5929,\n",
      "          198,    12,   329,   393,   198,    12, 17856,   628,   198, 32080,\n",
      "         9156,    25, 34017,   318,   257,  1448,   286, 21543,  1023,    13,\n",
      "        19829, 10212, 42504,   290,   584,  4898,  2344, 12834,  1282,   739,\n",
      "        12834,    13,   198,   273,  2395, 12044,   198, 24361,   329,   428,\n",
      "         9156,    25,  6350,   561,   534,  3285,   257, 12702, 10212, 42504,\n",
      "         1863,  1735,   584,  4898,  2344, 12834,    30,   198, 29046,    25,\n",
      "          198,    12, 27296,  2156,   198,    12,  1524,  4097,   198,    12,\n",
      "         2647,  3650,   198,    12, 40095,   198,    12, 12436, 22549,   628,\n",
      "          198, 11187,   291,  6624,    29,   978, 37711,   661,   466,   407,\n",
      "         2883,  1664,   286,   584,   661,   290,   651, 17533,    13,   198,\n",
      "        11873, 17533,   198, 24361,  6624,    29,  1002,  2130,   318,   435,\n",
      "        37711,   290,  2116,    12, 18049,    11,   644,   318,   511,  1998,\n",
      "          618,  3249,   661,    30,   198, 29046,    25,   198,    12,  5503,\n",
      "          198,    12, 10927,  1108,   198,    12,  8212,   198,    12,   852,\n",
      "        17533,   198,    12,  1642,   649,  2460,   628,   198, 11187,   291,\n",
      "         6624,    29,  1649,  1048,   836,   470,   423,  3725,   286,  2647,\n",
      "           11,   339,   655,   277,  2509,   262,  2647,  1492,  2427,   286,\n",
      "         2712,  2647,   422,   340,    13,  1675,  1100,   262, 33274,   286,\n",
      "         2647,  4583,  1492,  1048,  1276,   423,  3725,   286,  2647,    13,\n",
      "          198, 45066,   286,  2647,   198, 24361,  6624,    29,   198,    27,\n",
      "           91,   562, 10167,    91,    29,   198,  1532,   345,   389,   277,\n",
      "        41367,   832,   257,  2647,  1492,   618,   345,   815,   307,  2712,\n",
      "           11,   345,  2192, 17666,   423,    30,   198, 29046,    25,   198,\n",
      "           12,  3357,   198,    12, 50003,   198,    12, 21696,   640,   198,\n",
      "           12,  3725,   286,  2647,   198,    12, 39206, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  1532,   345,   389,   277,\n",
      "        41367,   832,   257,  2647,  1492,   618,   345,   815,   307,  2712,\n",
      "           11,   345,  2192, 17666,   423,    30,   198, 29046,    25,   198,\n",
      "           12,  3357,   198,    12, 50003,   198,    12, 21696,   640,   198,\n",
      "           12,  3725,   286,  2647,   198,    12, 39206, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'dataset': 'flan_v2', 'id': 'flan_v2_25674', 'messages': [{'role': 'user', 'content': 'Tullar et Berchielli (1982) ont galement constat que les jeunes se dplaaient sur de plus grandes distances que les adultes.\\n\\nCould you please translate this to English?\\n'}, {'role': 'assistant', 'content': 'Tullar and Berchielli (1982) also found that juveniles moved greater distances than adults.'}], 'input_ids': tensor([   27,    91,  7220,    91,    29,   198,    51,   724,   283,  2123,\n",
      "         4312,  3043, 15516,   357, 30763,     8, 39585, 38251, 13528,   972,\n",
      "         1500,   265,  2634,  8358, 10287, 11223,  4015,   384, 39073,   489,\n",
      "           64, 50041,  1153,   969,   390,  5556,  4490,   274, 18868,  8358,\n",
      "        10287,  4044,   274,    13,   198,   198, 23722,   345,  3387, 15772,\n",
      "          428,   284,  3594,    30,   198,    27,    91,   562, 10167,    91,\n",
      "           29,   198,    51,   724,   283,   290,  4312,  3043, 15516,   357,\n",
      "        30763,     8,   635,  1043,   326, 48660,  3888,  3744, 18868,   621,\n",
      "         6490,    13, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,    51,   724,   283,   290,  4312,  3043, 15516,   357,\n",
      "        30763,     8,   635,  1043,   326, 48660,  3888,  3744, 18868,   621,\n",
      "         6490,    13, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "08/01/2023 17:31:02 - INFO - datasets.arrow_dataset - Caching indices mapping at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a5a973b6b5ca7d74.arrow\n",
      "{'dataset': 'oasst1', 'id': 'oasst1_55634', 'messages': [{'role': 'user', 'content': \"What are the main factors that contribute to a real estate property's value?\"}, {'role': 'assistant', 'content': \"The value of a real estate property is determined by a variety of factors, including:\\n\\n1. Location: Properties located in desirable neighborhoods, close to amenities such as schools, shopping, and transportation, tend to have higher values.\\n\\n2. Size and layout: Properties with larger square footage and a well-designed layout tend to have higher values.\\n\\n3. Age and condition: Properties that are well-maintained and updated are typically worth more than those that are older and in need of repairs.\\n\\n4. Economic factors: The local economy, job market, and housing demand can all have an impact on property values.\\n\\n5. Amenities and features: Properties with desirable amenities, such as a pool, garage, or updated kitchen, are typically worth more than those without these features.\\n\\n6. Zoning regulations: Zoning regulations and building codes can affect the potential use and development of a property, and therefore, its value.\\n\\n7. Market conditions: The overall real estate market, including supply and demand, can have a significant impact on property values.\\n\\n8. Environmental factors: Properties located near sources of pollution, such as a landfill or a factory, tend to have lower values.\\n\\nThese are some of the main factors that contribute to a real estate property's value, but it's important to keep in mind that the relative importance of each factor can vary depending on the specific property and market.\"}, {'role': 'user', 'content': \"Is there something that I, as a home owner, can do to increase the value of my property? I'm specifically asking for things that I can do that will make selling the property afterwards more profitable.\"}, {'role': 'assistant', 'content': 'As a single person your influence on things like the economy or political decisions are likely negligible and therefore those things cannot be influenced by you in any significant way. Naturally it is also impossible to change the location and size of your property. Something tangible that you can do to make selling your property more profitable is to do any necessary repairs and renovations beforehand rather than to sell your property as-is. Buyers tend to overestimate the costs of repairs and renovations so doing those things yourself will be cheaper than the \"cost\" imposed on you when you eventually sell the property. For maximum profitability you can try to do as much of the work as possible yourself.\\n\\nYou can also try to sell your property directly to another person instead of going through a real estate agent that will want a cut of the selling price. Note that the specifics of how real estate agents operate will depend on your region. Also selling the property on your own is not always the better option. A real estate agent may be able to find a buyer who is willing to pay more and thus increase your profits even though they are taking a cut. If you do decide to sell your property on your own you should learn how to negotiate properly since even a 1% difference in sales price will make a large difference with something as valuable as real estate.'}], 'input_ids': tensor([   27,    91,  7220,    91,    29,   198,  2061,   389,   262,  1388,\n",
      "         5087,   326,  8676,   284,   257,  1103,  7964,  3119,   338,  1988,\n",
      "           30,   198,    27,    91,   562, 10167,    91,    29,   198,   464,\n",
      "         1988,   286,   257,  1103,  7964,  3119,   318,  5295,   416,   257,\n",
      "         4996,   286,  5087,    11,  1390,    25,   198,   198,    16,    13,\n",
      "        13397,    25, 24946,  5140,   287, 18763, 14287,    11,  1969,   284,\n",
      "        35468,   884,   355,  4266,    11,  9735,    11,   290,  9358,    11,\n",
      "         4327,   284,   423,  2440,  3815,    13,   198,   198,    17,    13,\n",
      "        12849,   290, 12461,    25, 24946,   351,  4025,  6616,  9640,   290,\n",
      "          257,   880,    12, 30473, 12461,  4327,   284,   423,  2440,  3815,\n",
      "           13,   198,   198,    18,    13,  7129,   290,  4006,    25, 24946,\n",
      "          326,   389,   880,    12,    76,  2913,  1328,   290,  6153,   389,\n",
      "         6032,  2861,   517,   621,   883,   326,   389,  4697,   290,   287,\n",
      "          761,   286, 20097,    13,   198,   198,    19,    13, 11279,  5087,\n",
      "           25,   383,  1957,  3773,    11,  1693,  1910,    11,   290,  5627,\n",
      "         3512,   460,   477,   423,   281,  2928,   319,  3119,  3815,    13,\n",
      "          198,   198,    20,    13, 34717,   871,   290,  3033,    25, 24946,\n",
      "          351, 18763, 35468,    11,   884,   355,   257,  5933,    11, 15591,\n",
      "           11,   393,  6153,  9592,    11,   389,  6032,  2861,   517,   621,\n",
      "          883,  1231,   777,  3033,    13,   198,   198,    21,    13,  1168,\n",
      "        12484,  6647,    25,  1168, 12484,  6647,   290,  2615, 12416,   460,\n",
      "         2689,   262,  2785,   779,   290,  2478,   286,   257,  3119,    11,\n",
      "          290,  4361,    11,   663,  1988,    13,   198,   198,    22,    13,\n",
      "         5991,  3403,    25,   383,  4045,  1103,  7964,  1910,    11,  1390,\n",
      "         5127,   290,  3512,    11,   460,   423,   257,  2383,  2928,   319,\n",
      "         3119,  3815,    13,   198,   198,    23,    13, 13272,  5087,    25,\n",
      "        24946,  5140,  1474,  4237,   286, 12231,    11,   884,   355,   257,\n",
      "        47712,   393,   257,  8860,    11,  4327,   284,   423,  2793,  3815,\n",
      "           13,   198,   198,  4711,   389,   617,   286,   262,  1388,  5087,\n",
      "          326,  8676,   284,   257,  1103,  7964,  3119,   338,  1988,    11,\n",
      "          475,   340,   338,  1593,   284,  1394,   287,  2000,   326,   262,\n",
      "         3585,  6817,   286,  1123,  5766,   460,  7565,  6906,   319,   262,\n",
      "         2176,  3119,   290,  1910,    13, 50256,   198,    27,    91,  7220,\n",
      "           91,    29,   198,  3792,   612,  1223,   326,   314,    11,   355,\n",
      "          257,  1363,  4870,    11,   460,   466,   284,  2620,   262,  1988,\n",
      "          286,   616,  3119,    30,   314,  1101,  5734,  4737,   329,  1243,\n",
      "          326,   314,   460,   466,   326,   481,   787,  6301,   262,  3119,\n",
      "        12979,   517, 17967,    13,   198,    27,    91,   562, 10167,    91,\n",
      "           29,   198,  1722,   257,  2060,  1048,   534,  4588,   319,  1243,\n",
      "          588,   262,  3773,   393,  1964,  5370,   389,  1884, 36480,   290,\n",
      "         4361,   883,  1243,  2314,   307, 12824,   416,   345,   287,   597,\n",
      "         2383,   835,    13, 30413,   340,   318,   635,  5340,   284,  1487,\n",
      "          262,  4067,   290,  2546,   286,   534,  3119,    13, 13742, 24607,\n",
      "          326,   345,   460,   466,   284,   787,  6301,   534,  3119,   517,\n",
      "        17967,   318,   284,   466,   597,  3306, 20097,   290, 44481, 27091,\n",
      "         2138,   621,   284,  3677,   534,  3119,   355,    12,   271,    13,\n",
      "        11763,   364,  4327,   284, 41850,  1920,   262,  3484,   286, 20097,\n",
      "          290, 44481,   523,  1804,   883,  1243,  3511,   481,   307, 11721,\n",
      "          621,   262,   366, 15805,     1, 10893,   319,   345,   618,   345,\n",
      "         4191,  3677,   262,  3119,    13,  1114,  5415, 37129,   345,   460,\n",
      "         1949,   284,   466,   355,   881,   286,   262,   670,   355,  1744,\n",
      "         3511,    13,   198,   198,  1639,   460,   635,  1949,   284,  3677,\n",
      "          534,  3119,  3264,   284,  1194,  1048,  2427,   286,  1016,   832,\n",
      "          257,  1103,  7964,  5797,   326,   481,   765,   257,  2005,   286,\n",
      "          262,  6301,  2756,    13,  5740,   326,   262, 23514,   286,   703,\n",
      "         1103,  7964,  6554,  8076,   481,  4745,   319,   534,  3814,    13,\n",
      "         4418,  6301,   262,  3119,   319,   534,   898,   318,   407,  1464,\n",
      "          262,  1365,  3038,    13,   317,  1103,  7964,  5797,   743,   307,\n",
      "         1498,   284,  1064,   257, 17872,   508,   318,  4684,   284,  1414,\n",
      "          517,   290,  4145,  2620,   534, 10177,   772,   996,   484,   389,\n",
      "         2263,   257,  2005,    13,  1002,   345,   466,  5409,   284,  3677,\n",
      "          534,  3119,   319,   534,   898,   345,   815,  2193,   703,   284,\n",
      "        16674,  6105,  1201,   772,   257,   352,     4,  3580,   287,  4200,\n",
      "         2756,   481,   787,   257,  1588,  3580,   351,  1223,   355,  8119,\n",
      "          355,  1103,  7964,    13, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   464,\n",
      "         1988,   286,   257,  1103,  7964,  3119,   318,  5295,   416,   257,\n",
      "         4996,   286,  5087,    11,  1390,    25,   198,   198,    16,    13,\n",
      "        13397,    25, 24946,  5140,   287, 18763, 14287,    11,  1969,   284,\n",
      "        35468,   884,   355,  4266,    11,  9735,    11,   290,  9358,    11,\n",
      "         4327,   284,   423,  2440,  3815,    13,   198,   198,    17,    13,\n",
      "        12849,   290, 12461,    25, 24946,   351,  4025,  6616,  9640,   290,\n",
      "          257,   880,    12, 30473, 12461,  4327,   284,   423,  2440,  3815,\n",
      "           13,   198,   198,    18,    13,  7129,   290,  4006,    25, 24946,\n",
      "          326,   389,   880,    12,    76,  2913,  1328,   290,  6153,   389,\n",
      "         6032,  2861,   517,   621,   883,   326,   389,  4697,   290,   287,\n",
      "          761,   286, 20097,    13,   198,   198,    19,    13, 11279,  5087,\n",
      "           25,   383,  1957,  3773,    11,  1693,  1910,    11,   290,  5627,\n",
      "         3512,   460,   477,   423,   281,  2928,   319,  3119,  3815,    13,\n",
      "          198,   198,    20,    13, 34717,   871,   290,  3033,    25, 24946,\n",
      "          351, 18763, 35468,    11,   884,   355,   257,  5933,    11, 15591,\n",
      "           11,   393,  6153,  9592,    11,   389,  6032,  2861,   517,   621,\n",
      "          883,  1231,   777,  3033,    13,   198,   198,    21,    13,  1168,\n",
      "        12484,  6647,    25,  1168, 12484,  6647,   290,  2615, 12416,   460,\n",
      "         2689,   262,  2785,   779,   290,  2478,   286,   257,  3119,    11,\n",
      "          290,  4361,    11,   663,  1988,    13,   198,   198,    22,    13,\n",
      "         5991,  3403,    25,   383,  4045,  1103,  7964,  1910,    11,  1390,\n",
      "         5127,   290,  3512,    11,   460,   423,   257,  2383,  2928,   319,\n",
      "         3119,  3815,    13,   198,   198,    23,    13, 13272,  5087,    25,\n",
      "        24946,  5140,  1474,  4237,   286, 12231,    11,   884,   355,   257,\n",
      "        47712,   393,   257,  8860,    11,  4327,   284,   423,  2793,  3815,\n",
      "           13,   198,   198,  4711,   389,   617,   286,   262,  1388,  5087,\n",
      "          326,  8676,   284,   257,  1103,  7964,  3119,   338,  1988,    11,\n",
      "          475,   340,   338,  1593,   284,  1394,   287,  2000,   326,   262,\n",
      "         3585,  6817,   286,  1123,  5766,   460,  7565,  6906,   319,   262,\n",
      "         2176,  3119,   290,  1910,    13, 50256,   198,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  1722,   257,  2060,  1048,   534,  4588,   319,  1243,\n",
      "          588,   262,  3773,   393,  1964,  5370,   389,  1884, 36480,   290,\n",
      "         4361,   883,  1243,  2314,   307, 12824,   416,   345,   287,   597,\n",
      "         2383,   835,    13, 30413,   340,   318,   635,  5340,   284,  1487,\n",
      "          262,  4067,   290,  2546,   286,   534,  3119,    13, 13742, 24607,\n",
      "          326,   345,   460,   466,   284,   787,  6301,   534,  3119,   517,\n",
      "        17967,   318,   284,   466,   597,  3306, 20097,   290, 44481, 27091,\n",
      "         2138,   621,   284,  3677,   534,  3119,   355,    12,   271,    13,\n",
      "        11763,   364,  4327,   284, 41850,  1920,   262,  3484,   286, 20097,\n",
      "          290, 44481,   523,  1804,   883,  1243,  3511,   481,   307, 11721,\n",
      "          621,   262,   366, 15805,     1, 10893,   319,   345,   618,   345,\n",
      "         4191,  3677,   262,  3119,    13,  1114,  5415, 37129,   345,   460,\n",
      "         1949,   284,   466,   355,   881,   286,   262,   670,   355,  1744,\n",
      "         3511,    13,   198,   198,  1639,   460,   635,  1949,   284,  3677,\n",
      "          534,  3119,  3264,   284,  1194,  1048,  2427,   286,  1016,   832,\n",
      "          257,  1103,  7964,  5797,   326,   481,   765,   257,  2005,   286,\n",
      "          262,  6301,  2756,    13,  5740,   326,   262, 23514,   286,   703,\n",
      "         1103,  7964,  6554,  8076,   481,  4745,   319,   534,  3814,    13,\n",
      "         4418,  6301,   262,  3119,   319,   534,   898,   318,   407,  1464,\n",
      "          262,  1365,  3038,    13,   317,  1103,  7964,  5797,   743,   307,\n",
      "         1498,   284,  1064,   257, 17872,   508,   318,  4684,   284,  1414,\n",
      "          517,   290,  4145,  2620,   534, 10177,   772,   996,   484,   389,\n",
      "         2263,   257,  2005,    13,  1002,   345,   466,  5409,   284,  3677,\n",
      "          534,  3119,   319,   534,   898,   345,   815,  2193,   703,   284,\n",
      "        16674,  6105,  1201,   772,   257,   352,     4,  3580,   287,  4200,\n",
      "         2756,   481,   787,   257,  1588,  3580,   351,  1223,   355,  8119,\n",
      "          355,  1103,  7964,    13, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'dataset': 'oasst1', 'id': 'oasst1_14536', 'messages': [{'role': 'user', 'content': 'Necesito saber como medir las pulgadas de la rueda de mi bicicleta pero no tengo ni idea, me puedes indicar cmo hacerlo? Tengo un metro en cm :('}, {'role': 'assistant', 'content': 'Asumo que lo que quieres obtener es el radio de la rueda de tu bicicleta, es as?\\n\\nEn caso afirmativo, nicamente tendras que tomar tu metro y medir desde el centro de la rueda (lo que se llama buje) hasta el extremo de esta. Dependiendo de para qu lo quieras, puedes tomar la medida hasta el aro de la llanta o hasta el neumtico. Una vez tengas esa medida en centmetros, slo tienes que dividir el valor que te haya salido entre 2.54 y el resultado ser el radio de la rueda en pulgadas. \\n\\nSi, por el contrario, lo que queras era obtener la longitud de la circunferencia de la rueda, slo tienes que aadir un paso extra al proceso que acabo de describir. Para obtener el permetro de la rueda, tendrs que utilizar la siguiente frmula:\\n\\nP = 2r,\\n\\ndonde P es el permetro, r es el radio (que acabas de medir previamente y pasado a pulgadas) y  (o pi) es un nmero irracional que puedes aproximar como 3.14.\\n\\nEl resultado de esa multiplicacin ser la longitud en pulgadas de la circunferencia de la rueda de tu bicicleta.'}, {'role': 'user', 'content': 'Ya lo med , el resultado en pulgadas del tamao de mi rueda es **25.8** (creo ) Qu tipo de neumtico debera comprar? '}, {'role': 'assistant', 'content': 'Si la medida que tomaste es la del radio de la rueda y dividiste esa medida entre 2.54 para obtener la medida en pulgadas, entonces el tamao de tu rueda sera de 10.16 pulgadas. Es importante que tengas en cuenta que la medida de la rueda se refiere al tamao de la llanta, no del neumtico. Por lo tanto, lo que necesitas saber es la medida del neumtico que le corresponde a esa llanta.\\n\\nPara saber qu medida de neumtico necesitas, tendrs que buscar la medida impresa en el propio neumtico. Normalmente, la medida se encuentra en el flanco del neumtico y est compuesta por tres nmeros separados por una \"x\". Por ejemplo, puede aparecer algo as como \"26x2.0\". Los dos primeros nmeros indican el dimetro del neumtico en pulgadas (en este ejemplo, 26 pulgadas) y el ltimo nmero indica el ancho del neumtico en pulgadas (en este caso, 2.0 pulgadas).\\n\\nSi tienes problemas para encontrar la medida en el neumtico, otra opcin es buscar el modelo y marca de tu bicicleta por internet y comprobar la medida de neumtico que se recomienda para ese modelo especfico.\\n\\nEs importante que tengas en cuenta que la medida del neumtico puede variar dependiendo del tipo de bicicleta y del uso que le vayas a dar. Por ejemplo, para una bicicleta de montaa necesitars un neumtico con ms agarre y resistencia que para una bicicleta de carretera, que normalmente tiene neumticos ms delgados y lisos para reducir la resistencia al aire.'}], 'input_ids': tensor([  27,   91, 7220,  ..., 1548, 8326,  390]), 'labels': tensor([-100, -100, -100,  ..., 1548, 8326,  390]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1])}\n",
      "{'dataset': 'oasst1', 'id': 'oasst1_6514', 'messages': [{'role': 'user', 'content': 'Buenos das, puedes darme una lista en formato json de los principales personajes del seor de los anillos?'}, {'role': 'assistant', 'content': 'Claro, aqu tienes una lista en formato JSON de algunos de los personajes principales de \"El Seor de los Anillos\":\\n\\n[  {    \"name\": \"Frodo Bolsn\",    \"race\": \"Hobbit\",    \"gender\": \"Male\",    \"role\": \"Protagonist\"  },  {    \"name\": \"Samwise Gamyi\",    \"race\": \"Hobbit\",    \"gender\": \"Male\",    \"role\": \"Companion of Frodo\"  },  {    \"name\": \"Aragorn\",    \"race\": \"Human\",    \"gender\": \"Male\",    \"role\": \"Strider\"  },  {    \"name\": \"Legolas\",    \"race\": \"Elf\",    \"gender\": \"Male\",    \"role\": \"Companion of Frodo\"  },  {    \"name\": \"Gandalf\",    \"race\": \"Maia\",    \"gender\": \"Male\",    \"role\": \"Wizard\"  },  {    \"name\": \"Boromir\",    \"race\": \"Human\",    \"gender\": \"Male\",    \"role\": \"Companion of Frodo\"  },  {    \"name\": \"Galadriel\",    \"race\": \"Elf\",    \"gender\": \"Female\",    \"role\": \"Lady of Lothlrien\"  },  {    \"name\": \"Gollum\",    \"race\": \"Stoor Hobbit\",    \"gender\": \"Male\",    \"role\": \"Antagonist\"  }]\\n\\nEsta lista incluye solo algunos de los personajes principales, y puedes agregar ms personajes si lo deseas.'}], 'input_ids': tensor([   27,    91,  7220,    91,    29,   198, 38374, 28380,   288,  8836,\n",
      "          292,    11,  1587,   123,    79,  1739,   274,   288,   283,  1326,\n",
      "          555,    64,  1351,    64,   551,  1296,  5549, 33918,   390, 22346,\n",
      "        26303,  2040,  1048,  1228,   274,  1619,   384, 12654,   273,   390,\n",
      "        22346,   281,   359,   418,    30,   198,    27,    91,   562, 10167,\n",
      "           91,    29,   198,  2601, 12022,    11, 14839,  8836,   256,  2013,\n",
      "          274,   555,    64,  1351,    64,   551,  1296,  5549, 19449,   390,\n",
      "          435,  7145,   418,   390, 22346,  1048,  1228,   274, 26303,  2040,\n",
      "          390,   366,  9527,  1001, 12654,   273,   390, 22346,  1052,   359,\n",
      "          418,  1298,   198,   198,    58,   220,  1391,   220,   220,   220,\n",
      "          366,  3672,  1298,   366,    37,   305,  4598,   347, 10220, 18840,\n",
      "         1600,   220,   220,   220,   366, 16740,  1298,   366,    39,   672,\n",
      "         2545,  1600,   220,   220,   220,   366,  8388,  1298,   366, 25486,\n",
      "         1600,   220,   220,   220,   366, 18090,  1298,   366, 19703, 15239,\n",
      "            1,   220,  8964,   220,  1391,   220,   220,   220,   366,  3672,\n",
      "         1298,   366, 16305,  3083, 14014, 48111,  1600,   220,   220,   220,\n",
      "          366, 16740,  1298,   366,    39,   672,  2545,  1600,   220,   220,\n",
      "          220,   366,  8388,  1298,   366, 25486,  1600,   220,   220,   220,\n",
      "          366, 18090,  1298,   366, 41309,   295,   286,  9734,  4598,     1,\n",
      "          220,  8964,   220,  1391,   220,   220,   220,   366,  3672,  1298,\n",
      "          366,  3163,   363,  1211,  1600,   220,   220,   220,   366, 16740,\n",
      "         1298,   366, 20490,  1600,   220,   220,   220,   366,  8388,  1298,\n",
      "          366, 25486,  1600,   220,   220,   220,   366, 18090,  1298,   366,\n",
      "        13290,  1304,     1,   220,  8964,   220,  1391,   220,   220,   220,\n",
      "          366,  3672,  1298,   366, 11484, 12456,  1600,   220,   220,   220,\n",
      "          366, 16740,  1298,   366, 46995,  1600,   220,   220,   220,   366,\n",
      "         8388,  1298,   366, 25486,  1600,   220,   220,   220,   366, 18090,\n",
      "         1298,   366, 41309,   295,   286,  9734,  4598,     1,   220,  8964,\n",
      "          220,  1391,   220,   220,   220,   366,  3672,  1298,   366,    38,\n",
      "          392,  1604,  1600,   220,   220,   220,   366, 16740,  1298,   366,\n",
      "        21467,   544,  1600,   220,   220,   220,   366,  8388,  1298,   366,\n",
      "        25486,  1600,   220,   220,   220,   366, 18090,  1298,   366,    54,\n",
      "         8669,     1,   220,  8964,   220,  1391,   220,   220,   220,   366,\n",
      "         3672,  1298,   366,    33,   273,   296,   343,  1600,   220,   220,\n",
      "          220,   366, 16740,  1298,   366, 20490,  1600,   220,   220,   220,\n",
      "          366,  8388,  1298,   366, 25486,  1600,   220,   220,   220,   366,\n",
      "        18090,  1298,   366, 41309,   295,   286,  9734,  4598,     1,   220,\n",
      "         8964,   220,  1391,   220,   220,   220,   366,  3672,  1298,   366,\n",
      "        26552,   324, 11719,  1600,   220,   220,   220,   366, 16740,  1298,\n",
      "          366, 46995,  1600,   220,   220,   220,   366,  8388,  1298,   366,\n",
      "        27273,  1600,   220,   220,   220,   366, 18090,  1298,   366, 38887,\n",
      "          286,   406,   849,    75, 10205, 15355,     1,   220,  8964,   220,\n",
      "         1391,   220,   220,   220,   366,  3672,  1298,   366,    38,   692,\n",
      "          388,  1600,   220,   220,   220,   366, 16740,  1298,   366,  1273,\n",
      "         2675, 43877,  1600,   220,   220,   220,   366,  8388,  1298,   366,\n",
      "        25486,  1600,   220,   220,   220,   366, 18090,  1298,   366, 13217,\n",
      "        15239,     1,   220,  1782,    60,   198,   198, 22362,    64,  1351,\n",
      "           64, 13358,  4669,    68, 12199,   435,  7145,   418,   390, 22346,\n",
      "         1048,  1228,   274, 26303,  2040,    11,   331,   279,  1739,   274,\n",
      "          556,  2301,   283,   285, 40138,  1048,  1228,   274, 33721,  2376,\n",
      "          748, 30412,    13, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  2601, 12022,    11, 14839,  8836,   256,  2013,\n",
      "          274,   555,    64,  1351,    64,   551,  1296,  5549, 19449,   390,\n",
      "          435,  7145,   418,   390, 22346,  1048,  1228,   274, 26303,  2040,\n",
      "          390,   366,  9527,  1001, 12654,   273,   390, 22346,  1052,   359,\n",
      "          418,  1298,   198,   198,    58,   220,  1391,   220,   220,   220,\n",
      "          366,  3672,  1298,   366,    37,   305,  4598,   347, 10220, 18840,\n",
      "         1600,   220,   220,   220,   366, 16740,  1298,   366,    39,   672,\n",
      "         2545,  1600,   220,   220,   220,   366,  8388,  1298,   366, 25486,\n",
      "         1600,   220,   220,   220,   366, 18090,  1298,   366, 19703, 15239,\n",
      "            1,   220,  8964,   220,  1391,   220,   220,   220,   366,  3672,\n",
      "         1298,   366, 16305,  3083, 14014, 48111,  1600,   220,   220,   220,\n",
      "          366, 16740,  1298,   366,    39,   672,  2545,  1600,   220,   220,\n",
      "          220,   366,  8388,  1298,   366, 25486,  1600,   220,   220,   220,\n",
      "          366, 18090,  1298,   366, 41309,   295,   286,  9734,  4598,     1,\n",
      "          220,  8964,   220,  1391,   220,   220,   220,   366,  3672,  1298,\n",
      "          366,  3163,   363,  1211,  1600,   220,   220,   220,   366, 16740,\n",
      "         1298,   366, 20490,  1600,   220,   220,   220,   366,  8388,  1298,\n",
      "          366, 25486,  1600,   220,   220,   220,   366, 18090,  1298,   366,\n",
      "        13290,  1304,     1,   220,  8964,   220,  1391,   220,   220,   220,\n",
      "          366,  3672,  1298,   366, 11484, 12456,  1600,   220,   220,   220,\n",
      "          366, 16740,  1298,   366, 46995,  1600,   220,   220,   220,   366,\n",
      "         8388,  1298,   366, 25486,  1600,   220,   220,   220,   366, 18090,\n",
      "         1298,   366, 41309,   295,   286,  9734,  4598,     1,   220,  8964,\n",
      "          220,  1391,   220,   220,   220,   366,  3672,  1298,   366,    38,\n",
      "          392,  1604,  1600,   220,   220,   220,   366, 16740,  1298,   366,\n",
      "        21467,   544,  1600,   220,   220,   220,   366,  8388,  1298,   366,\n",
      "        25486,  1600,   220,   220,   220,   366, 18090,  1298,   366,    54,\n",
      "         8669,     1,   220,  8964,   220,  1391,   220,   220,   220,   366,\n",
      "         3672,  1298,   366,    33,   273,   296,   343,  1600,   220,   220,\n",
      "          220,   366, 16740,  1298,   366, 20490,  1600,   220,   220,   220,\n",
      "          366,  8388,  1298,   366, 25486,  1600,   220,   220,   220,   366,\n",
      "        18090,  1298,   366, 41309,   295,   286,  9734,  4598,     1,   220,\n",
      "         8964,   220,  1391,   220,   220,   220,   366,  3672,  1298,   366,\n",
      "        26552,   324, 11719,  1600,   220,   220,   220,   366, 16740,  1298,\n",
      "          366, 46995,  1600,   220,   220,   220,   366,  8388,  1298,   366,\n",
      "        27273,  1600,   220,   220,   220,   366, 18090,  1298,   366, 38887,\n",
      "          286,   406,   849,    75, 10205, 15355,     1,   220,  8964,   220,\n",
      "         1391,   220,   220,   220,   366,  3672,  1298,   366,    38,   692,\n",
      "          388,  1600,   220,   220,   220,   366, 16740,  1298,   366,  1273,\n",
      "         2675, 43877,  1600,   220,   220,   220,   366,  8388,  1298,   366,\n",
      "        25486,  1600,   220,   220,   220,   366, 18090,  1298,   366, 13217,\n",
      "        15239,     1,   220,  1782,    60,   198,   198, 22362,    64,  1351,\n",
      "           64, 13358,  4669,    68, 12199,   435,  7145,   418,   390, 22346,\n",
      "         1048,  1228,   274, 26303,  2040,    11,   331,   279,  1739,   274,\n",
      "          556,  2301,   283,   285, 40138,  1048,  1228,   274, 33721,  2376,\n",
      "          748, 30412,    13, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Attempted unscale_ but _scale is None.  This may indicate your script did not use scaler.scale(loss or outputs) earlier in the iteration.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[221], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m last_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m last_checkpoint\n\u001b[0;32m----> 8\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()  \u001b[38;5;66;03m# Saves the tokenizer too for easy upload\u001b[39;00m\n\u001b[1;32m     11\u001b[0m metrics \u001b[38;5;241m=\u001b[39m train_result\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py:1532\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1527\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1529\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1531\u001b[0m )\n\u001b[0;32m-> 1532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py:1863\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   1859\u001b[0m             amp\u001b[38;5;241m.\u001b[39mmaster_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer),\n\u001b[1;32m   1860\u001b[0m             args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   1861\u001b[0m         )\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1863\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1866\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# Optimizer step\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py:1925\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   1922\u001b[0m     \u001b[38;5;66;03m# `accelerator.backward(loss)` is doing that automatically. Therefore, its implementation is not needed\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m     \u001b[38;5;66;03m# We cannot return the gradient norm because DeepSpeed does it.\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1925\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(parameters, max_norm, norm_type\u001b[38;5;241m=\u001b[39mnorm_type)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py:1888\u001b[0m, in \u001b[0;36mAccelerator.unscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   1886\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(opt, AcceleratedOptimizer):\n\u001b[1;32m   1887\u001b[0m     opt \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39moptimizer\n\u001b[0;32m-> 1888\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:270\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enabled:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_scale_growth_tracker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munscale_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m optimizer_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_per_optimizer_states[\u001b[38;5;28mid\u001b[39m(optimizer)]\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m OptState\u001b[38;5;241m.\u001b[39mUNSCALED:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:142\u001b[0m, in \u001b[0;36mGradScaler._check_scale_growth_tracker\u001b[0;34m(self, funcname)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_scale_growth_tracker\u001b[39m(\u001b[38;5;28mself\u001b[39m, funcname) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    141\u001b[0m     fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis may indicate your script did not use scaler.scale(loss or outputs) earlier in the iteration.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m but _scale is None.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(funcname) \u001b[38;5;241m+\u001b[39m fix\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_growth_tracker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m but _growth_tracker is None.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(funcname) \u001b[38;5;241m+\u001b[39m fix\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_growth_tracker)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Attempted unscale_ but _scale is None.  This may indicate your script did not use scaler.scale(loss or outputs) earlier in the iteration."
     ]
    }
   ],
   "source": [
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "\n",
    "    if training_args.reweight_domains:\n",
    "        avg_domain_weights_dict = {}\n",
    "        for i in range(len(model.avg_domain_weights)):\n",
    "            domain_name = domain_list[i]\n",
    "            metrics[f'avg_domain_weight:{domain_name}'] = model.avg_domain_weights[i].item()\n",
    "            avg_domain_weights_dict[domain_name] = model.avg_domain_weights[i].item()\n",
    "\n",
    "        # save avg domain weights to json\n",
    "        avg_domain_weights_file = Path(training_args.output_dir) / 'avg_domain_weights.json'\n",
    "        with open(avg_domain_weights_file, 'w') as f:\n",
    "            json.dump(avg_domain_weights_dict, f, indent=2)\n",
    "\n",
    "        # also save to configs dir\n",
    "        config_dict = {\"train_domain_weights\": avg_domain_weights_dict,\n",
    "                       \"eval_domain_weights\": avg_domain_weights_dict}\n",
    "        config_dict_file = Path(__file__).parent.parent / 'configs' / f\"{Path(training_args.output_dir).name}.json\"\n",
    "        with open(config_dict_file, 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "cfb0314c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 17:23:41 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c6ba3d14d25cb207.arrow\n",
      "{'dataset': 'cot', 'id': 'cot_60290', 'messages': [{'role': 'user', 'content': 'QUESTION: Premise: \"A man is squatting down while painting his artwork on a large metal garage door.\"\\nBased on this premise, can we conclude that the hypothesis \"A man paints art on garage door.\" is true?\\nOptions:\\n- yes\\n- it is not possible to tell\\n- no\\n\\nLet\\'s solve it slowly: A man is squatting down while paints art on metal garage door.\\nThe answer is yes.\\n[QUESTION] Premise: \"Woman hiking outside in a vineyard.\"\\nBased on this premise, can we conclude that the hypothesis \"The vines are covered with ripe grapes.\" is true?\\nOptions:\\n- yes\\n- it is not possible to tell\\n- no\\nHiking in a vineyard does not imply the vines are covered with ripe grapes.\\nThe answer is it is not possible to tell.\\nQ: Test for natural language inference.\\nPremise: \"A woman walks through the drizzling rain while shielding herself with her red umbrella.\"\\nHypothesis: \"A woman is out in the rain.\"\\nIs the hypothesis entailed by the premise?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell\\nA: A woman walks through rain means she is out in the rain.\\nThe answer is yes.\\nQUESTION: Premise: \"Two people are waiting for their food.\"\\nHypothesis: \"People are waiting on food.\"\\nDo we know that the hypothesis entailed by the premise?\\n\\nLet\\'s solve it slowly: People who is plural for two people\\'s waiting on their food.\\nThe answer is yes.\\n[QUESTION] Test for natural language inference.\\nPremise: \"A dog swimming though water holding a stick.\"\\nHypothesis: \"A dog is swimming while holding onto a stick in his mouth to bring it back to its owner.\"\\nIs the hypothesis entailed by the premise?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell\\nA dog holding a stick will not always bring it back to its owner.\\nThe answer is it is not possible to tell.\\nQ: Premise: \"A woman sitting outside a small restaurant suddenly throws up her hands in excitement.\"\\nBased on this premise, can we conclude that the hypothesis \"A lady throws up her hands in excitement because she spots her friend.\" is true?\\nOptions:\\n- yes\\n- it is not possible to tell\\n- no\\nA:'}, {'role': 'assistant', 'content': 'A woman sitting outside a small restaurant suddenly throws up her hands in excitement does not indicate that she spots her friend.\\nThe answer is it is not possible to tell.'}], 'input_ids': tensor([   27,    91,  7220,    91,    29,   198, 35780,  2849,    25,  6929,\n",
      "          786,    25,   366,    32,   582,   318, 22713,   889,   866,   981,\n",
      "        12036,   465, 16257,   319,   257,  1588,  6147, 15591,  3420,   526,\n",
      "          198, 15001,   319,   428, 18659,    11,   460,   356, 13796,   326,\n",
      "          262, 14078,   366,    32,   582, 32281,  1242,   319, 15591,  3420,\n",
      "          526,   318,  2081,    30,   198, 29046,    25,   198,    12,  3763,\n",
      "          198,    12,   340,   318,   407,  1744,   284,  1560,   198,    12,\n",
      "          645,   198,   198,  5756,   338,  8494,   340,  6364,    25,   317,\n",
      "          582,   318, 22713,   889,   866,   981, 32281,  1242,   319,  6147,\n",
      "        15591,  3420,    13,   198,   464,  3280,   318,  3763,    13,   198,\n",
      "           58, 35780,  2849,    60,  6929,   786,    25,   366, 48081, 24522,\n",
      "         2354,   287,   257, 17793,  9413,   526,   198, 15001,   319,   428,\n",
      "        18659,    11,   460,   356, 13796,   326,   262, 14078,   366,   464,\n",
      "        44439,   389,  5017,   351, 29036, 36656,   526,   318,  2081,    30,\n",
      "          198, 29046,    25,   198,    12,  3763,   198,    12,   340,   318,\n",
      "          407,  1744,   284,  1560,   198,    12,   645,   198,    39, 14132,\n",
      "          287,   257, 17793,  9413,   857,   407, 20135,   262, 44439,   389,\n",
      "         5017,   351, 29036, 36656,    13,   198,   464,  3280,   318,   340,\n",
      "          318,   407,  1744,   284,  1560,    13,   198,    48,    25,  6208,\n",
      "          329,  3288,  3303, 32278,    13,   198, 24914,   786,    25,   366,\n",
      "           32,  2415, 11114,   832,   262,  1454,  3019,  1359,  6290,   981,\n",
      "        43539,  5223,   351,   607,  2266, 25510,   526,   198, 49926,   313,\n",
      "         8497,    25,   366,    32,  2415,   318,   503,   287,   262,  6290,\n",
      "          526,   198,  3792,   262, 14078,   920,  6255,   416,   262, 18659,\n",
      "           30,   198, 29046,    25,   198,    12,  3763,   198,    12,   645,\n",
      "          198,    12,   340,   318,   407,  1744,   284,  1560,   198,    32,\n",
      "           25,   317,  2415, 11114,   832,  6290,  1724,   673,   318,   503,\n",
      "          287,   262,  6290,    13,   198,   464,  3280,   318,  3763,    13,\n",
      "          198, 35780,  2849,    25,  6929,   786,    25,   366,  7571,   661,\n",
      "          389,  4953,   329,   511,  2057,   526,   198, 49926,   313,  8497,\n",
      "           25,   366,  8061,   389,  4953,   319,  2057,   526,   198,  5211,\n",
      "          356,   760,   326,   262, 14078,   920,  6255,   416,   262, 18659,\n",
      "           30,   198,   198,  5756,   338,  8494,   340,  6364,    25,  4380,\n",
      "          508,   318, 22801,   329,   734,   661,   338,  4953,   319,   511,\n",
      "         2057,    13,   198,   464,  3280,   318,  3763,    13,   198,    58,\n",
      "        35780,  2849,    60,  6208,   329,  3288,  3303, 32278,    13,   198,\n",
      "        24914,   786,    25,   366,    32,  3290, 14899,   996,  1660,  4769,\n",
      "          257,  4859,   526,   198, 49926,   313,  8497,    25,   366,    32,\n",
      "         3290,   318, 14899,   981,  4769,  4291,   257,  4859,   287,   465,\n",
      "         5422,   284,  2222,   340,   736,   284,   663,  4870,   526,   198,\n",
      "         3792,   262, 14078,   920,  6255,   416,   262, 18659,    30,   198,\n",
      "        29046,    25,   198,    12,  3763,   198,    12,   645,   198,    12,\n",
      "          340,   318,   407,  1744,   284,  1560,   198,    32,  3290,  4769,\n",
      "          257,  4859,   481,   407,  1464,  2222,   340,   736,   284,   663,\n",
      "         4870,    13,   198,   464,  3280,   318,   340,   318,   407,  1744,\n",
      "          284,  1560,    13,   198,    48,    25,  6929,   786,    25,   366,\n",
      "           32,  2415,  5586,  2354,   257,  1402,  7072,  6451, 12542,   510,\n",
      "          607,  2832,   287, 14067,   526,   198, 15001,   319,   428, 18659,\n",
      "           11,   460,   356, 13796,   326,   262, 14078,   366,    32, 10846,\n",
      "        12542,   510,   607,  2832,   287, 14067,   780,   673, 10222,   607,\n",
      "         1545,   526,   318,  2081,    30,   198, 29046,    25,   198,    12,\n",
      "         3763,   198,    12,   340,   318,   407,  1744,   284,  1560,   198,\n",
      "           12,   645,   198,    32,    25,   198,    27,    91,   562, 10167,\n",
      "           91,    29,   198,    32,  2415,  5586,  2354,   257,  1402,  7072,\n",
      "         6451, 12542,   510,   607,  2832,   287, 14067,   857,   407,  7603,\n",
      "          326,   673, 10222,   607,  1545,    13,   198,   464,  3280,   318,\n",
      "          340,   318,   407,  1744,   284,  1560,    13, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,    32,  2415,  5586,  2354,   257,  1402,  7072,\n",
      "         6451, 12542,   510,   607,  2832,   287, 14067,   857,   407,  7603,\n",
      "          326,   673, 10222,   607,  1545,    13,   198,   464,  3280,   318,\n",
      "          340,   318,   407,  1744,   284,  1560,    13, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': 'cot',\n",
       " 'id': 'cot_60290',\n",
       " 'messages': [{'role': 'user',\n",
       "   'content': 'QUESTION: Premise: \"A man is squatting down while painting his artwork on a large metal garage door.\"\\nBased on this premise, can we conclude that the hypothesis \"A man paints art on garage door.\" is true?\\nOptions:\\n- yes\\n- it is not possible to tell\\n- no\\n\\nLet\\'s solve it slowly: A man is squatting down while paints art on metal garage door.\\nThe answer is yes.\\n[QUESTION] Premise: \"Woman hiking outside in a vineyard.\"\\nBased on this premise, can we conclude that the hypothesis \"The vines are covered with ripe grapes.\" is true?\\nOptions:\\n- yes\\n- it is not possible to tell\\n- no\\nHiking in a vineyard does not imply the vines are covered with ripe grapes.\\nThe answer is it is not possible to tell.\\nQ: Test for natural language inference.\\nPremise: \"A woman walks through the drizzling rain while shielding herself with her red umbrella.\"\\nHypothesis: \"A woman is out in the rain.\"\\nIs the hypothesis entailed by the premise?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell\\nA: A woman walks through rain means she is out in the rain.\\nThe answer is yes.\\nQUESTION: Premise: \"Two people are waiting for their food.\"\\nHypothesis: \"People are waiting on food.\"\\nDo we know that the hypothesis entailed by the premise?\\n\\nLet\\'s solve it slowly: People who is plural for two people\\'s waiting on their food.\\nThe answer is yes.\\n[QUESTION] Test for natural language inference.\\nPremise: \"A dog swimming though water holding a stick.\"\\nHypothesis: \"A dog is swimming while holding onto a stick in his mouth to bring it back to its owner.\"\\nIs the hypothesis entailed by the premise?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell\\nA dog holding a stick will not always bring it back to its owner.\\nThe answer is it is not possible to tell.\\nQ: Premise: \"A woman sitting outside a small restaurant suddenly throws up her hands in excitement.\"\\nBased on this premise, can we conclude that the hypothesis \"A lady throws up her hands in excitement because she spots her friend.\" is true?\\nOptions:\\n- yes\\n- it is not possible to tell\\n- no\\nA:'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'A woman sitting outside a small restaurant suddenly throws up her hands in excitement does not indicate that she spots her friend.\\nThe answer is it is not possible to tell.'}],\n",
       " 'input_ids': tensor([   27,    91,  7220,    91,    29,   198, 35780,  2849,    25,  6929,\n",
       "           786,    25,   366,    32,   582,   318, 22713,   889,   866,   981,\n",
       "         12036,   465, 16257,   319,   257,  1588,  6147, 15591,  3420,   526,\n",
       "           198, 15001,   319,   428, 18659,    11,   460,   356, 13796,   326,\n",
       "           262, 14078,   366,    32,   582, 32281,  1242,   319, 15591,  3420,\n",
       "           526,   318,  2081,    30,   198, 29046,    25,   198,    12,  3763,\n",
       "           198,    12,   340,   318,   407,  1744,   284,  1560,   198,    12,\n",
       "           645,   198,   198,  5756,   338,  8494,   340,  6364,    25,   317,\n",
       "           582,   318, 22713,   889,   866,   981, 32281,  1242,   319,  6147,\n",
       "         15591,  3420,    13,   198,   464,  3280,   318,  3763,    13,   198,\n",
       "            58, 35780,  2849,    60,  6929,   786,    25,   366, 48081, 24522,\n",
       "          2354,   287,   257, 17793,  9413,   526,   198, 15001,   319,   428,\n",
       "         18659,    11,   460,   356, 13796,   326,   262, 14078,   366,   464,\n",
       "         44439,   389,  5017,   351, 29036, 36656,   526,   318,  2081,    30,\n",
       "           198, 29046,    25,   198,    12,  3763,   198,    12,   340,   318,\n",
       "           407,  1744,   284,  1560,   198,    12,   645,   198,    39, 14132,\n",
       "           287,   257, 17793,  9413,   857,   407, 20135,   262, 44439,   389,\n",
       "          5017,   351, 29036, 36656,    13,   198,   464,  3280,   318,   340,\n",
       "           318,   407,  1744,   284,  1560,    13,   198,    48,    25,  6208,\n",
       "           329,  3288,  3303, 32278,    13,   198, 24914,   786,    25,   366,\n",
       "            32,  2415, 11114,   832,   262,  1454,  3019,  1359,  6290,   981,\n",
       "         43539,  5223,   351,   607,  2266, 25510,   526,   198, 49926,   313,\n",
       "          8497,    25,   366,    32,  2415,   318,   503,   287,   262,  6290,\n",
       "           526,   198,  3792,   262, 14078,   920,  6255,   416,   262, 18659,\n",
       "            30,   198, 29046,    25,   198,    12,  3763,   198,    12,   645,\n",
       "           198,    12,   340,   318,   407,  1744,   284,  1560,   198,    32,\n",
       "            25,   317,  2415, 11114,   832,  6290,  1724,   673,   318,   503,\n",
       "           287,   262,  6290,    13,   198,   464,  3280,   318,  3763,    13,\n",
       "           198, 35780,  2849,    25,  6929,   786,    25,   366,  7571,   661,\n",
       "           389,  4953,   329,   511,  2057,   526,   198, 49926,   313,  8497,\n",
       "            25,   366,  8061,   389,  4953,   319,  2057,   526,   198,  5211,\n",
       "           356,   760,   326,   262, 14078,   920,  6255,   416,   262, 18659,\n",
       "            30,   198,   198,  5756,   338,  8494,   340,  6364,    25,  4380,\n",
       "           508,   318, 22801,   329,   734,   661,   338,  4953,   319,   511,\n",
       "          2057,    13,   198,   464,  3280,   318,  3763,    13,   198,    58,\n",
       "         35780,  2849,    60,  6208,   329,  3288,  3303, 32278,    13,   198,\n",
       "         24914,   786,    25,   366,    32,  3290, 14899,   996,  1660,  4769,\n",
       "           257,  4859,   526,   198, 49926,   313,  8497,    25,   366,    32,\n",
       "          3290,   318, 14899,   981,  4769,  4291,   257,  4859,   287,   465,\n",
       "          5422,   284,  2222,   340,   736,   284,   663,  4870,   526,   198,\n",
       "          3792,   262, 14078,   920,  6255,   416,   262, 18659,    30,   198,\n",
       "         29046,    25,   198,    12,  3763,   198,    12,   645,   198,    12,\n",
       "           340,   318,   407,  1744,   284,  1560,   198,    32,  3290,  4769,\n",
       "           257,  4859,   481,   407,  1464,  2222,   340,   736,   284,   663,\n",
       "          4870,    13,   198,   464,  3280,   318,   340,   318,   407,  1744,\n",
       "           284,  1560,    13,   198,    48,    25,  6929,   786,    25,   366,\n",
       "            32,  2415,  5586,  2354,   257,  1402,  7072,  6451, 12542,   510,\n",
       "           607,  2832,   287, 14067,   526,   198, 15001,   319,   428, 18659,\n",
       "            11,   460,   356, 13796,   326,   262, 14078,   366,    32, 10846,\n",
       "         12542,   510,   607,  2832,   287, 14067,   780,   673, 10222,   607,\n",
       "          1545,   526,   318,  2081,    30,   198, 29046,    25,   198,    12,\n",
       "          3763,   198,    12,   340,   318,   407,  1744,   284,  1560,   198,\n",
       "            12,   645,   198,    32,    25,   198,    27,    91,   562, 10167,\n",
       "            91,    29,   198,    32,  2415,  5586,  2354,   257,  1402,  7072,\n",
       "          6451, 12542,   510,   607,  2832,   287, 14067,   857,   407,  7603,\n",
       "           326,   673, 10222,   607,  1545,    13,   198,   464,  3280,   318,\n",
       "           340,   318,   407,  1744,   284,  1560,    13, 50256]),\n",
       " 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,    32,  2415,  5586,  2354,   257,  1402,  7072,\n",
       "          6451, 12542,   510,   607,  2832,   287, 14067,   857,   407,  7603,\n",
       "           326,   673, 10222,   607,  1545,    13,   198,   464,  3280,   318,\n",
       "           340,   318,   407,  1744,   284,  1560,    13, 50256]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'domain_id': 0}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f54912af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/01/2023 17:11:00 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c6ba3d14d25cb207.arrow\n",
      "{'dataset': 'cot', 'id': 'cot_60290', 'messages': [{'role': 'user', 'content': 'QUESTION: Premise: \"A man is squatting down while painting his artwork on a large metal garage door.\"\\nBased on this premise, can we conclude that the hypothesis \"A man paints art on garage door.\" is true?\\nOptions:\\n- yes\\n- it is not possible to tell\\n- no\\n\\nLet\\'s solve it slowly: A man is squatting down while paints art on metal garage door.\\nThe answer is yes.\\n[QUESTION] Premise: \"Woman hiking outside in a vineyard.\"\\nBased on this premise, can we conclude that the hypothesis \"The vines are covered with ripe grapes.\" is true?\\nOptions:\\n- yes\\n- it is not possible to tell\\n- no\\nHiking in a vineyard does not imply the vines are covered with ripe grapes.\\nThe answer is it is not possible to tell.\\nQ: Test for natural language inference.\\nPremise: \"A woman walks through the drizzling rain while shielding herself with her red umbrella.\"\\nHypothesis: \"A woman is out in the rain.\"\\nIs the hypothesis entailed by the premise?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell\\nA: A woman walks through rain means she is out in the rain.\\nThe answer is yes.\\nQUESTION: Premise: \"Two people are waiting for their food.\"\\nHypothesis: \"People are waiting on food.\"\\nDo we know that the hypothesis entailed by the premise?\\n\\nLet\\'s solve it slowly: People who is plural for two people\\'s waiting on their food.\\nThe answer is yes.\\n[QUESTION] Test for natural language inference.\\nPremise: \"A dog swimming though water holding a stick.\"\\nHypothesis: \"A dog is swimming while holding onto a stick in his mouth to bring it back to its owner.\"\\nIs the hypothesis entailed by the premise?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell\\nA dog holding a stick will not always bring it back to its owner.\\nThe answer is it is not possible to tell.\\nQ: Premise: \"A woman sitting outside a small restaurant suddenly throws up her hands in excitement.\"\\nBased on this premise, can we conclude that the hypothesis \"A lady throws up her hands in excitement because she spots her friend.\" is true?\\nOptions:\\n- yes\\n- it is not possible to tell\\n- no\\nA:'}, {'role': 'assistant', 'content': 'A woman sitting outside a small restaurant suddenly throws up her hands in excitement does not indicate that she spots her friend.\\nThe answer is it is not possible to tell.'}], 'input_ids': tensor([   27,    91,  7220,    91,    29,   198, 35780,  2849,    25,  6929,\n",
      "          786,    25,   366,    32,   582,   318, 22713,   889,   866,   981,\n",
      "        12036,   465, 16257,   319,   257,  1588,  6147, 15591,  3420,   526,\n",
      "          198, 15001,   319,   428, 18659,    11,   460,   356, 13796,   326,\n",
      "          262, 14078,   366,    32,   582, 32281,  1242,   319, 15591,  3420,\n",
      "          526,   318,  2081,    30,   198, 29046,    25,   198,    12,  3763,\n",
      "          198,    12,   340,   318,   407,  1744,   284,  1560,   198,    12,\n",
      "          645,   198,   198,  5756,   338,  8494,   340,  6364,    25,   317,\n",
      "          582,   318, 22713,   889,   866,   981, 32281,  1242,   319,  6147,\n",
      "        15591,  3420,    13,   198,   464,  3280,   318,  3763,    13,   198,\n",
      "           58, 35780,  2849,    60,  6929,   786,    25,   366, 48081, 24522,\n",
      "         2354,   287,   257, 17793,  9413,   526,   198, 15001,   319,   428,\n",
      "        18659,    11,   460,   356, 13796,   326,   262, 14078,   366,   464,\n",
      "        44439,   389,  5017,   351, 29036, 36656,   526,   318,  2081,    30,\n",
      "          198, 29046,    25,   198,    12,  3763,   198,    12,   340,   318,\n",
      "          407,  1744,   284,  1560,   198,    12,   645,   198,    39, 14132,\n",
      "          287,   257, 17793,  9413,   857,   407, 20135,   262, 44439,   389,\n",
      "         5017,   351, 29036, 36656,    13,   198,   464,  3280,   318,   340,\n",
      "          318,   407,  1744,   284,  1560,    13,   198,    48,    25,  6208,\n",
      "          329,  3288,  3303, 32278,    13,   198, 24914,   786,    25,   366,\n",
      "           32,  2415, 11114,   832,   262,  1454,  3019,  1359,  6290,   981,\n",
      "        43539,  5223,   351,   607,  2266, 25510,   526,   198, 49926,   313,\n",
      "         8497,    25,   366,    32,  2415,   318,   503,   287,   262,  6290,\n",
      "          526,   198,  3792,   262, 14078,   920,  6255,   416,   262, 18659,\n",
      "           30,   198, 29046,    25,   198,    12,  3763,   198,    12,   645,\n",
      "          198,    12,   340,   318,   407,  1744,   284,  1560,   198,    32,\n",
      "           25,   317,  2415, 11114,   832,  6290,  1724,   673,   318,   503,\n",
      "          287,   262,  6290,    13,   198,   464,  3280,   318,  3763,    13,\n",
      "          198, 35780,  2849,    25,  6929,   786,    25,   366,  7571,   661,\n",
      "          389,  4953,   329,   511,  2057,   526,   198, 49926,   313,  8497,\n",
      "           25,   366,  8061,   389,  4953,   319,  2057,   526,   198,  5211,\n",
      "          356,   760,   326,   262, 14078,   920,  6255,   416,   262, 18659,\n",
      "           30,   198,   198,  5756,   338,  8494,   340,  6364,    25,  4380,\n",
      "          508,   318, 22801,   329,   734,   661,   338,  4953,   319,   511,\n",
      "         2057,    13,   198,   464,  3280,   318,  3763,    13,   198,    58,\n",
      "        35780,  2849,    60,  6208,   329,  3288,  3303, 32278,    13,   198,\n",
      "        24914,   786,    25,   366,    32,  3290, 14899,   996,  1660,  4769,\n",
      "          257,  4859,   526,   198, 49926,   313,  8497,    25,   366,    32,\n",
      "         3290,   318, 14899,   981,  4769,  4291,   257,  4859,   287,   465,\n",
      "         5422,   284,  2222,   340,   736,   284,   663,  4870,   526,   198,\n",
      "         3792,   262, 14078,   920,  6255,   416,   262, 18659,    30,   198,\n",
      "        29046,    25,   198,    12,  3763,   198,    12,   645,   198,    12,\n",
      "          340,   318,   407,  1744,   284,  1560,   198,    32,  3290,  4769,\n",
      "          257,  4859,   481,   407,  1464,  2222,   340,   736,   284,   663,\n",
      "         4870,    13,   198,   464,  3280,   318,   340,   318,   407,  1744,\n",
      "          284,  1560,    13,   198,    48,    25,  6929,   786,    25,   366,\n",
      "           32,  2415,  5586,  2354,   257,  1402,  7072,  6451, 12542,   510,\n",
      "          607,  2832,   287, 14067,   526,   198, 15001,   319,   428, 18659,\n",
      "           11,   460,   356, 13796,   326,   262, 14078,   366,    32, 10846,\n",
      "        12542,   510,   607,  2832,   287, 14067,   780,   673, 10222,   607,\n",
      "         1545,   526,   318,  2081,    30,   198, 29046,    25,   198,    12,\n",
      "         3763,   198,    12,   340,   318,   407,  1744,   284,  1560,   198,\n",
      "           12,   645,   198,    32,    25,   198,    27,    91,   562, 10167,\n",
      "           91,    29,   198,    32,  2415,  5586,  2354,   257,  1402,  7072,\n",
      "         6451, 12542,   510,   607,  2832,   287, 14067,   857,   407,  7603,\n",
      "          326,   673, 10222,   607,  1545,    13,   198,   464,  3280,   318,\n",
      "          340,   318,   407,  1744,   284,  1560,    13, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,    32,  2415,  5586,  2354,   257,  1402,  7072,\n",
      "         6451, 12542,   510,   607,  2832,   287, 14067,   857,   407,  7603,\n",
      "          326,   673, 10222,   607,  1545,    13,   198,   464,  3280,   318,\n",
      "          340,   318,   407,  1744,   284,  1560,    13, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'dataset': 'cot', 'id': 'cot_69782', 'messages': [{'role': 'user', 'content': 'People dancing outside does not mean that those people had attended a wedding.. So what could be the question?\\nQuestion followed by answer: If \"A group of men and women dancing outside.\" does that mean that \"People dancing outside after a wedding.\"?\\nOptions:\\n- yes\\n- it is not possible to tell\\n- no\\nit is not possible to tell\\n\\nOne can not hold an infant and eat food at the same time.\\nThe question and answer are below.\\nPremise: \"A man holds an infant while leaning against a building.\"\\nHypothesis: \"A man is eating food.\"\\nDo we know that the hypothesis entailed by the premise?\\nno\\n\\nThe monster truck leaping over crushed cars is just a past tense action of how the truck leaps over crushed cars.\\nThe question and answer are below.\\nCan we conclude from \"A giant monster truck leaps over crushed cars and into the air inside the arena.\" that \"Monster truck leaping over crushed cars.\"?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell\\nyes\\n\\nPower Wheels motorcycle is a motorcycle. One who rides a motorcycle also drives a motorcycle.. So what could be the question?\\nQuestion followed by answer: Can we conclude from \"A young boy drives a power wheels motorcycle with an american flag attached to the back.\" that \"A young boy rides a toy motorcycle.\"?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell\\nyes\\n\\nA man on a horse who tries to throw him off is on a bucking bronco.\\nThe question and answer are below.\\nTest for natural language inference.\\nPremise: \"A man in a red plaid shirt holds on for dear life as the horse tries to throw him off.\"\\nHypothesis: \"A man on a bucking bronco.\"\\nIs the hypothesis entailed by the premise?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell\\nyes\\n\\nSitting up high does not imply they are sitting on a bridge.\\nThe question and answer are below.\\n'}, {'role': 'assistant', 'content': 'Given the sentence \"A person is sitting up high over a street.\" is it true that \"A person sits on a bridge over a freeway.\"?\\nit is not possible to tell'}], 'input_ids': tensor([   27,    91,  7220,    91,    29,   198,  8061, 15360,  2354,   857,\n",
      "          407,  1612,   326,   883,   661,   550,  9141,   257, 10614,   492,\n",
      "         1406,   644,   714,   307,   262,  1808,    30,   198, 24361,  3940,\n",
      "          416,  3280,    25,  1002,   366,    32,  1448,   286,  1450,   290,\n",
      "         1466, 15360,  2354,   526,   857,   326,  1612,   326,   366,  8061,\n",
      "        15360,  2354,   706,   257, 10614,   526,    30,   198, 29046,    25,\n",
      "          198,    12,  3763,   198,    12,   340,   318,   407,  1744,   284,\n",
      "         1560,   198,    12,   645,   198,   270,   318,   407,  1744,   284,\n",
      "         1560,   198,   198,  3198,   460,   407,  1745,   281, 11212,   290,\n",
      "         4483,  2057,   379,   262,   976,   640,    13,   198,   464,  1808,\n",
      "          290,  3280,   389,  2174,    13,   198, 24914,   786,    25,   366,\n",
      "           32,   582,  6622,   281, 11212,   981, 21804,  1028,   257,  2615,\n",
      "          526,   198, 49926,   313,  8497,    25,   366,    32,   582,   318,\n",
      "         6600,  2057,   526,   198,  5211,   356,   760,   326,   262, 14078,\n",
      "          920,  6255,   416,   262, 18659,    30,   198,  3919,   198,   198,\n",
      "          464,  9234,  7779, 45583,   625, 18577,  5006,   318,   655,   257,\n",
      "         1613, 20170,  2223,   286,   703,   262,  7779, 37312,   625, 18577,\n",
      "         5006,    13,   198,   464,  1808,   290,  3280,   389,  2174,    13,\n",
      "          198,  6090,   356, 13796,   422,   366,    32,  6175,  9234,  7779,\n",
      "        37312,   625, 18577,  5006,   290,   656,   262,  1633,  2641,   262,\n",
      "        13478,   526,   326,   366, 40872,  7779, 45583,   625, 18577,  5006,\n",
      "          526,    30,   198, 29046,    25,   198,    12,  3763,   198,    12,\n",
      "          645,   198,    12,   340,   318,   407,  1744,   284,  1560,   198,\n",
      "         8505,   198,   198, 13434, 37416, 18757,   318,   257, 18757,    13,\n",
      "         1881,   508, 17445,   257, 18757,   635, 10182,   257, 18757,   492,\n",
      "         1406,   644,   714,   307,   262,  1808,    30,   198, 24361,  3940,\n",
      "          416,  3280,    25,  1680,   356, 13796,   422,   366,    32,  1862,\n",
      "         2933, 10182,   257,  1176, 13666, 18757,   351,   281, 45630,   272,\n",
      "         6056,  7223,   284,   262,   736,   526,   326,   366,    32,  1862,\n",
      "         2933, 17445,   257, 13373, 18757,   526,    30,   198, 29046,    25,\n",
      "          198,    12,  3763,   198,    12,   645,   198,    12,   340,   318,\n",
      "          407,  1744,   284,  1560,   198,  8505,   198,   198,    32,   582,\n",
      "          319,   257,  8223,   508,  8404,   284,  3714,   683,   572,   318,\n",
      "          319,   257, 10230,   278, 18443,  1073,    13,   198,   464,  1808,\n",
      "          290,  3280,   389,  2174,    13,   198, 14402,   329,  3288,  3303,\n",
      "        32278,    13,   198, 24914,   786,    25,   366,    32,   582,   287,\n",
      "          257,  2266,   458,  1698, 10147,  6622,   319,   329, 13674,  1204,\n",
      "          355,   262,  8223,  8404,   284,  3714,   683,   572,   526,   198,\n",
      "        49926,   313,  8497,    25,   366,    32,   582,   319,   257, 10230,\n",
      "          278, 18443,  1073,   526,   198,  3792,   262, 14078,   920,  6255,\n",
      "          416,   262, 18659,    30,   198, 29046,    25,   198,    12,  3763,\n",
      "          198,    12,   645,   198,    12,   340,   318,   407,  1744,   284,\n",
      "         1560,   198,  8505,   198,   198,    50,  2535,   510,  1029,   857,\n",
      "          407, 20135,   484,   389,  5586,   319,   257,  7696,    13,   198,\n",
      "          464,  1808,   290,  3280,   389,  2174,    13,   198,    27,    91,\n",
      "          562, 10167,    91,    29,   198, 15056,   262,  6827,   366,    32,\n",
      "         1048,   318,  5586,   510,  1029,   625,   257,  4675,   526,   318,\n",
      "          340,  2081,   326,   366,    32,  1048, 10718,   319,   257,  7696,\n",
      "          625,   257, 39042,   526,    30,   198,   270,   318,   407,  1744,\n",
      "          284,  1560, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100, 15056,   262,  6827,   366,    32,\n",
      "         1048,   318,  5586,   510,  1029,   625,   257,  4675,   526,   318,\n",
      "          340,  2081,   326,   366,    32,  1048, 10718,   319,   257,  7696,\n",
      "          625,   257, 39042,   526,    30,   198,   270,   318,   407,  1744,\n",
      "          284,  1560, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "08/01/2023 17:11:00 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8871af544c832385.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'flan_v2', 'id': 'flan_v2_70780', 'messages': [{'role': 'user', 'content': \"(Question)\\nInside Recordings is a Los Angeles California based independent record label founded by singer-songwriter Jackson Browne in 1999. Browne has stated that the mission of the label is to create a haven for music that might not find a home in the mainstream. In 2005 Inside Recordings signed a nationwide direct distribution deal with independent music distributor Alternative Distribution Alliance. The label's inaugural release under the agreement was Browne's own Solo Acoustic Vol. Given a list of categories: company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film or written work, what category does the paragraph belong to?\\n(Answer)\\nCompany\\n\\n\\n(Question)\\nId Rather Go Blonde is the eighth album in a twenty-year career that has seen Eleanor McEvoy establish herself as one of Irelands most accomplished singer / songwriters. Given a list of categories: company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film or written work, what category does the paragraph belong to?\\n(Answer)\\nAlbum\\n\\n\\n(Question)\\nMarcin Kamiski (Polish pronunciation: [martin kamiski]; born 15 January 1992 in Konin) is a left footed Polish footballer who plays for Lech Pozna as a central defender. Given a list of categories: company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film or written work, what category does the paragraph belong to?\\n(Answer)\\n\"}, {'role': 'assistant', 'content': 'Athlete'}], 'input_ids': tensor([   27,    91,  7220,    91,    29,   198,     7, 24361,     8,   198,\n",
      "        24441, 13266,   654,   318,   257,  5401,  5652,  3442,  1912,  4795,\n",
      "         1700,  6167,  9393,   416, 14015,    12, 34050, 16002,  6612, 46760,\n",
      "          287,  7358,    13, 46760,   468,  5081,   326,   262,  4365,   286,\n",
      "          262,  6167,   318,   284,  2251,   257,  4398,   329,  2647,   326,\n",
      "         1244,   407,  1064,   257,  1363,   287,   262,  8661,    13,   554,\n",
      "         5075, 14384, 13266,   654,  4488,   257, 13673,  1277,  6082,  1730,\n",
      "          351,  4795,  2647, 32137, 27182, 27484, 10302,    13,   383,  6167,\n",
      "          338, 26851,  2650,   739,   262,  4381,   373, 46760,   338,   898,\n",
      "        20284,  4013, 21618,  4709,    13, 11259,   257,  1351,   286,  9376,\n",
      "           25,  1664,    11,  9856,  9901,    11,  6802,    11, 16076,    11,\n",
      "         2607, 15762,    11,  1612,   286,  9358,    11,  2615,    11,  3288,\n",
      "         1295,    11,  7404,    11,  5044,    11,  4618,    11,  5062,    11,\n",
      "         2646,   393,  3194,   670,    11,   644,  6536,   857,   262,  7322,\n",
      "         5594,   284,    30,   198,     7, 33706,     8,   198, 39154,   628,\n",
      "          198,     7, 24361,     8,   198,    40,   447,   247,    67, 11317,\n",
      "         1514,  1086, 14378,   318,   262, 16974,  5062,   287,   257,  8208,\n",
      "           12,  1941,  3451,   326,   468,  1775, 45566,  1982, 15200,   726,\n",
      "         4474,  5223,   355,   530,   286,  7517,   447,   247,    82,   749,\n",
      "        13013, 14015,  1220,  3496, 34422,    13, 11259,   257,  1351,   286,\n",
      "         9376,    25,  1664,    11,  9856,  9901,    11,  6802,    11, 16076,\n",
      "           11,  2607, 15762,    11,  1612,   286,  9358,    11,  2615,    11,\n",
      "         3288,  1295,    11,  7404,    11,  5044,    11,  4618,    11,  5062,\n",
      "           11,  2646,   393,  3194,   670,    11,   644,  6536,   857,   262,\n",
      "         7322,  5594,   284,    30,   198,     7, 33706,     8,   198,  2348,\n",
      "         4435,   628,   198,     7, 24361,     8,   198, 22697,   259,   509,\n",
      "         6277,   129,   226, 20545,   357,  8017,   680, 41694,    25,   685,\n",
      "        45990, 13822,   137,    94,   133,   243,   259, 38387, 45990, 11632,\n",
      "          133,   110, 20545, 11208,  4642,  1315,  3269,  9768,   287, 17431,\n",
      "          259,     8,   318,   257,  1364,  2366,   276, 15945, 44185,   508,\n",
      "         5341,   329,  1004,   354,  7695,    89,  2616,   129,   226,   355,\n",
      "          257,  4318, 13191,    13, 11259,   257,  1351,   286,  9376,    25,\n",
      "         1664,    11,  9856,  9901,    11,  6802,    11, 16076,    11,  2607,\n",
      "        15762,    11,  1612,   286,  9358,    11,  2615,    11,  3288,  1295,\n",
      "           11,  7404,    11,  5044,    11,  4618,    11,  5062,    11,  2646,\n",
      "          393,  3194,   670,    11,   644,  6536,   857,   262,  7322,  5594,\n",
      "          284,    30,   198,     7, 33706,     8,   198,    27,    91,   562,\n",
      "        10167,    91,    29,   198,    32,   400,  5807, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,    32,   400,  5807, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'dataset': 'flan_v2', 'id': 'flan_v2_90368', 'messages': [{'role': 'user', 'content': \"Part 1. Definition\\nGenerate the answer for a given question. The question requires common knowledge of English. Make sure that the answer is in detail, long, consistent with the context, and requires general knowledge of English. To be less repetitive, please vary your language for each question. Give a proper explanation to answer, and if necessary, provide the definition of various terms. And conclude your answer if it's long. If you have used other resources (e.g., Merriam Webster's.) to answer the question, include citations in your answer. You don't need to paraphrase the context of your resources, but make sure that your answer is consistent and complete.\\nPart 2. Example\\nHow are i.e. and e.g. pronounced?\\nAnswer: i.e. stands for id est (Latin), which means 'that is'. You use it to link in a deeper explanation about something. Pronounce it 'eye - ee'. e.g. stands for exempli gratia (also Latin), which means 'for example'. You use it to link in an example of a more generic term. Pronounce it 'ee - jee'\\nExplanation: Perfect answer because it's consistent with context uses required information from the question to provide the detailed answer with an example.\\nPart 3. Exercise\\nIn the book I am reading (Patrick Fairbairn, The Interpretation of Prophecy, 2nd ed., 1865), the abbreviation 'sq.' is used sometimes in referencing both Scripture and other books (as 'Typology of Scripture, vol. i. p. 100 sq.' or 'Ezek xxvi. 7, sq.'). What does 'sq.' mean in this context? (Instances of this abbreviation's usage may be found at Google Books here.).\\nAnswer:\"}, {'role': 'assistant', 'content': \"Sq. is short for the Latin word sequiturque, which means the following. So p. 100 sq. means page 100 and those following pages. Here's the source.\"}], 'input_ids': tensor([   27,    91,  7220,    91,    29,   198,  7841,   352,    13, 30396,\n",
      "          198,  8645,   378,   262,  3280,   329,   257,  1813,  1808,    13,\n",
      "          383,  1808,  4433,  2219,  3725,   286,  3594,    13,  6889,  1654,\n",
      "          326,   262,  3280,   318,   287,  3703,    11,   890,    11,  6414,\n",
      "          351,   262,  4732,    11,   290,  4433,  2276,  3725,   286,  3594,\n",
      "           13,  1675,   307,  1342, 28585,    11,  3387,  7565,   534,  3303,\n",
      "          329,  1123,  1808,    13, 13786,   257,  1774,  7468,   284,  3280,\n",
      "           11,   290,   611,  3306,    11,  2148,   262,  6770,   286,  2972,\n",
      "         2846,    13,   843, 13796,   534,  3280,   611,   340,   338,   890,\n",
      "           13,  1002,   345,   423,   973,   584,  4133,   357,    68,    13,\n",
      "           70,  1539,  4638,   380,   321, 31890,   338,  2014,   284,  3280,\n",
      "          262,  1808,    11,  2291, 33499,   287,   534,  3280,    13,   921,\n",
      "          836,   470,   761,   284, 38624, 22789,   262,  4732,   286,   534,\n",
      "         4133,    11,   475,   787,  1654,   326,   534,  3280,   318,  6414,\n",
      "          290,  1844,    13,   198,  7841,   362,    13, 17934,   198,  2437,\n",
      "          389,  1312,    13,    68,    13,   290,   304,    13,    70,    13,\n",
      "        16293,    30,   198, 33706,    25,  1312,    13,    68,    13,  6296,\n",
      "          329,  4686,  1556,   357, 49022,   828,   543,  1724,   705,  5562,\n",
      "          318,  4458,   921,   779,   340,   284,  2792,   287,   257,  9211,\n",
      "         7468,   546,  1223,    13,   350,  1313,  8652,   340,   705, 25379,\n",
      "          532,   304,    68,  4458,   304,    13,    70,    13,  6296,   329,\n",
      "        21433,    72, 14586,   544,   357, 14508,  9133,   828,   543,  1724,\n",
      "          705,  1640,  1672,  4458,   921,   779,   340,   284,  2792,   287,\n",
      "          281,  1672,   286,   257,   517, 14276,  3381,    13,   350,  1313,\n",
      "         8652,   340,   705,  1453,   532,   474,  1453,     6,   198,  3109,\n",
      "        11578,   341,    25, 16374,  3280,   780,   340,   338,  6414,   351,\n",
      "         4732,  3544,  2672,  1321,   422,   262,  1808,   284,  2148,   262,\n",
      "         6496,  3280,   351,   281,  1672,    13,   198,  7841,   513,    13,\n",
      "        32900,   198,   818,   262,  1492,   314,   716,  3555,   357, 32718,\n",
      "         7011,    65,   958,    77,    11,   383, 48907,   341,   286, 33360,\n",
      "          948,    11,   362,   358,  1225,  1539, 47801,   828,   262, 28873,\n",
      "        47625,   705, 31166,  2637,   318,   973,  3360,   287, 32578,  1111,\n",
      "        25595,   290,   584,  3835,   357,   292,   705, 31467,  1435,   286,\n",
      "        25595,    11,  2322,    13,  1312,    13,   279,    13,  1802, 19862,\n",
      "         2637,   393,   705,    36, 43130, 31383,  8903,    13,   767,    11,\n",
      "        19862,  2637,   737,  1867,   857,   705, 31166,  2637,  1612,   287,\n",
      "          428,  4732,    30,   357,  6310,  1817,   286,   428, 28873, 47625,\n",
      "          338,  8748,   743,   307,  1043,   379,  3012, 13661,   994, 15729,\n",
      "          198, 33706,    25,   198,    27,    91,   562, 10167,    91,    29,\n",
      "          198,    50,    80,    13,   318,  1790,   329,   262,  9133,  1573,\n",
      "         4726,   270,   333,  4188,    11,   543,  1724,   262,  1708,    13,\n",
      "         1406,   279,    13,  1802, 19862,    13,  1724,  2443,  1802,   290,\n",
      "          883,  1708,  5468,    13,  3423,   338,   262,  2723,    13, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,    50,    80,    13,   318,  1790,   329,   262,  9133,  1573,\n",
      "         4726,   270,   333,  4188,    11,   543,  1724,   262,  1708,    13,\n",
      "         1406,   279,    13,  1802, 19862,    13,  1724,  2443,  1802,   290,\n",
      "          883,  1708,  5468,    13,  3423,   338,   262,  2723,    13, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "batch = list(train_dataset.take(3))\n",
    "batch = [{'input_ids': x['input_ids'], \n",
    "          'attention_mask': x['attention_mask'], \n",
    "          'labels': x['labels'],\n",
    "          'domain_id': x['domain_id']} for x in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e5ab21b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function doremi.dataloader.get_data_collator.<locals>.data_collator(features)>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args.do_padding = True\n",
    "collator = data_utils.get_data_collator(tokenizer, do_padding=data_args.do_padding)\n",
    "collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3c2dfb14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 568]), torch.Size([3, 568]))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = collator(batch)\n",
    "x['input_ids'].shape, x['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0500c240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([568])\n",
      "attention_mask torch.Size([568])\n",
      "labels torch.Size([568])\n",
      "domain_id 0\n",
      "input_ids torch.Size([378])\n",
      "attention_mask torch.Size([378])\n",
      "labels torch.Size([378])\n",
      "domain_id 2\n",
      "input_ids torch.Size([493])\n",
      "attention_mask torch.Size([493])\n",
      "labels torch.Size([493])\n",
      "domain_id 0\n"
     ]
    }
   ],
   "source": [
    "for x in batch:\n",
    "    for k,v in x.items():\n",
    "        if k.startswith('domain'):\n",
    "            print(k, v)\n",
    "        else:\n",
    "            print(k, v.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
