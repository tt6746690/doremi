{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334172b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_setup; jpt_setup()\n",
    "import os; os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9294819",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import math\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import chain\n",
    "from typing import Optional\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "import pyarrow\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset, IterableDataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.trainer_callback import TrainerState\n",
    "from transformers.trainer import TRAINER_STATE_NAME\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "\n",
    "from doremi.training_args import ModelArguments, DataTrainingArguments, FullTrainingArguments\n",
    "import doremi.dataloader as data_utils\n",
    "from doremi.trainer import DoReMiTrainer\n",
    "from doremi.dataloader import determine_skip_per_domain\n",
    "from doremi.dataloader import interleave_datasets\n",
    "\n",
    "\n",
    "try:\n",
    "    import doremi.models as doremi_models\n",
    "except Exception:\n",
    "    \n",
    "    pass\n",
    "try:\n",
    "    from flash_attn.models.gpt_neox import gpt_neox_config_to_gpt2_config\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.27.0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/language-modeling/requirements.txt\")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47cef8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_dir = \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi\"\n",
    "cache_dir = '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache'\n",
    "preprocessed_data = \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed\"\n",
    "\n",
    "envs = {\n",
    "    \"CACHE\": cache_dir,\n",
    "    \"DOREMI_DIR\": package_dir,\n",
    "    \"PILE_DIR\": os.path.join(package_dir, \"data\", 'raw'),\n",
    "    \"PREPROCESSED_PILE_DIR\": preprocessed_data,\n",
    "    \"MODEL_OUTPUT_DIR\": os.path.join(package_dir, 'results'),\n",
    "    \"PARTITION\": \"el8\",\n",
    "    \"HF_HOME\": cache_dir,\n",
    "    \"TRANSFORMERS_CACHE\": cache_dir,\n",
    "    \"HF_DATASETS_CACHE\": cache_dir,\n",
    "    \"HF_DATASETS_IN_MEMORY_MAX_SIZE\": \"0\",\n",
    "    \"TORCH_EXTENSIONS_DIR\": cache_dir,\n",
    "    \"TMPDIR\": cache_dir,\n",
    "    \"WANDB_DIR\": os.path.join(cache_dir, \"wandb\"),\n",
    "    \"WANDB_MODE\": 'offline',\n",
    "    \"PREPROCESSED_DATA\": preprocessed_data,\n",
    "    'PREPROCESSED_CACHE': os.path.join(cache_dir, 'preprocessed_cache', 'perdomain_pile_preprocessed'),\n",
    "\n",
    "}\n",
    "\n",
    "for k, v in envs.items():\n",
    "    os.environ[k] = v\n",
    "    \n",
    "os.makedirs(cache_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb14c3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1301] 2023-08-08 22:38:52,920 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "[INFO|training_args.py:1716] 2023-08-08 22:38:52,921 >> PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/EleutherAI/pythia-160m using 1 GPUs, 1 batch size per GPU, 128 gradient accumulation steps,for 1562 max steps.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ModelArguments(model_name_or_path='/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/EleutherAI/pythia-160m', model_type='pythia', config_overrides=None, config_name=None, tokenizer_name=None, cache_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache', use_fast_tokenizer=True, model_revision='main', use_auth_token=False, torch_dtype=None),\n",
       " DataTrainingArguments(dataset_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed', dataset_name='', max_train_samples=None, max_eval_samples=None, max_token_length=1024, block_size=None, overwrite_cache=False, do_padding=True, add_domain_id=True, preprocessing_num_workers=None, shuffle=True),\n",
       " FullTrainingArguments(output_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/jpt_pythia-160m_humanmix_uniform_baseline', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=128, eval_accumulation_steps=None, eval_delay=0, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.99, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=1562, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/jpt_pythia-160m_humanmix_uniform_baseline/runs/Aug08_22-38-52_dcs021', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=True, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=10, save_total_limit=1, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=1111, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='train_baseline', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_HF: 'adamw_hf'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, domain_config_path='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/configs/humanmix_uniform_baseline_50kvocab.json', lr_end=0.001, reweight_domains=False, reweight_eta=1.0, reweight_eps=0.0001, doremi_optimizer='doremiv1', reference_model_name_or_path='.', lr_scheduler_name=None, train_domain_weights_tmp_file=None))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_name = 'train_baseline'\n",
    "\n",
    "num_gpus = 1\n",
    "nodes = 1\n",
    "use_doremi = False\n",
    "doremi_optimizer = 'doremiv2'\n",
    "\n",
    "hf_models_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/'\n",
    "model_type = 'gpt2'\n",
    "model_name_or_path = os.path.join(hf_models_dir, 'gpt2-medium'); abbr_model_name = 'gpt2-medium'\n",
    "model_name_or_path = ('/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/'\n",
    "                      'ft1/gpt2-medium_humanmix_doremi'); abbr_model_name = 'gpt2-medium'\n",
    "model_name_or_path = os.path.join(hf_models_dir, 'gpt2-xl'); abbr_model_name = 'gpt2-xl'\n",
    "model_name_or_path = os.path.join(hf_models_dir, 'EleutherAI/pythia-160m'); model_type = 'pythia'; abbr_model_name = 'pythia-160m'\n",
    "\n",
    "\n",
    "cache_dir = envs['CACHE']\n",
    "abbr_train_file = 'humanmix_uniform'\n",
    "domain_config_path = os.path.abspath(f'../configs/{abbr_train_file}_baseline_50kvocab.json')\n",
    "dataset_dir = preprocessed_data\n",
    "\n",
    "\n",
    "total_batch_size = 128 # # 64*8=512\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_accumulation_steps = int(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "max_steps = int(200000/total_batch_size); save_steps = 10 # 200k steps.\n",
    "gradient_checkpointing = True\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{per_device_train_batch_size} batch size per GPU, \"\n",
    "      f\"{gradient_accumulation_steps} gradient accumulation steps,\"\n",
    "      f\"for {max_steps} max steps.\")\n",
    "\n",
    "\n",
    "fsdp = \"full_shard auto_wrap\"\n",
    "fsdp = False\n",
    "if 'gpt2' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "elif 'mpt' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MPTBlock'\n",
    "elif 'pythia' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPTNeoXLayer'\n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "    \n",
    "\n",
    "# use `dataset_dir` instead of `dataset_name` to specify `preprocessed_dir`\n",
    "# --dataset_name=pile \\\n",
    "\n",
    "## learning rate for pretraining, substituted with finetuning hyperparameters\n",
    "# --learning_rate 1e-3 \\\n",
    "# --lr_end 1e-4 \\\n",
    "# --adam_epsilon 1e-8 \\\n",
    "\n",
    "## don't need cosine scheduling for finetuning\n",
    "# --weight_decay 0.01 \\\n",
    "# --lr_scheduler_name linear_warmup_cosine \\\n",
    "# --warmup_ratio 0.06 \\\n",
    "\n",
    "## avoids grad scaling error\n",
    "# --fp16 \\\n",
    "## for training model from scratch\n",
    "# --config_overrides=\"n_positions=1024,n_embd=1024,n_layer=18,n_head=16\" \\\n",
    "\n",
    "## added the following\n",
    "# add_domain_id: for non-pile preprocessed dataset\n",
    "# do_padding: true for variable size sequences, as in instruction tuning datasets.\n",
    "# --max_train_samples 1000 \\\n",
    "\n",
    "output_dirname = f'{abbr_model_name}_{abbr_train_file}'\n",
    "if use_doremi:\n",
    "    output_dirname += '_doremi'\n",
    "else:\n",
    "    output_dirname += '_baseline'\n",
    "output_dirname = 'jpt_'+output_dirname\n",
    "output_dir = os.path.join(envs['MODEL_OUTPUT_DIR'], output_dirname)\n",
    "\n",
    "\n",
    "if use_doremi:\n",
    "    reference_model_name_or_path = (\n",
    "        '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/'\n",
    "        'ft1/gpt2-medium_humanmix_baseline/')\n",
    "    doremi_options = f\"\"\"\n",
    "        --doremi_optimizer={doremi_optimizer} \\\n",
    "        --reweight_eta=1 \\\n",
    "        --reweight_eps=1e-4 \\\n",
    "        --train_domain_weights_tmp_file={os.path.join(output_dir, 'domain_weights')} \\\n",
    "        --reweight_domains \\\n",
    "        --remove_unused_columns=False \\\n",
    "        --reference_model_name_or_path={reference_model_name_or_path} \\\n",
    "    \"\"\"\n",
    "else:\n",
    "    doremi_options = ''\n",
    "\n",
    "cmd = f\"\"\"\n",
    "--model_name_or_path={model_name_or_path} \\\n",
    "--model_type={model_type} \\\n",
    "--do_train \\\n",
    "--cache_dir={cache_dir} \\\n",
    "--dataset_dir={dataset_dir} \\\n",
    "--domain_config_path={domain_config_path} \\\n",
    "--max_token_length=1024 \\\n",
    "--per_device_train_batch_size={per_device_train_batch_size} \\\n",
    "--gradient_accumulation_steps={gradient_accumulation_steps} \\\n",
    "--dataloader_num_workers=1 \\\n",
    "--learning_rate=2e-5 \\\n",
    "--lr_scheduler_type=linear \\\n",
    "--warmup_ratio=0.03 \\\n",
    "--weight_decay=0. \\\n",
    "--max_grad_norm=1.0 \\\n",
    "--max_steps={max_steps} \\\n",
    "--evaluation_strategy=no \\\n",
    "--save_strategy=steps \\\n",
    "--save_steps={save_steps} \\\n",
    "--save_total_limit=1 \\\n",
    "--run_name={job_name} \\\n",
    "--seed=1111 \\\n",
    "--logging_strategy=steps \\\n",
    "--logging_steps=10 \\\n",
    "--logging_first_step \\\n",
    "--report_to='none' \\\n",
    "--optim=adamw_hf \\\n",
    "--adam_beta1=0.9 \\\n",
    "--adam_beta2=0.99 \\\n",
    "--gradient_checkpointing \\\n",
    "{'--fsdp_transformer_layer_cls_to_wrap=\"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "    if fsdp else ''} \\\n",
    "{'--gradient_checkpointing' if gradient_checkpointing  else ''} \\\n",
    "--add_domain_id=True \\\n",
    "--do_padding=True \\\n",
    "{doremi_options if doremi_options else ''} \\\n",
    "--overwrite_output_dir \\\n",
    "--output_dir={output_dir} \\\n",
    "\"\"\"\n",
    "# --overwrite_output_dir \\\n",
    "\n",
    "import shlex\n",
    "args = shlex.split(cmd)\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, FullTrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n",
    "model_args, data_args, training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d39f3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/08/2023 22:39:20 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "08/08/2023 22:39:20 - INFO - __main__ - Training/evaluation parameters FullTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.99,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=1,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "domain_config_path=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/configs/humanmix_uniform_baseline_50kvocab.json,\n",
      "doremi_optimizer=doremiv1,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=128,\n",
      "gradient_checkpointing=True,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=True,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/jpt_pythia-160m_humanmix_uniform_baseline/runs/Aug08_22-38-52_dcs021,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_end=0.001,\n",
      "lr_scheduler_name=None,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=1562,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/jpt_pythia-160m_humanmix_uniform_baseline,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "reference_model_name_or_path=.,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "reweight_domains=False,\n",
      "reweight_eps=0.0001,\n",
      "reweight_eta=1.0,\n",
      "run_name=train_baseline,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=10,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=1111,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_domain_weights_tmp_file=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "if training_args.should_log:\n",
    "    # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "    \n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8d2de45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "num_skip_examples = 0\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n",
    "        state = TrainerState.load_from_json(str(Path(last_checkpoint) / TRAINER_STATE_NAME))\n",
    "        global_batch_size = training_args.train_batch_size * training_args.gradient_accumulation_steps * training_args.world_size\n",
    "        num_skip_examples = state.global_step * global_batch_size\n",
    "        logger.info(f\"Skipping {num_skip_examples} examples\")\n",
    "        \n",
    "last_checkpoint, num_skip_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e28eed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0f9dc83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:710] 2023-08-08 22:34:04,796 >> loading configuration file /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/EleutherAI/pythia-160m/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-08 22:34:04,798 >> Model config GPTNeoXConfig {\n",
      "  \"_name_or_path\": \"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/EleutherAI/pythia-160m\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoXForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neox\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rotary_emb_base\": 10000,\n",
      "  \"rotary_pct\": 0.25,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.32.0.dev0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_parallel_residual\": true,\n",
      "  \"vocab_size\": 50304\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "\n",
    "config_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "    \"use_cache\": True if not training_args.gradient_checkpointing else False,\n",
    "}\n",
    "if model_args.config_name:\n",
    "    config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
    "elif model_args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "    # wpq: `max_model_length` not set correctly if load pre-downloaded models on disk.\n",
    "    if model_args.model_type == 'gpt_neox_flash':\n",
    "        config = gpt_neox_config_to_gpt2_config(config)\n",
    "        config.use_flash_attn = True\n",
    "        config.fused_mlp = True\n",
    "        config.fused_bias_fc = True\n",
    "        config.fused_dropout_add_ln = True\n",
    "        config.pad_vocab_size_multiple = 8\n",
    "        config.activation_function = 'gelu_new'\n",
    "        config.n_inner = None\n",
    "        # disable absolute\n",
    "        config.max_position_embeddings = 0\n",
    "else:\n",
    "    if model_args.model_type == 'gpt_flash': \n",
    "        config = GPT2Config(\n",
    "                vocab_size=50257, n_positions=2048, n_embd=2048,\n",
    "                n_layer=24, n_head=16, \n",
    "                scale_attn_by_inverse_layer_idx=True, \n",
    "                rotary_emb_fraction=0.5,\n",
    "                use_flash_attn=True, fused_mlp=True,\n",
    "                fused_bias_fc=True, fused_dropout_add_ln=True, \n",
    "                pad_vocab_size_multiple=8)\n",
    "        # disable absolute\n",
    "        config.max_position_embeddings = 0\n",
    "    elif model_args.model_type == 'gpt_neox_flash':\n",
    "        # convert to GPT2 config\n",
    "        config = CONFIG_MAPPING['gpt_neox']() \n",
    "        config = gpt_neox_config_to_gpt2_config(config)\n",
    "        config.use_flash_attn = True\n",
    "        config.fused_mlp = True\n",
    "        config.fused_bias_fc = True\n",
    "        config.fused_dropout_add_ln = True\n",
    "        config.pad_vocab_size_multiple = 8\n",
    "        config.activation_function = 'gelu_new'\n",
    "        config.n_inner = None\n",
    "        # disable absolute\n",
    "        config.max_position_embeddings = 0\n",
    "    else:\n",
    "        config = CONFIG_MAPPING[model_args.model_type]()\n",
    "    # wpq: `max_model_length` not set correctly if load pre-downloaded models on disk.\n",
    "    logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "    if model_args.config_overrides is not None:\n",
    "        logger.info(f\"Overriding config: {model_args.config_overrides}\")\n",
    "        config.update_from_string(model_args.config_overrides)\n",
    "        logger.info(f\"New config: {config}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b29572f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-08-08 22:34:42,119 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-08 22:34:42,120 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-08 22:34:42,121 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-08 22:34:42,122 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-08 22:34:42,123 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-08 22:34:42,124 >> loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoXTokenizerFast(name_or_path='/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/EleutherAI/pythia-160m', vocab_size=50254, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# wpq: `max_model_length` not specified correctly when loading pre-downloaded models from disk.\n",
    "# here specify `max_model_length` so that data collator can function properly.\n",
    "if model_args.model_type == 'gpt2':\n",
    "    model_max_length = 1024\n",
    "elif model_args.model_type in {'llama', 'pythia'}:\n",
    "    model_max_length = 2048\n",
    "else:\n",
    "    model_max_length = None\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"use_fast\": model_args.use_fast_tokenizer,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "    \"model_max_length\": model_max_length,\n",
    "}\n",
    "\n",
    "if model_args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
    "\n",
    "elif model_args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )\n",
    "    \n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83c98bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:2604] 2023-08-08 22:36:27,998 >> loading weights file /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/EleutherAI/pythia-160m/model.safetensors\n",
      "[INFO|configuration_utils.py:603] 2023-08-08 22:36:28,017 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.32.0.dev0\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3333] 2023-08-08 22:36:30,456 >> All model checkpoint weights were used when initializing GPTNeoXForCausalLMDoReMi.\n",
      "\n",
      "[INFO|modeling_utils.py:3341] 2023-08-08 22:36:30,457 >> All the weights of GPTNeoXForCausalLMDoReMi were initialized from the model checkpoint at /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/EleutherAI/pythia-160m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoXForCausalLMDoReMi for predictions without further training.\n",
      "[INFO|modeling_utils.py:2953] 2023-08-08 22:36:30,462 >> Generation config file not found, using a generation config created from the model config.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLMDoReMi(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 768)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
       "  (loss_fct): CrossEntropyLoss()\n",
       "  (pertoken_loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if model_args.model_name_or_path:\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "    if model_args.model_type in {'gpt_flash', 'gpt_neox_flash'}:\n",
    "        model = doremi_models.GPTFlashAttnLMHeadModel.from_pretrained(\n",
    "            model_args.model_name_or_path, config=config)\n",
    "    else:\n",
    "        if model_args.model_type in {'gpt2'}:\n",
    "            model_cls = doremi_models.GPT2LMHeadModelDoReMi\n",
    "        elif model_args.model_type in {'pythia'}:\n",
    "            model_cls = doremi_models.GPTNeoXForCausalLMDoReMi\n",
    "        else:\n",
    "            model_cls = AutoModelForCausalLM\n",
    "        model = model_cls.from_pretrained(\n",
    "            model_args.model_name_or_path, \n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "            torch_dtype=torch_dtype,\n",
    "        )\n",
    "else:\n",
    "    if model_args.model_type in {'gpt_flash', 'gpt_neox_flash'}:\n",
    "        model = doremi_models.GPTFlashAttnLMHeadModel(config)\n",
    "    elif model_args.model_type in {'gpt2'}:\n",
    "        model = doremi_models.GPT2LMHeadModelDoReMi(config)\n",
    "    elif model_args.model_type in {'pythia'}:\n",
    "        model = doremi_models.GPTNeoXForCausalLMDoReMi(config)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "    n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())\n",
    "    logger.info(f\"Training new model from scratch - Total size={n_params/2**20:.2f}M params\")\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6364e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Tuple, Union\n",
    "# from transformers import GPT2LMHeadModel\n",
    "# # from doremi.models import CausalLMOutputWithDomainIDs\n",
    "# from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "# from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# @dataclass\n",
    "# class CausalLMOutputWithDomainIDs(CausalLMOutputWithCrossAttentions):\n",
    "#     domain_ids: Optional[torch.LongTensor] = None\n",
    "#     reference_pertoken_loss: Optional[torch.FloatTensor] = None  # corresponds to uniq_domain_ids\n",
    "#     pertoken_loss: Optional[torch.FloatTensor] = None  # corresponds to uniq_domain_ids\n",
    "#     token_mask: Optional[torch.BoolTensor] = None  # 1 for tokens that are not padding\n",
    "\n",
    "\n",
    "\n",
    "# #             model_args.model_name_or_path,\n",
    "# #             from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "# #             config=config,\n",
    "# #             cache_dir=model_args.cache_dir,\n",
    "# #             revision=model_args.model_revision,\n",
    "# #             use_auth_token=True if model_args.use_auth_token else None,\n",
    "# #             torch_dtype=torch_dtype,\n",
    "\n",
    "# # model = doremi_models.GPTFlashAttnLMHeadModel.from_pretrained(\n",
    "# #     model_args.model_name_or_path, config=config)\n",
    "\n",
    "        \n",
    "# model = GPTLMHeadModelDoReMi.from_pretrained(\n",
    "#     model_args.model_name_or_path, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fe33ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collate_fn = data_utils.get_data_collator(tokenizer, do_padding=data_args.do_padding)\n",
    "\n",
    "# batch = list(train_dataset.take(3))\n",
    "# batch = collate_fn(batch)\n",
    "# batch = {k: v.to('cpu') for k,v in batch.items()}\n",
    "# print(batch['input_ids'].shape)\n",
    "# print(model.device, batch['input_ids'].device)\n",
    "\n",
    "# out = model(**batch, return_pertoken_losses=True, )\n",
    "# print(out.pertoken_loss.shape, out.token_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bf74f1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cot', 'dolly', 'flan_v2', 'oasst1'] 4 {'cot': 0.25, 'flan_v2': 0.25, 'dolly': 0.25, 'oasst1': 0.25}\n",
      "08/08/2023 22:37:06 - WARNING - doremi.dataloader - No split used or split directory not found: using same data for all splits.\n",
      "08/08/2023 22:37:06 - INFO - datasets.builder - Using custom data configuration default-20e8d8f5df875937\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/08/2023 22:37:06 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/08/2023 22:37:06 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fc2ada89aa43779e609cbe0123a2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5e2824c3a7de4516_00000_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5e2824c3a7de4516_00001_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5e2824c3a7de4516_00002_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5e2824c3a7de4516_00003_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5e2824c3a7de4516_00004_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5e2824c3a7de4516_00005_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5e2824c3a7de4516_00006_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5e2824c3a7de4516_00007_of_00008.arrow\n",
      "08/08/2023 22:37:06 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5e2824c3a7de4516_*_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "08/08/2023 22:37:06 - INFO - datasets.builder - Using custom data configuration default-af4ad76b10ec5d4f\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/08/2023 22:37:06 - INFO - datasets.builder - Using custom data configuration default-68fd04897f8e942c\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/08/2023 22:37:06 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/08/2023 22:37:06 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e226a213dc246b59796430455d8de56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-839697b853eec600_00000_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-839697b853eec600_00001_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-839697b853eec600_00002_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-839697b853eec600_00003_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-839697b853eec600_00004_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-839697b853eec600_00005_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-839697b853eec600_00006_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-839697b853eec600_00007_of_00008.arrow\n",
      "08/08/2023 22:37:06 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-839697b853eec600_*_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "08/08/2023 22:37:06 - INFO - datasets.builder - Using custom data configuration default-731e28892caf260a\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/08/2023 22:37:06 - INFO - datasets.builder - Using custom data configuration default-a01381664fd2589b\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/08/2023 22:37:06 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/08/2023 22:37:06 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0c2dad11f14115b749f2502d5578a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-84a80786f2af044e_00000_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-84a80786f2af044e_00001_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-84a80786f2af044e_00002_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-84a80786f2af044e_00003_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-84a80786f2af044e_00004_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-84a80786f2af044e_00005_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-84a80786f2af044e_00006_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-84a80786f2af044e_00007_of_00008.arrow\n",
      "08/08/2023 22:37:06 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-84a80786f2af044e_*_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "08/08/2023 22:37:06 - INFO - datasets.builder - Using custom data configuration default-a7224724b5549c7c\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/08/2023 22:37:06 - INFO - datasets.builder - Using custom data configuration default-d43591dedd2996a1\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/08/2023 22:37:06 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/08/2023 22:37:06 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7abbb697d042b58e2ab6496babcbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-731e81448230ef08_00000_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-731e81448230ef08_00001_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-731e81448230ef08_00002_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-731e81448230ef08_00003_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-731e81448230ef08_00004_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-731e81448230ef08_00005_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-731e81448230ef08_00006_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-731e81448230ef08_00007_of_00008.arrow\n",
      "08/08/2023 22:37:06 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-731e81448230ef08_*_of_00008.arrow\n",
      "08/08/2023 22:37:06 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "08/08/2023 22:37:06 - INFO - datasets.builder - Using custom data configuration default-393bd3b767077de3\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR|tokenization_utils_base.py:1056] 2023-08-08 22:37:06,740 >> Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/08/2023 22:37:06 - INFO - datasets.builder - Using custom data configuration default-96635e9b3ed9d7c9\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/08/2023 22:37:06 - INFO - datasets.builder - Using custom data configuration default-deea1cceea3d2a7b\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/08/2023 22:37:06 - INFO - datasets.builder - Using custom data configuration default-c2c2291c2e13d44c\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/08/2023 22:37:06 - INFO - datasets.builder - Using custom data configuration default-c65a69b70938dfd1\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/08/2023 22:37:06 - INFO - datasets.builder - Using custom data configuration default-0a8470a6b2dbf279\n",
      "08/08/2023 22:37:06 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(training_args.domain_config_path, 'r') as f:\n",
    "    domain_config = json.load(f)\n",
    "\n",
    "train_domain_weights_dict = domain_config['train_domain_weights']\n",
    "eval_domain_weights_dict = domain_config['eval_domain_weights']\n",
    "# whenever we convert dict to array, we sort by key\n",
    "domain_list = list(sorted(train_domain_weights_dict.keys()))\n",
    "num_domains = len(domain_list)\n",
    "\n",
    "print(domain_list, num_domains, train_domain_weights_dict)\n",
    "\n",
    "if training_args.do_train:\n",
    "    # data script could change tokenizer shape\n",
    "    train_dataset = data_utils.get_preprocessed_mixed_dataset(\n",
    "            preprocessed_dir=data_args.dataset_dir,\n",
    "            domain_weights_dict=train_domain_weights_dict,\n",
    "            dataset_name=data_args.dataset_name,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            split='train',\n",
    "            max_samples=data_args.max_train_samples,\n",
    "            add_domain_id=data_args.add_domain_id,\n",
    "            tmp_file=None,\n",
    "            seed=training_args.seed,\n",
    "            tokenizer=tokenizer,\n",
    "            shuffle=data_args.shuffle,\n",
    "            num_skip_examples=num_skip_examples,\n",
    "            shard_reversal=training_args.reweight_domains,\n",
    "            training_args=training_args,\n",
    "            data_args=data_args,\n",
    "    )\n",
    "\n",
    "if training_args.do_eval:\n",
    "    eval_dataset = data_utils.get_preprocessed_mixed_dataset(\n",
    "            preprocessed_dir=data_args.dataset_dir,\n",
    "            domain_weights_dict=eval_domain_weights_dict,\n",
    "            dataset_name=data_args.dataset_name,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            split='validation',\n",
    "            add_domain_id=data_args.add_domain_id,\n",
    "            max_samples=data_args.max_eval_samples,\n",
    "            tokenizer=tokenizer,\n",
    "            no_interleave=True,\n",
    "            training_args=training_args,\n",
    "            data_args=data_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4e911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# preprocessed_dir=data_args.dataset_dir\n",
    "# domain_weights_dict=train_domain_weights_dict\n",
    "# dataset_name=data_args.dataset_name\n",
    "# cache_dir=model_args.cache_dir\n",
    "# split='train'\n",
    "# max_samples=data_args.max_train_samples\n",
    "# add_domain_id=data_args.add_domain_id\n",
    "# tmp_file=None\n",
    "# seed=training_args.seed\n",
    "# tokenizer=tokenizer\n",
    "# shuffle=data_args.shuffle\n",
    "# num_skip_examples=num_skip_examples\n",
    "# shard_reversal=training_args.reweight_domains\n",
    "# no_interleave=False\n",
    "\n",
    "# print(preprocessed_dir)\n",
    "# print(domain_weights_dict)\n",
    "# print(dataset_name)\n",
    "# print(cache_dir)\n",
    "# print(split)\n",
    "# print(max_samples)\n",
    "# print(add_domain_id)\n",
    "# print(seed)\n",
    "# print(shuffle)\n",
    "# print(num_skip_examples)\n",
    "# print(shard_reversal)\n",
    "\n",
    "\n",
    "# domain_names = list(sorted(domain_weights_dict.keys()))\n",
    "# domain_to_idx = {domain_names[i]: i for i in range(len(domain_names))}\n",
    "# domain_weights = np.asarray([domain_weights_dict[domain_name] for domain_name in domain_names])\n",
    "# domain_weights = domain_weights / domain_weights.sum()\n",
    "\n",
    "# print()\n",
    "# print(json.dumps({'domain_names': domain_names, \n",
    "#                   'domain_to_idx': domain_to_idx, \n",
    "#                   'domain_weights': list(domain_weights)},\n",
    "#                 indent=4))\n",
    "\n",
    "\n",
    "# # write domain weights to file if tmp_file is set\n",
    "# if tmp_file is not None:\n",
    "#     probabilities_tmp_file = tmp_file\n",
    "\n",
    "#     with open(str(probabilities_tmp_file), 'wb') as f:\n",
    "#         pickle.dump(domain_weights, f)\n",
    "#     probabilities = None\n",
    "# else:\n",
    "#     probabilities = domain_weights\n",
    "#     probabilities_tmp_file = None\n",
    "\n",
    "\n",
    "# print()\n",
    "# print(json.dumps({'probabilities': list(probabilities)}, indent=4))\n",
    "\n",
    "# # from doremi.dataloader import get_perdomain_datasets\n",
    "# # all_ds = get_perdomain_datasets(\n",
    "# #     preprocessed_dir, \n",
    "# #     domain_weights_dict,\n",
    "# #     cache_dir=cache_dir,\n",
    "# #     split=split,\n",
    "# #     seed=seed,\n",
    "# #     domain_weights=domain_weights,\n",
    "# #     domain_names=domain_names,\n",
    "# #     num_skip_examples=num_skip_examples,\n",
    "# #     shuffle=shuffle,\n",
    "# #     shard_reversal=shard_reversal\n",
    "# # )\n",
    "\n",
    "# domain_name_to_skip_num = determine_skip_per_domain(num_skip_examples, seed, domain_weights, domain_names)\n",
    "\n",
    "# preprocessed_dir = Path(preprocessed_dir)\n",
    "# if split is not None and (preprocessed_dir / split).exists():\n",
    "#     preprocessed_dir = preprocessed_dir / split\n",
    "# else:\n",
    "#     logger.warn(f\"No split used or split directory not found: using same data for all splits.\")\n",
    "\n",
    "# domains = list(sorted(domain_weights_dict.keys()))\n",
    "\n",
    "# print(preprocessed_dir)\n",
    "# print(domain_name_to_skip_num)\n",
    "# print()\n",
    "# print(json.dumps({'preprocessed_dir': str(preprocessed_dir), \n",
    "#                   'domain_name_to_skip_num': domain_name_to_skip_num}, indent=4))\n",
    "\n",
    "\n",
    "# all_ds = {}\n",
    "# for domain in domains:\n",
    "#     domain_dir = preprocessed_dir / domain\n",
    "    \n",
    "#     ## wpq: read instruction tuning dataset off `jsonl` files\n",
    "#     if (domain_dir / f'{domain}_data.jsonl').exists():\n",
    "#         from datasets import load_dataset\n",
    "#         from functools import partial\n",
    "#         from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n",
    "#         from doremi.dataloader import skippable_data_gen_dataset\n",
    "\n",
    "#         data_files = {'train': str(domain_dir / f'{domain}_data.jsonl')}\n",
    "#         raw_datasets = load_dataset(\n",
    "#             \"json\",\n",
    "#             data_files=data_files,\n",
    "#             cache_dir=cache_dir,\n",
    "#             use_auth_token=True if model_args.use_auth_token else None,\n",
    "#         )\n",
    "#         # Preprocessing the datasets.\n",
    "#         if \"prompt\" in raw_datasets[\"train\"].column_names and \"completion\" in raw_datasets[\"train\"].column_names:\n",
    "#             encode_function = partial(\n",
    "#                 encode_with_prompt_completion_format,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 max_seq_length=1024,\n",
    "#             )\n",
    "#         elif \"messages\" in raw_datasets[\"train\"].column_names:\n",
    "#             encode_function = partial(\n",
    "#                 encode_with_messages_format,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 max_seq_length=1024,\n",
    "#             )\n",
    "#         else:\n",
    "#             raise ValueError(\"You need to have either 'prompt'&'completion' or 'messages' in your column names.\")\n",
    "\n",
    "#         with training_args.main_process_first(local=False, desc=\"Processing instruction data\"):\n",
    "#             lm_datasets = raw_datasets.map(\n",
    "#                 encode_function,\n",
    "#                 num_proc=16,\n",
    "#                 batched=False,\n",
    "#             )\n",
    "#             lm_datasets.set_format(type=\"pt\")\n",
    "#         ds = lm_datasets['train']\n",
    "#         ds = IterableDataset.from_generator(\n",
    "#                 skippable_data_gen_dataset,\n",
    "#                 gen_kwargs={'ds': ds,\n",
    "#                             'num_skip_examples': domain_name_to_skip_num[domain],\n",
    "#                             'loop': (split == 'train'),\n",
    "#                             'seed': seed,\n",
    "#                             'shuffle': shuffle}\n",
    "#                 )\n",
    "#         seed += 1\n",
    "#     elif (domain_dir / 'dataset_info.json').exists():\n",
    "#         ds = load_from_disk(dataset_path=str(domain_dir))\n",
    "#         logger.info(f\"Loaded {domain_dir}. Length: {len(ds)}\")\n",
    "#     else:\n",
    "#         curr_shards = list(domain_dir.iterdir())\n",
    "#         if shard_reversal:\n",
    "#             curr_shards = list(reversed(curr_shards))\n",
    "#         # shuffle shard order\n",
    "#         random.Random(seed).shuffle(curr_shards)\n",
    "#         ds = IterableDataset.from_generator(\n",
    "#                 skippable_data_gen,\n",
    "#                 gen_kwargs={'shards': curr_shards,\n",
    "#                             'num_skip_examples': domain_name_to_skip_num[domain],\n",
    "#                             'loop': (split == 'train'),\n",
    "#                             'seed': seed,\n",
    "#                             'shuffle': shuffle}\n",
    "#                 )\n",
    "#         seed += 1\n",
    "#     all_ds[domain] = ds\n",
    "    \n",
    "\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# def add_domain_id_generator(ds, domain_idx):\n",
    "#     for ex in ds:\n",
    "#         ex['domain_id'] = domain_idx\n",
    "#         yield ex\n",
    "        \n",
    "# domain_ds_ls = []\n",
    "# for domain_name in domain_names:\n",
    "#     domain_idx = domain_to_idx[domain_name]\n",
    "#     domain_ds = all_ds[domain_name]\n",
    "#     # add domain_id if necessary\n",
    "#     if add_domain_id:\n",
    "#         domain_ds = IterableDataset.from_generator(\n",
    "#             add_domain_id_generator, \n",
    "#             gen_kwargs={'ds': domain_ds, 'domain_idx': domain_idx})\n",
    "#     domain_ds_ls.append(domain_ds)\n",
    "\n",
    "# if no_interleave:\n",
    "#     # instead of interleaving, run through each dataset\n",
    "#     def data_generator(shards):\n",
    "#         for shard in shards:\n",
    "#             for ex in shard:\n",
    "#                 yield ex\n",
    "#     ds = IterableDataset.from_generator(data_generator, gen_kwargs={'shards': domain_ds_ls})\n",
    "#     logger.info(\"Not interleaving dataset - will not sample according to domain weights\")\n",
    "\n",
    "# else:\n",
    "#     ds = interleave_datasets(\n",
    "#             domain_ds_ls,\n",
    "#             probabilities=probabilities,\n",
    "#             probabilities_file=probabilities_tmp_file,\n",
    "#             seed=seed)\n",
    "    \n",
    "\n",
    "# def take_data_generator(ds, max_samples):\n",
    "#     idx = 0\n",
    "#     for ex in ds:\n",
    "#         yield ex\n",
    "#         idx += 1\n",
    "#         if max_samples is not None and idx >= max_samples:\n",
    "#             return\n",
    "\n",
    "# ds = IterableDataset.from_generator(take_data_generator, gen_kwargs={'ds': ds, 'max_samples': max_samples})\n",
    "# train_dataset = ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5426031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ds = load_dataset(\n",
    "#     \"json\",\n",
    "#     data_files='test.jsonl',\n",
    "#     cache_dir=model_args.cache_dir)['train']\n",
    "# test_ds[0]\n",
    "\n",
    "# for x in test_ds.to_iterable_dataset():\n",
    "#     print(x)\n",
    "# for i, v in enumerate(ds):\n",
    "#     if i == 10:\n",
    "#         break\n",
    "#     print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dba9182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if training_args.reweight_domains:\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "    if model_args.model_type in {'gpt_flash', 'gpt_neox_flash'}:\n",
    "        model_cls = doremi_models.GPTFlashAttnLMHeadModel\n",
    "        reference_model = model_cls.from_pretrained(\n",
    "            training_args.reference_model_name_or_path,\n",
    "            config=config)\n",
    "    else:\n",
    "        if model_args.model_type in {'gpt2'}:\n",
    "            model_cls = doremi_models.GPT2LMHeadModelDoReMi\n",
    "        elif model_args.model_type in {'pythia'}:\n",
    "            model_cls = doremi_models.GPTNeoXForCausalLMDoReMi\n",
    "        else:\n",
    "            model_cls = AutoModelForCausalLM\n",
    "        reference_model = model_cls.from_pretrained(\n",
    "            training_args.reference_model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "            torch_dtype=torch_dtype,\n",
    "        )\n",
    "    for param in reference_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    reference_model.eval()\n",
    "    model.reference_model = reference_model\n",
    "    # initialize 0 as _ref\n",
    "    model.register_buffer('train_domain_weights', torch.tensor(\n",
    "            [train_domain_weights_dict[domain] for domain in domain_list]))\n",
    "    model.register_buffer('avg_domain_weights', model.train_domain_weights.clone())\n",
    "    model.register_buffer('perdomain_scores', torch.ones(len(train_domain_weights_dict)) * np.log(len(tokenizer)))\n",
    "    model.register_buffer('update_counter', torch.tensor(1))\n",
    "\n",
    "else:\n",
    "    reference_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3789d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:565] 2023-08-08 22:39:03,164 >> max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<doremi.trainer.DoReMiTrainer at 0x7ffed9974040>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# turn off find unused parameters\n",
    "training_args.ddp_find_unused_parameters = False\n",
    "\n",
    "# We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "# on a small vocab and want a smaller embedding size, remove this test.\n",
    "# embedding_size = model.get_input_embeddings.weight.shape[0]\n",
    "# if len(tokenizer) > embedding_size:\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = DoReMiTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_utils.get_data_collator(\n",
    "        tokenizer, do_padding=data_args.do_padding, max_seq_length=data_args.max_token_length),\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "872a02f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "False\n",
      "1\n",
      "0\n",
      "False\n",
      "[]\n",
      "[]\n",
      "False\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(trainer.args.max_grad_norm)\n",
    "print(trainer.do_grad_scaling)\n",
    "print(trainer.args.n_gpu)\n",
    "print(trainer.args.local_rank)\n",
    "print(trainer.is_model_parallel)\n",
    "print(trainer.args.sharded_ddp)\n",
    "print(trainer.args.fsdp)\n",
    "print(trainer.is_fsdp_enabled)\n",
    "print(trainer.args.distributed_state)\n",
    "print(trainer.args.world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38a27bcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1682] 2023-08-08 22:39:04,866 >> ***** Running training *****\n",
      "[INFO|trainer.py:1683] 2023-08-08 22:39:04,867 >>   Num examples = 199,936\n",
      "[INFO|trainer.py:1684] 2023-08-08 22:39:04,868 >>   Num Epochs = 9,223,372,036,854,775,807\n",
      "[INFO|trainer.py:1685] 2023-08-08 22:39:04,869 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1688] 2023-08-08 22:39:04,870 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "[INFO|trainer.py:1689] 2023-08-08 22:39:04,871 >>   Gradient Accumulation steps = 128\n",
      "[INFO|trainer.py:1690] 2023-08-08 22:39:04,872 >>   Total optimization steps = 1,562\n",
      "[INFO|trainer.py:1691] 2023-08-08 22:39:04,874 >>   Number of trainable parameters = 162,322,944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/08/2023 22:39:04 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2711299ea724aa2d.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer_utils.py:696] 2023-08-08 22:39:04,922 >> The following columns in the training set don't have a corresponding argument in `GPTNeoXForCausalLMDoReMi.forward` and have been ignored: domain_id. If domain_id are not expected by `GPTNeoXForCausalLMDoReMi.forward`,  you can safely ignore this message.\n",
      "[WARNING|logging.py:280] 2023-08-08 22:39:04,923 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/08/2023 22:39:04 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5e467f527ede3e03.arrow\n",
      "08/08/2023 22:39:04 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0ed22c958e1c1445.arrow\n",
      "08/08/2023 22:39:05 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26e80827a48fbfab.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "\n",
    "    if training_args.reweight_domains:\n",
    "        avg_domain_weights_dict = {}\n",
    "        for i in range(len(model.avg_domain_weights)):\n",
    "            domain_name = domain_list[i]\n",
    "            metrics[f'avg_domain_weight:{domain_name}'] = model.avg_domain_weights[i].item()\n",
    "            avg_domain_weights_dict[domain_name] = model.avg_domain_weights[i].item()\n",
    "\n",
    "        # save avg domain weights to json\n",
    "        avg_domain_weights_file = Path(training_args.output_dir) / 'avg_domain_weights.json'\n",
    "        with open(avg_domain_weights_file, 'w') as f:\n",
    "            json.dump(avg_domain_weights_dict, f, indent=2)\n",
    "\n",
    "        # also save to configs dir\n",
    "        config_dict = {\"train_domain_weights\": avg_domain_weights_dict,\n",
    "                       \"eval_domain_weights\": avg_domain_weights_dict}\n",
    "        config_dict_file = Path(__file__).parent.parent / 'configs' / f\"{Path(training_args.output_dir).name}.json\"\n",
    "        with open(config_dict_file, 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3112ea50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/08/2023 22:40:58 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2711299ea724aa2d.arrow\n",
      "08/08/2023 22:40:58 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5e467f527ede3e03.arrow\n",
      "08/08/2023 22:40:58 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0ed22c958e1c1445.arrow\n",
      "tensor(2.3500, device='cuda:0', grad_fn=<DivBackward0>) torch.Size([3, 1023]) torch.Size([3, 1023])\n"
     ]
    }
   ],
   "source": [
    "data_collator=data_utils.get_data_collator(tokenizer, do_padding=data_args.do_padding)\n",
    "\n",
    "inputs = list(train_dataset.take(3))\n",
    "inputs = data_collator(inputs)\n",
    "\n",
    "outputs = model.forward(**{k:v.to('cuda') for k,v in inputs.items()}, return_pertoken_losses=True)\n",
    "\n",
    "print(outputs.loss, outputs.pertoken_loss.shape, outputs.token_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "584e6abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/08/2023 22:39:33 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2711299ea724aa2d.arrow\n",
      "08/08/2023 22:39:33 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5e467f527ede3e03.arrow\n",
      "08/08/2023 22:39:33 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0ed22c958e1c1445.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:280] 2023-08-08 22:39:33,825 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'reference_pertoken_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_inputs(inputs)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m---> 11\u001b[0m     loss, pertoken_loss, reference_pertoken_loss, token_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_pertoken_losses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     excess_loss \u001b[38;5;241m=\u001b[39m pertoken_loss \u001b[38;5;241m-\u001b[39m reference_pertoken_loss\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mshape, pertoken_loss\u001b[38;5;241m.\u001b[39mshape, token_mask\u001b[38;5;241m.\u001b[39mshape, \n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/doremi/trainer.py:252\u001b[0m, in \u001b[0;36mDoReMiTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, return_pertoken_losses)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (loss, outputs)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_pertoken_losses:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (loss,\n\u001b[1;32m    251\u001b[0m             outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpertoken_loss\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m--> 252\u001b[0m             \u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreference_pertoken_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    253\u001b[0m             outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/utils/generic.py:318\u001b[0m, in \u001b[0;36mModelOutput.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    317\u001b[0m     inner_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tuple()[k]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'reference_pertoken_loss'"
     ]
    }
   ],
   "source": [
    "data_collator=data_utils.get_data_collator(tokenizer, do_padding=data_args.do_padding)\n",
    "self = trainer\n",
    "\n",
    "inputs = list(train_dataset.take(3))\n",
    "inputs = data_collator(inputs)\n",
    "\n",
    "model.train()\n",
    "inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "with self.compute_loss_context_manager():\n",
    "    loss, pertoken_loss, reference_pertoken_loss, token_mask = self.compute_loss(model, inputs, return_pertoken_losses=True)\n",
    "    excess_loss = pertoken_loss - reference_pertoken_loss\n",
    "\n",
    "loss.shape, pertoken_loss.shape, token_mask.shape, \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ba699",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = trainer\n",
    "\n",
    "pertoken_scores = excess_loss\n",
    "token_masks = token_mask.bool()\n",
    "domain_ids = inputs['domain_ids']\n",
    "\n",
    "# self.update_domain_weights(pertoken_scores, token_masks, domain_ids)\n",
    "\n",
    "scores = pertoken_scores\n",
    "scores_mask = token_masks\n",
    "\n",
    "\n",
    "train_domain_weights = self.read_weights()\n",
    "\n",
    "scores = scores.detach()\n",
    "domain_ids = domain_ids.detach()\n",
    "\n",
    "\n",
    "perdomain_scores = []\n",
    "for domain_id in range(len(train_domain_weights)):\n",
    "    domain_mask = (domain_ids == domain_id)\n",
    "    perdomain_scores_mask = scores_mask[domain_mask]\n",
    "    if domain_mask.sum() > 0:\n",
    "        curr_domain_scores = torch.clip(scores[domain_mask][perdomain_scores_mask], min=0).mean()\n",
    "    else:\n",
    "        curr_domain_scores = self.model.perdomain_scores[domain_id]\n",
    "    perdomain_scores.append(curr_domain_scores)\n",
    "\n",
    "# `perdomain_scores` with size (#domains,) is the `` in the paper, \n",
    "#     e.g., the gradient of loss w.r.t.  dL/d\n",
    "self.model.perdomain_scores[:] = torch.tensor(perdomain_scores)\n",
    "\n",
    "# `train_domain_weights` is ; `log_new_train_domain_weights` is log(a_t')\n",
    "# following implements: _t' = _{t-1} exp(_t), e.g., log(a_t') = log(_{t-1}) + _t\n",
    "log_new_train_domain_weights = torch.log(train_domain_weights+1e-30) + self.args.reweight_eta * self.model.perdomain_scores\n",
    "\n",
    "# following implements: \n",
    "# softmax(log(_t')) = (exp(log(a_t'[1])) /  exp(log(a_t'[i])), ...)\n",
    "#                    = (a_t'[1]/ a_t'[i], ...)\n",
    "new_train_domain_weights = nn.functional.softmax(log_new_train_domain_weights, dim=-1)\n",
    "\n",
    "# following implements:\n",
    "#  = (1-c)softmax(log(_t')) + cu where c=1e-4 by default, and u=1/#domains.\n",
    "train_domain_weights = (1-self.args.reweight_eps) * new_train_domain_weights + self.args.reweight_eps / len(new_train_domain_weights)\n",
    "\n",
    "# wpq: clip the weights to [, 1-] just to make sure.\n",
    "eps = 1e-8\n",
    "train_domain_weights = torch.clip(train_domain_weights, min=eps, max=1-eps)\n",
    "\n",
    "self.write_weights(train_domain_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794732f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pertoken_loss.shape, token_mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dccfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doremiv2\n",
    "import torch.distributed as dist\n",
    "\n",
    "# compute the rescaled loss, divide by domain weights\n",
    "train_domain_weights = self.read_weights().to(pertoken_loss.device)\n",
    "# if doing non-uniform sampling, normalize by inverse sampling weight\n",
    "train_domain_weights = train_domain_weights / self.sampling_weights.to(train_domain_weights.device)\n",
    "train_domain_weights = train_domain_weights / train_domain_weights.sum()\n",
    "\n",
    "curr_domain_weights = train_domain_weights[inputs['domain_ids']].unsqueeze(-1).expand_as(pertoken_loss).detach()\n",
    "curr_domain_weights = curr_domain_weights * token_mask\n",
    "\n",
    "normalizer = curr_domain_weights.sum()\n",
    "\n",
    "## wpq: remove since `token_mask` not used any more.\n",
    "# token_mask = token_mask.detach().type(pertoken_loss.dtype)\n",
    "curr_domain_weights = curr_domain_weights / normalizer # sum to 1.\n",
    "## wpq: clip `curr_domain_weights` to [0, 1]\n",
    "curr_domain_weights = torch.clip(curr_domain_weights, min=1e-8, max=1-1e-8)\n",
    "loss = (pertoken_loss * curr_domain_weights.detach()).sum()\n",
    "loss\n",
    "\n",
    "\n",
    "\n",
    "_i _i _{x\\in D} |x|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d41e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_domain_weights = self.read_weights().to(pertoken_loss.device)\n",
    "# if doing non-uniform sampling, normalize by inverse sampling weight\n",
    "train_domain_weights = train_domain_weights / self.sampling_weights.to(train_domain_weights.device)\n",
    "train_domain_weights = train_domain_weights / train_domain_weights.sum()\n",
    "\n",
    "# (#domains,) total number of tokens amongst samples from each domain\n",
    "perdomain_num_tokens = []\n",
    "for domain_id in range(len(train_domain_weights)):\n",
    "    domain_mask = (inputs['domain_ids'] == domain_id)\n",
    "    if domain_mask.sum() > 0:\n",
    "        num_tokens = token_mask[domain_mask].sum()\n",
    "    else:\n",
    "        num_tokens = torch.tensor(0., device=token_mask.device)\n",
    "    perdomain_num_tokens.append(num_tokens)\n",
    "perdomain_num_tokens = torch.stack(perdomain_num_tokens)\n",
    "\n",
    "# avoid division by zero\n",
    "perdomain_num_tokens[torch.where(perdomain_num_tokens==0)] = 1.\n",
    "# (#domains,) equivalent to  / _{x\\in D_i} |x|\n",
    "perdomain_coeff = train_domain_weights/perdomain_num_tokens\n",
    "# (bsz, seq_len-1)\n",
    "coeff = perdomain_coeff[inputs['domain_ids']].unsqueeze(-1) * token_mask\n",
    "loss = (pertoken_loss * coeff.detach()).sum()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0345a1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d8af7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acfe8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# avoid division by zero\n",
    "perdomain_num_tokens[torch.where(perdomain_num_tokens==0)] = 1.\n",
    "# (#domains,) equivalent to  / _{x\\in D_i} |x|\n",
    "perdomain_coeff = train_domain_weights/perdomain_num_tokens\n",
    "# (bsz, seq_len-1)\n",
    "coeff = perdomain_coeff[inputs['domain_ids']].unsqueeze(-1) * token_mask\n",
    "loss = (pertoken_loss * coeff.detach()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0cb3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed4216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the rescaled loss, divide by domain weights\n",
    "train_domain_weights = self.read_weights().to(pertoken_loss.device)\n",
    "# if doing non-uniform sampling, normalize by inverse sampling weight\n",
    "train_domain_weights = train_domain_weights / self.sampling_weights.to(train_domain_weights.device)\n",
    "train_domain_weights = train_domain_weights / train_domain_weights.sum()\n",
    "\n",
    "curr_domain_weights = train_domain_weights[inputs['domain_ids']].unsqueeze(-1).expand_as(pertoken_loss).detach()\n",
    "curr_domain_weights = curr_domain_weights * token_mask\n",
    "\n",
    "normalizer = curr_domain_weights.sum()\n",
    "\n",
    "## wpq: remove since `token_mask` not used any more.\n",
    "# token_mask = token_mask.detach().type(pertoken_loss.dtype)\n",
    "curr_domain_weights = curr_domain_weights / normalizer # sum to 1.\n",
    "## wpq: clip `curr_domain_weights` to [0, 1]\n",
    "curr_domain_weights = torch.clip(curr_domain_weights, min=1e-8, max=1-1e-8)\n",
    "loss = (pertoken_loss * curr_domain_weights.detach()).sum()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7cd299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# token_mask = token_mask.detach().type(pertoken_loss.dtype)\n",
    "# curr_domain_weights = curr_domain_weights / normalizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756aee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute the rescaled loss, divide by domain weights\n",
    "# train_domain_weights = self.read_weights().to(pertoken_loss.device)\n",
    "# if doing non-uniform sampling, normalize by inverse sampling weight\n",
    "train_domain_weights = train_domain_weights / self.sampling_weights.to(train_domain_weights.device)\n",
    "train_domain_weights = train_domain_weights / train_domain_weights.sum()\n",
    "train_domain_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
