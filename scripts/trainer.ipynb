{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334172b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_setup; jpt_setup()\n",
    "import os; os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9294819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-07 18:03:05,072] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import logging\n",
    "import math\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import chain\n",
    "from typing import Optional\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "import pyarrow\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset, IterableDataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.trainer_callback import TrainerState\n",
    "from transformers.trainer import TRAINER_STATE_NAME\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "\n",
    "from doremi.training_args import ModelArguments, DataTrainingArguments, FullTrainingArguments\n",
    "import doremi.dataloader as data_utils\n",
    "from doremi.trainer import DoReMiTrainer\n",
    "from doremi.dataloader import determine_skip_per_domain\n",
    "from doremi.dataloader import interleave_datasets\n",
    "\n",
    "\n",
    "try:\n",
    "    import doremi.models as doremi_models\n",
    "except Exception:\n",
    "    \n",
    "    pass\n",
    "try:\n",
    "    from flash_attn.models.gpt_neox import gpt_neox_config_to_gpt2_config\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.27.0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/language-modeling/requirements.txt\")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47cef8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_dir = \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi\"\n",
    "cache_dir = '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache'\n",
    "preprocessed_data = \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed\"\n",
    "\n",
    "envs = {\n",
    "    \"CACHE\": cache_dir,\n",
    "    \"DOREMI_DIR\": package_dir,\n",
    "    \"PILE_DIR\": os.path.join(package_dir, \"data\", 'raw'),\n",
    "    \"PREPROCESSED_PILE_DIR\": preprocessed_data,\n",
    "    \"MODEL_OUTPUT_DIR\": os.path.join(package_dir, 'results'),\n",
    "    \"PARTITION\": \"el8\",\n",
    "    \"HF_HOME\": cache_dir,\n",
    "    \"TRANSFORMERS_CACHE\": cache_dir,\n",
    "    \"HF_DATASETS_CACHE\": cache_dir,\n",
    "    \"HF_DATASETS_IN_MEMORY_MAX_SIZE\": \"0\",\n",
    "    \"TORCH_EXTENSIONS_DIR\": cache_dir,\n",
    "    \"TMPDIR\": cache_dir,\n",
    "    \"WANDB_DIR\": os.path.join(cache_dir, \"wandb\"),\n",
    "    \"WANDB_MODE\": 'offline',\n",
    "    \"PREPROCESSED_DATA\": preprocessed_data,\n",
    "    'PREPROCESSED_CACHE': os.path.join(cache_dir, 'preprocessed_cache', 'perdomain_pile_preprocessed'),\n",
    "\n",
    "}\n",
    "\n",
    "for k, v in envs.items():\n",
    "    os.environ[k] = v\n",
    "    \n",
    "os.makedirs(cache_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb14c3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1301] 2023-08-07 18:11:44,436 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "[INFO|training_args.py:1716] 2023-08-07 18:11:44,437 >> PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/ft1/gpt2-medium_humanmix_doremi using 1 GPUs, 1 batch size per GPU, 128 gradient accumulation steps,for 1562 max steps.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ModelArguments(model_name_or_path='/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/ft1/gpt2-medium_humanmix_doremi', model_type='gpt2', config_overrides=None, config_name=None, tokenizer_name=None, cache_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache', use_fast_tokenizer=True, model_revision='main', use_auth_token=False, torch_dtype=None),\n",
       " DataTrainingArguments(dataset_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed', dataset_name='', max_train_samples=None, max_eval_samples=None, max_token_length=1024, block_size=None, overwrite_cache=False, do_padding=True, add_domain_id=True, preprocessing_num_workers=None, shuffle=True),\n",
       " FullTrainingArguments(output_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/jpt_gpt2-medium_humanmix_doremi', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=128, eval_accumulation_steps=None, eval_delay=0, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.99, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=1562, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/jpt_gpt2-medium_humanmix_doremi/runs/Aug07_18-11-44_dcs043', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=True, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=10, save_total_limit=1, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=1111, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='train_baseline', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_HF: 'adamw_hf'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard', 'wandb'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, domain_config_path='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/configs/humanmix_uniform_baseline_50kvocab.json', lr_end=0.001, reweight_domains=True, reweight_eta=1.0, reweight_eps=0.0001, doremi_optimizer='doremiv1', reference_model_name_or_path='/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/ft1/gpt2-medium_humanmix_baseline/', lr_scheduler_name=None, train_domain_weights_tmp_file='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/jpt_gpt2-medium_humanmix_doremi/domain_weights'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_name = 'train_baseline'\n",
    "\n",
    "nodes = 1\n",
    "num_gpus = 1\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "use_doremi = True\n",
    "# model_name_or_path = ('/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/'\n",
    "#                       'gpt2-medium')\n",
    "model_name_or_path = ('/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/'\n",
    "                      'ft1/gpt2-medium_humanmix_doremi')\n",
    "model_type = 'gpt2'; abbr_model_name = 'gpt2-medium'\n",
    "\n",
    "cache_dir = envs['CACHE']\n",
    "domain_config_path = os.path.abspath('../configs/humanmix_uniform_baseline_50kvocab.json')\n",
    "dataset_dir = preprocessed_data\n",
    "\n",
    "\n",
    "total_batch_size = 128 # # 64*8=512\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_accumulation_steps = int(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "max_steps = int(200000/total_batch_size); save_steps = 10 # 200k steps.\n",
    "gradient_checkpointing = False\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{per_device_train_batch_size} batch size per GPU, \"\n",
    "      f\"{gradient_accumulation_steps} gradient accumulation steps,\"\n",
    "      f\"for {max_steps} max steps.\")\n",
    "\n",
    "# use `dataset_dir` instead of `dataset_name` to specify `preprocessed_dir`\n",
    "# --dataset_name=pile \\\n",
    "\n",
    "## learning rate for pretraining, substituted with finetuning hyperparameters\n",
    "# --learning_rate 1e-3 \\\n",
    "# --lr_end 1e-4 \\\n",
    "# --adam_epsilon 1e-8 \\\n",
    "\n",
    "## don't need cosine scheduling for finetuning\n",
    "# --weight_decay 0.01 \\\n",
    "# --lr_scheduler_name linear_warmup_cosine \\\n",
    "# --warmup_ratio 0.06 \\\n",
    "\n",
    "## avoids grad scaling error\n",
    "# --fp16 \\\n",
    "## for training model from scratch\n",
    "# --config_overrides=\"n_positions=1024,n_embd=1024,n_layer=18,n_head=16\" \\\n",
    "\n",
    "## added the following\n",
    "# add_domain_id: for non-pile preprocessed dataset\n",
    "# do_padding: true for variable size sequences, as in instruction tuning datasets.\n",
    "# --max_train_samples 1000 \\\n",
    "\n",
    "abbr_train_file = 'humanmix'\n",
    "output_dirname = f'{abbr_model_name}_{abbr_train_file}'\n",
    "if use_doremi:\n",
    "    output_dirname += '_doremi'\n",
    "else:\n",
    "    output_dirname += '_baseline'\n",
    "if test_run:\n",
    "    output_dirname = 'jpt_'+output_dirname\n",
    "output_dir = os.path.join(envs['MODEL_OUTPUT_DIR'], job_name, output_dirname)\n",
    "\n",
    "\n",
    "if use_doremi:\n",
    "    reference_model_name_or_path = (\n",
    "        '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/'\n",
    "        'ft1/gpt2-medium_humanmix_baseline/')\n",
    "    doremi_options = f\"\"\"\n",
    "        --doremi_optimizer=doremiv1 \\\n",
    "        --reweight_eta=1 \\\n",
    "        --reweight_eps=1e-4 \\\n",
    "        --train_domain_weights_tmp_file={os.path.join(output_dir, 'domain_weights')} \\\n",
    "        --reweight_domains \\\n",
    "        --remove_unused_columns=False \\\n",
    "        --reference_model_name_or_path={reference_model_name_or_path} \\\n",
    "    \"\"\"\n",
    "else:\n",
    "    doremi_options = ''\n",
    "\n",
    "cmd = f\"\"\"\n",
    "--model_name_or_path={model_name_or_path} \\\n",
    "--model_type={model_type} \\\n",
    "--do_train \\\n",
    "--cache_dir={cache_dir} \\\n",
    "--dataset_dir={dataset_dir} \\\n",
    "--domain_config_path={domain_config_path} \\\n",
    "--max_token_length=1024 \\\n",
    "--per_device_train_batch_size={per_device_train_batch_size} \\\n",
    "--gradient_accumulation_steps={gradient_accumulation_steps} \\\n",
    "--dataloader_num_workers=1 \\\n",
    "--learning_rate=2e-5 \\\n",
    "--lr_scheduler_type=linear \\\n",
    "--warmup_ratio=0.03 \\\n",
    "--weight_decay=0. \\\n",
    "--max_grad_norm=1.0 \\\n",
    "--max_steps={max_steps} \\\n",
    "--evaluation_strategy=no \\\n",
    "--save_strategy=steps \\\n",
    "--save_steps={save_steps} \\\n",
    "--save_total_limit=1 \\\n",
    "--run_name={job_name} \\\n",
    "--seed=1111 \\\n",
    "--logging_strategy=steps \\\n",
    "--logging_steps=10 \\\n",
    "--logging_first_step \\\n",
    "--report_to='all' \\\n",
    "--optim=adamw_hf \\\n",
    "--adam_beta1=0.9 \\\n",
    "--adam_beta2=0.99 \\\n",
    "--gradient_checkpointing \\\n",
    "--add_domain_id=True \\\n",
    "--do_padding=True \\\n",
    "{doremi_options if doremi_options else ''} \\\n",
    "--overwrite_output_dir \\\n",
    "--output_dir={output_dir} \\\n",
    "\"\"\"\n",
    "# --overwrite_output_dir \\\n",
    "\n",
    "import shlex\n",
    "args = shlex.split(cmd)\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, FullTrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n",
    "model_args, data_args, training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d39f3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/07/2023 18:03:14 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "08/07/2023 18:03:14 - INFO - __main__ - Training/evaluation parameters FullTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.99,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=1,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "domain_config_path=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/configs/humanmix_baseline_50kvocab.json,\n",
      "doremi_optimizer=doremiv1,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=128,\n",
      "gradient_checkpointing=True,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/jpt_gpt2-medium_humanmix_doremi/runs/Aug07_18-03-14_dcs043,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_end=0.001,\n",
      "lr_scheduler_name=None,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=1562,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/jpt_gpt2-medium_humanmix_doremi,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "reference_model_name_or_path=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/ft1/gpt2-medium_humanmix_baseline/,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard', 'wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "reweight_domains=True,\n",
      "reweight_eps=0.0001,\n",
      "reweight_eta=1.0,\n",
      "run_name=train_baseline,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=10,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=1111,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_domain_weights_tmp_file=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/train_baseline/jpt_gpt2-medium_humanmix_doremi/domain_weights,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "if training_args.should_log:\n",
    "    # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "    \n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8d2de45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "num_skip_examples = 0\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n",
    "        state = TrainerState.load_from_json(str(Path(last_checkpoint) / TRAINER_STATE_NAME))\n",
    "        global_batch_size = training_args.train_batch_size * training_args.gradient_accumulation_steps * training_args.world_size\n",
    "        num_skip_examples = state.global_step * global_batch_size\n",
    "        logger.info(f\"Skipping {num_skip_examples} examples\")\n",
    "        \n",
    "last_checkpoint, num_skip_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e28eed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0f9dc83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:710] 2023-08-07 18:03:14,932 >> loading configuration file /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/ft1/gpt2-medium_humanmix_doremi/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-07 18:03:14,934 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/ft1/gpt2-medium_humanmix_doremi\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModelDoReMi\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.32.0.dev0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "\n",
    "config_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "}\n",
    "if model_args.config_name:\n",
    "    config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
    "elif model_args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "    # wpq: `max_model_length` not set correctly if load pre-downloaded models on disk.\n",
    "    if model_args.model_type == 'gpt_neox_flash':\n",
    "        config = gpt_neox_config_to_gpt2_config(config)\n",
    "        config.use_flash_attn = True\n",
    "        config.fused_mlp = True\n",
    "        config.fused_bias_fc = True\n",
    "        config.fused_dropout_add_ln = True\n",
    "        config.pad_vocab_size_multiple = 8\n",
    "        config.activation_function = 'gelu_new'\n",
    "        config.n_inner = None\n",
    "        # disable absolute\n",
    "        config.max_position_embeddings = 0\n",
    "else:\n",
    "    if model_args.model_type == 'gpt_flash': \n",
    "        config = GPT2Config(\n",
    "                vocab_size=50257, n_positions=2048, n_embd=2048,\n",
    "                n_layer=24, n_head=16, \n",
    "                scale_attn_by_inverse_layer_idx=True, \n",
    "                rotary_emb_fraction=0.5,\n",
    "                use_flash_attn=True, fused_mlp=True,\n",
    "                fused_bias_fc=True, fused_dropout_add_ln=True, \n",
    "                pad_vocab_size_multiple=8)\n",
    "        # disable absolute\n",
    "        config.max_position_embeddings = 0\n",
    "    elif model_args.model_type == 'gpt_neox_flash':\n",
    "        # convert to GPT2 config\n",
    "        config = CONFIG_MAPPING['gpt_neox']() \n",
    "        config = gpt_neox_config_to_gpt2_config(config)\n",
    "        config.use_flash_attn = True\n",
    "        config.fused_mlp = True\n",
    "        config.fused_bias_fc = True\n",
    "        config.fused_dropout_add_ln = True\n",
    "        config.pad_vocab_size_multiple = 8\n",
    "        config.activation_function = 'gelu_new'\n",
    "        config.n_inner = None\n",
    "        # disable absolute\n",
    "        config.max_position_embeddings = 0\n",
    "    else:\n",
    "        config = CONFIG_MAPPING[model_args.model_type]()\n",
    "    # wpq: `max_model_length` not set correctly if load pre-downloaded models on disk.\n",
    "    logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "    if model_args.config_overrides is not None:\n",
    "        logger.info(f\"Overriding config: {model_args.config_overrides}\")\n",
    "        config.update_from_string(model_args.config_overrides)\n",
    "        logger.info(f\"New config: {config}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b29572f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-08-07 18:03:15,162 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-07 18:03:15,163 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-07 18:03:15,164 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-07 18:03:15,164 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-07 18:03:15,165 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-07 18:03:15,165 >> loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/ft1/gpt2-medium_humanmix_doremi', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# wpq: `max_model_length` not specified correctly when loading pre-downloaded models from disk.\n",
    "# here specify `max_model_length` so that data collator can function properly.\n",
    "if model_args.model_type == 'gpt2':\n",
    "    model_max_length = 1024\n",
    "else:\n",
    "    model_max_length = None\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"use_fast\": model_args.use_fast_tokenizer,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "    \"model_max_length\": model_max_length,\n",
    "}\n",
    "\n",
    "if model_args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
    "\n",
    "elif model_args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )\n",
    "    \n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83c98bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:2604] 2023-08-07 18:03:15,455 >> loading weights file /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/ft1/gpt2-medium_humanmix_doremi/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:603] 2023-08-07 18:03:18,138 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.32.0.dev0\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[WARNING|modeling_utils.py:3323] 2023-08-07 18:03:27,785 >> Some weights of the model checkpoint at /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/ft1/gpt2-medium_humanmix_doremi were not used when initializing GPT2LMHeadModelDoReMi: ['reference_model.transformer.h.2.attn.c_attn.bias', 'reference_model.transformer.h.2.mlp.c_fc.weight', 'reference_model.transformer.h.23.mlp.c_fc.bias', 'reference_model.transformer.h.22.mlp.c_fc.bias', 'reference_model.transformer.h.6.mlp.c_proj.bias', 'reference_model.transformer.h.19.ln_1.weight', 'reference_model.transformer.h.17.mlp.c_fc.bias', 'reference_model.transformer.h.20.ln_1.weight', 'reference_model.transformer.h.21.mlp.c_fc.weight', 'perdomain_scores', 'reference_model.transformer.h.22.ln_2.bias', 'reference_model.transformer.h.23.attn.c_attn.bias', 'reference_model.transformer.h.12.mlp.c_fc.weight', 'reference_model.transformer.h.0.attn.c_proj.weight', 'reference_model.transformer.h.18.mlp.c_fc.bias', 'reference_model.transformer.h.23.mlp.c_proj.weight', 'reference_model.transformer.h.5.ln_2.weight', 'reference_model.transformer.h.15.attn.c_attn.weight', 'reference_model.transformer.h.20.mlp.c_fc.weight', 'reference_model.transformer.h.13.mlp.c_fc.bias', 'reference_model.transformer.h.23.attn.c_attn.weight', 'reference_model.transformer.h.12.attn.c_proj.bias', 'reference_model.transformer.h.5.ln_1.weight', 'reference_model.transformer.h.18.ln_1.bias', 'reference_model.transformer.h.2.ln_2.weight', 'reference_model.transformer.h.13.attn.c_attn.bias', 'reference_model.transformer.h.16.attn.c_proj.bias', 'reference_model.transformer.h.21.mlp.c_fc.bias', 'reference_model.transformer.h.11.ln_1.bias', 'reference_model.transformer.h.13.mlp.c_proj.bias', 'reference_model.transformer.h.9.attn.c_attn.bias', 'reference_model.transformer.h.8.mlp.c_proj.weight', 'reference_model.transformer.h.4.ln_2.bias', 'reference_model.transformer.h.6.attn.c_attn.bias', 'reference_model.transformer.h.18.attn.c_attn.weight', 'reference_model.transformer.h.16.mlp.c_proj.bias', 'reference_model.transformer.h.13.ln_1.weight', 'reference_model.transformer.h.18.mlp.c_proj.bias', 'reference_model.transformer.h.0.mlp.c_proj.bias', 'reference_model.transformer.h.11.ln_2.bias', 'reference_model.transformer.h.22.mlp.c_proj.bias', 'reference_model.transformer.h.9.attn.c_proj.weight', 'reference_model.transformer.h.10.ln_1.weight', 'reference_model.transformer.h.5.mlp.c_proj.bias', 'reference_model.transformer.h.11.mlp.c_proj.bias', 'reference_model.transformer.h.11.attn.c_proj.bias', 'reference_model.transformer.h.20.attn.c_proj.bias', 'reference_model.transformer.h.3.mlp.c_proj.bias', 'reference_model.transformer.h.20.attn.c_attn.weight', 'reference_model.transformer.h.20.attn.c_attn.bias', 'reference_model.transformer.h.16.mlp.c_fc.weight', 'reference_model.transformer.h.20.mlp.c_proj.weight', 'reference_model.transformer.h.14.ln_2.bias', 'reference_model.transformer.h.15.ln_1.bias', 'reference_model.transformer.h.19.attn.c_attn.weight', 'reference_model.transformer.h.21.ln_2.bias', 'reference_model.transformer.h.1.attn.c_attn.weight', 'reference_model.transformer.h.1.mlp.c_proj.weight', 'reference_model.transformer.h.13.ln_2.weight', 'reference_model.transformer.h.23.mlp.c_proj.bias', 'reference_model.transformer.h.8.ln_2.bias', 'reference_model.transformer.h.5.attn.c_attn.weight', 'reference_model.transformer.h.2.attn.c_proj.bias', 'reference_model.transformer.h.4.mlp.c_fc.bias', 'reference_model.transformer.h.10.attn.c_proj.bias', 'reference_model.transformer.h.19.attn.c_proj.weight', 'reference_model.transformer.h.13.mlp.c_fc.weight', 'reference_model.transformer.h.0.ln_2.weight', 'reference_model.transformer.h.19.ln_1.bias', 'reference_model.transformer.h.23.ln_2.weight', 'reference_model.transformer.h.20.mlp.c_proj.bias', 'reference_model.transformer.h.19.attn.c_proj.bias', 'reference_model.transformer.h.10.ln_2.weight', 'reference_model.transformer.h.7.ln_2.bias', 'reference_model.transformer.h.1.ln_2.weight', 'reference_model.transformer.h.7.attn.c_proj.bias', 'reference_model.transformer.h.18.ln_1.weight', 'reference_model.transformer.h.18.attn.c_attn.bias', 'reference_model.transformer.h.9.attn.c_proj.bias', 'reference_model.transformer.h.2.ln_1.bias', 'reference_model.transformer.h.23.ln_2.bias', 'reference_model.transformer.h.0.mlp.c_fc.bias', 'reference_model.transformer.h.13.attn.c_proj.weight', 'reference_model.transformer.h.7.mlp.c_proj.bias', 'reference_model.transformer.h.22.attn.c_attn.weight', 'reference_model.transformer.h.11.attn.c_proj.weight', 'reference_model.transformer.h.10.mlp.c_proj.weight', 'reference_model.transformer.h.16.ln_2.weight', 'reference_model.transformer.h.3.ln_1.weight', 'reference_model.transformer.h.17.ln_2.bias', 'reference_model.transformer.h.9.ln_1.bias', 'reference_model.transformer.h.17.mlp.c_fc.weight', 'reference_model.transformer.h.12.ln_2.bias', 'reference_model.transformer.h.15.ln_2.bias', 'reference_model.transformer.h.14.attn.c_proj.bias', 'reference_model.transformer.h.14.attn.c_attn.bias', 'reference_model.transformer.h.10.mlp.c_proj.bias', 'reference_model.transformer.h.14.mlp.c_proj.bias', 'reference_model.transformer.h.1.attn.c_proj.bias', 'reference_model.transformer.h.15.attn.c_attn.bias', 'reference_model.transformer.h.21.attn.c_attn.weight', 'reference_model.transformer.h.14.ln_1.bias', 'reference_model.transformer.h.4.mlp.c_fc.weight', 'reference_model.transformer.h.18.mlp.c_fc.weight', 'reference_model.transformer.h.22.ln_1.bias', 'reference_model.transformer.h.7.attn.c_attn.weight', 'reference_model.transformer.h.5.mlp.c_fc.bias', 'reference_model.transformer.h.5.attn.c_attn.bias', 'reference_model.transformer.h.14.mlp.c_fc.weight', 'reference_model.transformer.h.11.attn.c_attn.bias', 'reference_model.transformer.h.22.attn.c_proj.weight', 'reference_model.transformer.h.1.mlp.c_fc.weight', 'reference_model.transformer.h.17.mlp.c_proj.bias', 'train_domain_weights', 'reference_model.transformer.h.6.ln_1.weight', 'reference_model.transformer.h.0.ln_2.bias', 'reference_model.transformer.h.8.ln_1.bias', 'reference_model.transformer.h.16.ln_1.bias', 'reference_model.transformer.h.11.ln_2.weight', 'reference_model.transformer.h.5.mlp.c_proj.weight', 'reference_model.transformer.h.15.attn.c_proj.weight', 'reference_model.transformer.h.13.mlp.c_proj.weight', 'reference_model.transformer.h.2.mlp.c_proj.bias', 'reference_model.transformer.h.3.mlp.c_proj.weight', 'reference_model.transformer.h.19.attn.c_attn.bias', 'reference_model.transformer.h.6.attn.c_proj.bias', 'reference_model.transformer.h.6.ln_1.bias', 'reference_model.transformer.h.17.attn.c_attn.weight', 'reference_model.transformer.h.16.ln_1.weight', 'reference_model.transformer.h.16.ln_2.bias', 'reference_model.transformer.h.11.mlp.c_proj.weight', 'reference_model.transformer.h.12.mlp.c_proj.bias', 'reference_model.transformer.h.13.attn.c_proj.bias', 'reference_model.transformer.h.2.ln_1.weight', 'reference_model.transformer.h.18.ln_2.weight', 'reference_model.transformer.h.9.ln_1.weight', 'reference_model.transformer.h.4.ln_1.bias', 'reference_model.transformer.h.21.ln_2.weight', 'reference_model.transformer.h.7.attn.c_attn.bias', 'reference_model.transformer.h.21.attn.c_attn.bias', 'reference_model.transformer.h.4.attn.c_attn.weight', 'reference_model.transformer.h.17.attn.c_proj.bias', 'reference_model.transformer.h.3.ln_2.weight', 'reference_model.transformer.h.22.attn.c_attn.bias', 'reference_model.transformer.h.12.mlp.c_fc.bias', 'reference_model.transformer.h.21.ln_1.weight', 'reference_model.transformer.h.10.ln_1.bias', 'reference_model.transformer.h.5.attn.c_proj.bias', 'reference_model.transformer.h.0.attn.c_attn.weight', 'reference_model.transformer.h.14.ln_2.weight', 'reference_model.transformer.h.18.attn.c_proj.bias', 'reference_model.transformer.h.21.mlp.c_proj.weight', 'reference_model.transformer.h.12.mlp.c_proj.weight', 'reference_model.transformer.h.4.attn.c_proj.weight', 'reference_model.transformer.h.11.attn.c_attn.weight', 'reference_model.transformer.h.0.attn.c_proj.bias', 'reference_model.transformer.h.1.mlp.c_fc.bias', 'reference_model.transformer.h.8.ln_2.weight', 'reference_model.transformer.h.12.attn.c_proj.weight', 'reference_model.transformer.h.17.ln_1.bias', 'reference_model.transformer.h.5.ln_2.bias', 'reference_model.transformer.h.9.mlp.c_fc.bias', 'reference_model.transformer.h.1.ln_1.bias', 'reference_model.transformer.h.3.attn.c_proj.bias', 'reference_model.transformer.h.17.attn.c_proj.weight', 'reference_model.transformer.h.7.ln_1.bias', 'reference_model.transformer.h.18.attn.c_proj.weight', 'reference_model.transformer.h.15.mlp.c_proj.weight', 'reference_model.transformer.h.22.mlp.c_proj.weight', 'reference_model.transformer.h.9.ln_2.bias', 'reference_model.transformer.h.4.ln_1.weight', 'reference_model.transformer.h.13.attn.c_attn.weight', 'reference_model.transformer.h.5.attn.c_proj.weight', 'reference_model.transformer.ln_f.bias', 'reference_model.transformer.h.6.ln_2.bias', 'reference_model.transformer.h.9.mlp.c_fc.weight', 'reference_model.transformer.h.17.mlp.c_proj.weight', 'reference_model.transformer.h.17.ln_2.weight', 'reference_model.transformer.h.10.attn.c_attn.bias', 'reference_model.transformer.h.12.ln_1.bias', 'reference_model.transformer.h.15.mlp.c_fc.bias', 'reference_model.transformer.h.3.ln_2.bias', 'reference_model.transformer.h.8.mlp.c_fc.weight', 'reference_model.transformer.h.17.attn.c_attn.bias', 'reference_model.transformer.h.23.ln_1.weight', 'reference_model.transformer.h.4.mlp.c_proj.bias', 'reference_model.transformer.h.13.ln_2.bias', 'reference_model.transformer.h.11.mlp.c_fc.weight', 'reference_model.transformer.h.9.mlp.c_proj.bias', 'reference_model.transformer.h.18.mlp.c_proj.weight', 'reference_model.transformer.h.4.ln_2.weight', 'reference_model.transformer.h.1.ln_1.weight', 'reference_model.transformer.wpe.weight', 'reference_model.transformer.h.2.attn.c_proj.weight', 'reference_model.transformer.h.19.ln_2.bias', 'reference_model.transformer.h.7.ln_1.weight', 'reference_model.transformer.h.1.ln_2.bias', 'reference_model.transformer.h.6.attn.c_proj.weight', 'reference_model.transformer.h.0.mlp.c_proj.weight', 'update_counter', 'reference_model.transformer.h.9.ln_2.weight', 'reference_model.transformer.h.20.mlp.c_fc.bias', 'reference_model.transformer.h.21.ln_1.bias', 'reference_model.transformer.h.2.mlp.c_proj.weight', 'reference_model.transformer.wte.weight', 'reference_model.transformer.h.2.ln_2.bias', 'reference_model.transformer.h.3.ln_1.bias', 'reference_model.transformer.h.5.ln_1.bias', 'reference_model.transformer.h.8.ln_1.weight', 'reference_model.transformer.h.10.mlp.c_fc.bias', 'reference_model.transformer.h.21.attn.c_proj.weight', 'reference_model.transformer.h.6.ln_2.weight', 'reference_model.transformer.h.20.ln_2.weight', 'reference_model.transformer.h.19.ln_2.weight', 'reference_model.transformer.h.4.attn.c_proj.bias', 'reference_model.transformer.h.23.attn.c_proj.weight', 'reference_model.transformer.h.12.ln_2.weight', 'reference_model.transformer.h.6.attn.c_attn.weight', 'reference_model.transformer.h.0.mlp.c_fc.weight', 'reference_model.transformer.ln_f.weight', 'reference_model.transformer.h.16.mlp.c_proj.weight', 'reference_model.transformer.h.7.mlp.c_proj.weight', 'reference_model.transformer.h.3.attn.c_attn.bias', 'reference_model.transformer.h.16.attn.c_proj.weight', 'reference_model.transformer.h.12.attn.c_attn.weight', 'reference_model.transformer.h.15.mlp.c_proj.bias', 'reference_model.transformer.h.12.attn.c_attn.bias', 'reference_model.transformer.h.8.attn.c_proj.bias', 'reference_model.transformer.h.14.ln_1.weight', 'reference_model.transformer.h.15.ln_1.weight', 'reference_model.transformer.h.23.attn.c_proj.bias', 'reference_model.transformer.h.4.attn.c_attn.bias', 'reference_model.transformer.h.0.ln_1.weight', 'reference_model.transformer.h.3.mlp.c_fc.weight', 'reference_model.transformer.h.8.attn.c_proj.weight', 'reference_model.transformer.h.22.mlp.c_fc.weight', 'reference_model.transformer.h.11.ln_1.weight', 'reference_model.transformer.h.16.attn.c_attn.weight', 'reference_model.transformer.h.11.mlp.c_fc.bias', 'reference_model.transformer.h.19.mlp.c_proj.bias', 'reference_model.transformer.h.19.mlp.c_fc.bias', 'reference_model.transformer.h.10.mlp.c_fc.weight', 'reference_model.transformer.h.14.mlp.c_proj.weight', 'reference_model.transformer.h.23.mlp.c_fc.weight', 'reference_model.transformer.h.22.ln_1.weight', 'reference_model.transformer.h.9.mlp.c_proj.weight', 'reference_model.transformer.h.13.ln_1.bias', 'reference_model.transformer.h.15.attn.c_proj.bias', 'reference_model.transformer.h.20.ln_1.bias', 'reference_model.transformer.h.2.mlp.c_fc.bias', 'reference_model.transformer.h.8.mlp.c_proj.bias', 'reference_model.transformer.h.1.mlp.c_proj.bias', 'reference_model.transformer.h.15.ln_2.weight', 'reference_model.transformer.h.5.mlp.c_fc.weight', 'reference_model.transformer.h.8.attn.c_attn.weight', 'reference_model.transformer.h.10.attn.c_proj.weight', 'reference_model.transformer.h.10.ln_2.bias', 'reference_model.transformer.h.3.attn.c_proj.weight', 'reference_model.transformer.h.3.mlp.c_fc.bias', 'reference_model.transformer.h.16.attn.c_attn.bias', 'reference_model.transformer.h.19.mlp.c_fc.weight', 'reference_model.transformer.h.21.attn.c_proj.bias', 'reference_model.transformer.h.3.attn.c_attn.weight', 'reference_model.transformer.h.17.ln_1.weight', 'reference_model.transformer.h.18.ln_2.bias', 'reference_model.transformer.h.2.attn.c_attn.weight', 'reference_model.transformer.h.0.attn.c_attn.bias', 'reference_model.transformer.h.6.mlp.c_proj.weight', 'reference_model.transformer.h.21.mlp.c_proj.bias', 'avg_domain_weights', 'reference_model.transformer.h.0.ln_1.bias', 'reference_model.transformer.h.23.ln_1.bias', 'reference_model.transformer.h.9.attn.c_attn.weight', 'reference_model.transformer.h.6.mlp.c_fc.bias', 'reference_model.transformer.h.12.ln_1.weight', 'reference_model.transformer.h.7.ln_2.weight', 'reference_model.transformer.h.1.attn.c_proj.weight', 'reference_model.transformer.h.20.ln_2.bias', 'reference_model.lm_head.weight', 'reference_model.transformer.h.4.mlp.c_proj.weight', 'reference_model.transformer.h.22.ln_2.weight', 'reference_model.transformer.h.8.mlp.c_fc.bias', 'reference_model.transformer.h.8.attn.c_attn.bias', 'reference_model.transformer.h.7.mlp.c_fc.weight', 'reference_model.transformer.h.20.attn.c_proj.weight', 'reference_model.transformer.h.19.mlp.c_proj.weight', 'reference_model.transformer.h.1.attn.c_attn.bias', 'reference_model.transformer.h.7.attn.c_proj.weight', 'reference_model.transformer.h.6.mlp.c_fc.weight', 'reference_model.transformer.h.10.attn.c_attn.weight', 'reference_model.transformer.h.22.attn.c_proj.bias', 'reference_model.transformer.h.14.mlp.c_fc.bias', 'reference_model.transformer.h.16.mlp.c_fc.bias', 'reference_model.transformer.h.14.attn.c_attn.weight', 'reference_model.transformer.h.14.attn.c_proj.weight', 'reference_model.transformer.h.15.mlp.c_fc.weight', 'reference_model.transformer.h.7.mlp.c_fc.bias']\n",
      "- This IS expected if you are initializing GPT2LMHeadModelDoReMi from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModelDoReMi from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3341] 2023-08-07 18:03:27,786 >> All the weights of GPT2LMHeadModelDoReMi were initialized from the model checkpoint at /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/ft1/gpt2-medium_humanmix_doremi.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModelDoReMi for predictions without further training.\n",
      "[INFO|configuration_utils.py:563] 2023-08-07 18:03:27,794 >> loading configuration file /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/ft1/gpt2-medium_humanmix_doremi/generation_config.json\n",
      "[INFO|configuration_utils.py:603] 2023-08-07 18:03:27,795 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.32.0.dev0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModelDoReMi(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       "  (loss_fct): CrossEntropyLoss()\n",
       "  (pertoken_loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if model_args.model_name_or_path:\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "    if model_args.model_type in {'gpt_flash', 'gpt_neox_flash'}:\n",
    "        model = doremi_models.GPTFlashAttnLMHeadModel.from_pretrained(\n",
    "            model_args.model_name_or_path, config=config)\n",
    "    elif model_args.model_type in ['gpt2']:\n",
    "        model = doremi_models.GPT2LMHeadModelDoReMi.from_pretrained(\n",
    "            model_args.model_name_or_path, \n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "            torch_dtype=torch_dtype,\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "            torch_dtype=torch_dtype,\n",
    "        )\n",
    "else:\n",
    "    if model_args.model_type in {'gpt_flash', 'gpt_neox_flash'}:\n",
    "        model = doremi_models.GPTFlashAttnLMHeadModel(config)\n",
    "    elif model_args.model_type in {'gpt2'}:\n",
    "        model = doremi_models.GPT2LMHeadModelDoReMi(config)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "    n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())\n",
    "    logger.info(f\"Training new model from scratch - Total size={n_params/2**20:.2f}M params\")\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6364e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Tuple, Union\n",
    "# from transformers import GPT2LMHeadModel\n",
    "# # from doremi.models import CausalLMOutputWithDomainIDs\n",
    "# from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "# from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# @dataclass\n",
    "# class CausalLMOutputWithDomainIDs(CausalLMOutputWithCrossAttentions):\n",
    "#     domain_ids: Optional[torch.LongTensor] = None\n",
    "#     reference_pertoken_loss: Optional[torch.FloatTensor] = None  # corresponds to uniq_domain_ids\n",
    "#     pertoken_loss: Optional[torch.FloatTensor] = None  # corresponds to uniq_domain_ids\n",
    "#     token_mask: Optional[torch.BoolTensor] = None  # 1 for tokens that are not padding\n",
    "\n",
    "\n",
    "\n",
    "# #             model_args.model_name_or_path,\n",
    "# #             from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "# #             config=config,\n",
    "# #             cache_dir=model_args.cache_dir,\n",
    "# #             revision=model_args.model_revision,\n",
    "# #             use_auth_token=True if model_args.use_auth_token else None,\n",
    "# #             torch_dtype=torch_dtype,\n",
    "\n",
    "# # model = doremi_models.GPTFlashAttnLMHeadModel.from_pretrained(\n",
    "# #     model_args.model_name_or_path, config=config)\n",
    "\n",
    "        \n",
    "# model = GPTLMHeadModelDoReMi.from_pretrained(\n",
    "#     model_args.model_name_or_path, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0fe33ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collate_fn = data_utils.get_data_collator(tokenizer, do_padding=data_args.do_padding)\n",
    "\n",
    "# batch = list(train_dataset.take(3))\n",
    "# batch = collate_fn(batch)\n",
    "# batch = {k: v.to('cpu') for k,v in batch.items()}\n",
    "# print(batch['input_ids'].shape)\n",
    "# print(model.device, batch['input_ids'].device)\n",
    "\n",
    "# out = model(**batch, return_pertoken_losses=True, )\n",
    "# print(out.pertoken_loss.shape, out.token_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bf74f1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cot', 'dolly', 'flan_v2', 'oasst1'] 4 {'cot': 0.25, 'flan_v2': 0.25, 'dolly': 0.25, 'oasst1': 0.25}\n",
      "08/07/2023 18:11:55 - WARNING - doremi.dataloader - No split used or split directory not found: using same data for all splits.\n",
      "08/07/2023 18:11:55 - INFO - datasets.builder - Using custom data configuration default-20e8d8f5df875937\n",
      "08/07/2023 18:11:55 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/07/2023 18:11:55 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/07/2023 18:11:55 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/07/2023 18:11:55 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/07/2023 18:11:55 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa2f08c90694e80afd0033ca139c993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-56d3f4f8e89f90dd_00000_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-56d3f4f8e89f90dd_00001_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-56d3f4f8e89f90dd_00002_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-56d3f4f8e89f90dd_00003_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-56d3f4f8e89f90dd_00004_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-56d3f4f8e89f90dd_00005_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-56d3f4f8e89f90dd_00006_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-56d3f4f8e89f90dd_00007_of_00008.arrow\n",
      "08/07/2023 18:11:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-56d3f4f8e89f90dd_*_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "08/07/2023 18:11:56 - INFO - datasets.builder - Using custom data configuration default-052d88b51b5a3b64\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/07/2023 18:11:56 - INFO - datasets.builder - Using custom data configuration default-68fd04897f8e942c\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/07/2023 18:11:56 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/07/2023 18:11:56 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f40d02fe94479eae38697a34f5cb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5c1692dc63a46c64_00000_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5c1692dc63a46c64_00001_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5c1692dc63a46c64_00002_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5c1692dc63a46c64_00003_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5c1692dc63a46c64_00004_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5c1692dc63a46c64_00005_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5c1692dc63a46c64_00006_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5c1692dc63a46c64_00007_of_00008.arrow\n",
      "08/07/2023 18:11:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5c1692dc63a46c64_*_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "08/07/2023 18:11:56 - INFO - datasets.builder - Using custom data configuration default-c1ebb40c21c57b04\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/07/2023 18:11:56 - INFO - datasets.builder - Using custom data configuration default-a01381664fd2589b\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/07/2023 18:11:56 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/07/2023 18:11:56 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8de3c433ab2431e8643d2b33150e66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1d4629b8fa2d3eee_00000_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1d4629b8fa2d3eee_00001_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1d4629b8fa2d3eee_00002_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1d4629b8fa2d3eee_00003_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1d4629b8fa2d3eee_00004_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1d4629b8fa2d3eee_00005_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1d4629b8fa2d3eee_00006_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1d4629b8fa2d3eee_00007_of_00008.arrow\n",
      "08/07/2023 18:11:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1d4629b8fa2d3eee_*_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "08/07/2023 18:11:56 - INFO - datasets.builder - Using custom data configuration default-44c43d2703c5874c\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/07/2023 18:11:56 - INFO - datasets.builder - Using custom data configuration default-d43591dedd2996a1\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/07/2023 18:11:56 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/07/2023 18:11:56 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1320e9161541259286afaf9ab8202e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1bb49efbfa7e333e_00000_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1bb49efbfa7e333e_00001_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1bb49efbfa7e333e_00002_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1bb49efbfa7e333e_00003_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1bb49efbfa7e333e_00004_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1bb49efbfa7e333e_00005_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1bb49efbfa7e333e_00006_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1bb49efbfa7e333e_00007_of_00008.arrow\n",
      "08/07/2023 18:11:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1bb49efbfa7e333e_*_of_00008.arrow\n",
      "08/07/2023 18:11:56 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "08/07/2023 18:11:56 - INFO - datasets.builder - Using custom data configuration default-2fecb34c98b79bd2\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/07/2023 18:11:56 - INFO - datasets.builder - Using custom data configuration default-cbed268508785a66\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/07/2023 18:11:56 - INFO - datasets.builder - Using custom data configuration default-6434dc48cedee9ff\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/07/2023 18:11:56 - INFO - datasets.builder - Using custom data configuration default-eefaeff9649d262f\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/07/2023 18:11:56 - INFO - datasets.builder - Using custom data configuration default-6b2c4be1458be189\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/07/2023 18:11:56 - INFO - datasets.builder - Using custom data configuration default-434af19337e2989c\n",
      "08/07/2023 18:11:56 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(training_args.domain_config_path, 'r') as f:\n",
    "    domain_config = json.load(f)\n",
    "\n",
    "train_domain_weights_dict = domain_config['train_domain_weights']\n",
    "eval_domain_weights_dict = domain_config['eval_domain_weights']\n",
    "# whenever we convert dict to array, we sort by key\n",
    "domain_list = list(sorted(train_domain_weights_dict.keys()))\n",
    "num_domains = len(domain_list)\n",
    "\n",
    "print(domain_list, num_domains, train_domain_weights_dict)\n",
    "\n",
    "if training_args.do_train:\n",
    "    # data script could change tokenizer shape\n",
    "    train_dataset = data_utils.get_preprocessed_mixed_dataset(\n",
    "            preprocessed_dir=data_args.dataset_dir,\n",
    "            domain_weights_dict=train_domain_weights_dict,\n",
    "            dataset_name=data_args.dataset_name,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            split='train',\n",
    "            max_samples=data_args.max_train_samples,\n",
    "            add_domain_id=data_args.add_domain_id,\n",
    "            tmp_file=None,\n",
    "            seed=training_args.seed,\n",
    "            tokenizer=tokenizer,\n",
    "            shuffle=data_args.shuffle,\n",
    "            num_skip_examples=num_skip_examples,\n",
    "            shard_reversal=training_args.reweight_domains,\n",
    "            training_args=training_args,\n",
    "    )\n",
    "\n",
    "if training_args.do_eval:\n",
    "    eval_dataset = data_utils.get_preprocessed_mixed_dataset(\n",
    "            preprocessed_dir=data_args.dataset_dir,\n",
    "            domain_weights_dict=eval_domain_weights_dict,\n",
    "            dataset_name=data_args.dataset_name,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            split='validation',\n",
    "            add_domain_id=data_args.add_domain_id,\n",
    "            max_samples=data_args.max_eval_samples,\n",
    "            tokenizer=tokenizer,\n",
    "            no_interleave=True,\n",
    "            training_args=training_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4e911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# preprocessed_dir=data_args.dataset_dir\n",
    "# domain_weights_dict=train_domain_weights_dict\n",
    "# dataset_name=data_args.dataset_name\n",
    "# cache_dir=model_args.cache_dir\n",
    "# split='train'\n",
    "# max_samples=data_args.max_train_samples\n",
    "# add_domain_id=data_args.add_domain_id\n",
    "# tmp_file=None\n",
    "# seed=training_args.seed\n",
    "# tokenizer=tokenizer\n",
    "# shuffle=data_args.shuffle\n",
    "# num_skip_examples=num_skip_examples\n",
    "# shard_reversal=training_args.reweight_domains\n",
    "# no_interleave=False\n",
    "\n",
    "# print(preprocessed_dir)\n",
    "# print(domain_weights_dict)\n",
    "# print(dataset_name)\n",
    "# print(cache_dir)\n",
    "# print(split)\n",
    "# print(max_samples)\n",
    "# print(add_domain_id)\n",
    "# print(seed)\n",
    "# print(shuffle)\n",
    "# print(num_skip_examples)\n",
    "# print(shard_reversal)\n",
    "\n",
    "\n",
    "# domain_names = list(sorted(domain_weights_dict.keys()))\n",
    "# domain_to_idx = {domain_names[i]: i for i in range(len(domain_names))}\n",
    "# domain_weights = np.asarray([domain_weights_dict[domain_name] for domain_name in domain_names])\n",
    "# domain_weights = domain_weights / domain_weights.sum()\n",
    "\n",
    "# print()\n",
    "# print(json.dumps({'domain_names': domain_names, \n",
    "#                   'domain_to_idx': domain_to_idx, \n",
    "#                   'domain_weights': list(domain_weights)},\n",
    "#                 indent=4))\n",
    "\n",
    "\n",
    "# # write domain weights to file if tmp_file is set\n",
    "# if tmp_file is not None:\n",
    "#     probabilities_tmp_file = tmp_file\n",
    "\n",
    "#     with open(str(probabilities_tmp_file), 'wb') as f:\n",
    "#         pickle.dump(domain_weights, f)\n",
    "#     probabilities = None\n",
    "# else:\n",
    "#     probabilities = domain_weights\n",
    "#     probabilities_tmp_file = None\n",
    "\n",
    "\n",
    "# print()\n",
    "# print(json.dumps({'probabilities': list(probabilities)}, indent=4))\n",
    "\n",
    "# # from doremi.dataloader import get_perdomain_datasets\n",
    "# # all_ds = get_perdomain_datasets(\n",
    "# #     preprocessed_dir, \n",
    "# #     domain_weights_dict,\n",
    "# #     cache_dir=cache_dir,\n",
    "# #     split=split,\n",
    "# #     seed=seed,\n",
    "# #     domain_weights=domain_weights,\n",
    "# #     domain_names=domain_names,\n",
    "# #     num_skip_examples=num_skip_examples,\n",
    "# #     shuffle=shuffle,\n",
    "# #     shard_reversal=shard_reversal\n",
    "# # )\n",
    "\n",
    "# domain_name_to_skip_num = determine_skip_per_domain(num_skip_examples, seed, domain_weights, domain_names)\n",
    "\n",
    "# preprocessed_dir = Path(preprocessed_dir)\n",
    "# if split is not None and (preprocessed_dir / split).exists():\n",
    "#     preprocessed_dir = preprocessed_dir / split\n",
    "# else:\n",
    "#     logger.warn(f\"No split used or split directory not found: using same data for all splits.\")\n",
    "\n",
    "# domains = list(sorted(domain_weights_dict.keys()))\n",
    "\n",
    "# print(preprocessed_dir)\n",
    "# print(domain_name_to_skip_num)\n",
    "# print()\n",
    "# print(json.dumps({'preprocessed_dir': str(preprocessed_dir), \n",
    "#                   'domain_name_to_skip_num': domain_name_to_skip_num}, indent=4))\n",
    "\n",
    "\n",
    "# all_ds = {}\n",
    "# for domain in domains:\n",
    "#     domain_dir = preprocessed_dir / domain\n",
    "    \n",
    "#     ## wpq: read instruction tuning dataset off `jsonl` files\n",
    "#     if (domain_dir / f'{domain}_data.jsonl').exists():\n",
    "#         from datasets import load_dataset\n",
    "#         from functools import partial\n",
    "#         from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n",
    "#         from doremi.dataloader import skippable_data_gen_dataset\n",
    "\n",
    "#         data_files = {'train': str(domain_dir / f'{domain}_data.jsonl')}\n",
    "#         raw_datasets = load_dataset(\n",
    "#             \"json\",\n",
    "#             data_files=data_files,\n",
    "#             cache_dir=cache_dir,\n",
    "#             use_auth_token=True if model_args.use_auth_token else None,\n",
    "#         )\n",
    "#         # Preprocessing the datasets.\n",
    "#         if \"prompt\" in raw_datasets[\"train\"].column_names and \"completion\" in raw_datasets[\"train\"].column_names:\n",
    "#             encode_function = partial(\n",
    "#                 encode_with_prompt_completion_format,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 max_seq_length=1024,\n",
    "#             )\n",
    "#         elif \"messages\" in raw_datasets[\"train\"].column_names:\n",
    "#             encode_function = partial(\n",
    "#                 encode_with_messages_format,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 max_seq_length=1024,\n",
    "#             )\n",
    "#         else:\n",
    "#             raise ValueError(\"You need to have either 'prompt'&'completion' or 'messages' in your column names.\")\n",
    "\n",
    "#         with training_args.main_process_first(local=False, desc=\"Processing instruction data\"):\n",
    "#             lm_datasets = raw_datasets.map(\n",
    "#                 encode_function,\n",
    "#                 num_proc=16,\n",
    "#                 batched=False,\n",
    "#             )\n",
    "#             lm_datasets.set_format(type=\"pt\")\n",
    "#         ds = lm_datasets['train']\n",
    "#         ds = IterableDataset.from_generator(\n",
    "#                 skippable_data_gen_dataset,\n",
    "#                 gen_kwargs={'ds': ds,\n",
    "#                             'num_skip_examples': domain_name_to_skip_num[domain],\n",
    "#                             'loop': (split == 'train'),\n",
    "#                             'seed': seed,\n",
    "#                             'shuffle': shuffle}\n",
    "#                 )\n",
    "#         seed += 1\n",
    "#     elif (domain_dir / 'dataset_info.json').exists():\n",
    "#         ds = load_from_disk(dataset_path=str(domain_dir))\n",
    "#         logger.info(f\"Loaded {domain_dir}. Length: {len(ds)}\")\n",
    "#     else:\n",
    "#         curr_shards = list(domain_dir.iterdir())\n",
    "#         if shard_reversal:\n",
    "#             curr_shards = list(reversed(curr_shards))\n",
    "#         # shuffle shard order\n",
    "#         random.Random(seed).shuffle(curr_shards)\n",
    "#         ds = IterableDataset.from_generator(\n",
    "#                 skippable_data_gen,\n",
    "#                 gen_kwargs={'shards': curr_shards,\n",
    "#                             'num_skip_examples': domain_name_to_skip_num[domain],\n",
    "#                             'loop': (split == 'train'),\n",
    "#                             'seed': seed,\n",
    "#                             'shuffle': shuffle}\n",
    "#                 )\n",
    "#         seed += 1\n",
    "#     all_ds[domain] = ds\n",
    "    \n",
    "\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# def add_domain_id_generator(ds, domain_idx):\n",
    "#     for ex in ds:\n",
    "#         ex['domain_id'] = domain_idx\n",
    "#         yield ex\n",
    "        \n",
    "# domain_ds_ls = []\n",
    "# for domain_name in domain_names:\n",
    "#     domain_idx = domain_to_idx[domain_name]\n",
    "#     domain_ds = all_ds[domain_name]\n",
    "#     # add domain_id if necessary\n",
    "#     if add_domain_id:\n",
    "#         domain_ds = IterableDataset.from_generator(\n",
    "#             add_domain_id_generator, \n",
    "#             gen_kwargs={'ds': domain_ds, 'domain_idx': domain_idx})\n",
    "#     domain_ds_ls.append(domain_ds)\n",
    "\n",
    "# if no_interleave:\n",
    "#     # instead of interleaving, run through each dataset\n",
    "#     def data_generator(shards):\n",
    "#         for shard in shards:\n",
    "#             for ex in shard:\n",
    "#                 yield ex\n",
    "#     ds = IterableDataset.from_generator(data_generator, gen_kwargs={'shards': domain_ds_ls})\n",
    "#     logger.info(\"Not interleaving dataset - will not sample according to domain weights\")\n",
    "\n",
    "# else:\n",
    "#     ds = interleave_datasets(\n",
    "#             domain_ds_ls,\n",
    "#             probabilities=probabilities,\n",
    "#             probabilities_file=probabilities_tmp_file,\n",
    "#             seed=seed)\n",
    "    \n",
    "\n",
    "# def take_data_generator(ds, max_samples):\n",
    "#     idx = 0\n",
    "#     for ex in ds:\n",
    "#         yield ex\n",
    "#         idx += 1\n",
    "#         if max_samples is not None and idx >= max_samples:\n",
    "#             return\n",
    "\n",
    "# ds = IterableDataset.from_generator(take_data_generator, gen_kwargs={'ds': ds, 'max_samples': max_samples})\n",
    "# train_dataset = ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5426031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ds = load_dataset(\n",
    "#     \"json\",\n",
    "#     data_files='test.jsonl',\n",
    "#     cache_dir=model_args.cache_dir)['train']\n",
    "# test_ds[0]\n",
    "\n",
    "# for x in test_ds.to_iterable_dataset():\n",
    "#     print(x)\n",
    "# for i, v in enumerate(ds):\n",
    "#     if i == 10:\n",
    "#         break\n",
    "#     print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dba9182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:2604] 2023-08-07 18:12:03,229 >> loading weights file /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/ft1/gpt2-medium_humanmix_baseline/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:603] 2023-08-07 18:12:03,744 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.32.0.dev0\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3333] 2023-08-07 18:12:13,326 >> All model checkpoint weights were used when initializing GPT2LMHeadModelDoReMi.\n",
      "\n",
      "[INFO|modeling_utils.py:3341] 2023-08-07 18:12:13,327 >> All the weights of GPT2LMHeadModelDoReMi were initialized from the model checkpoint at /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/ft1/gpt2-medium_humanmix_baseline/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModelDoReMi for predictions without further training.\n",
      "[INFO|configuration_utils.py:563] 2023-08-07 18:12:13,335 >> loading configuration file /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/ft1/gpt2-medium_humanmix_baseline/generation_config.json\n",
      "[INFO|configuration_utils.py:603] 2023-08-07 18:12:13,336 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.32.0.dev0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if training_args.reweight_domains:\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "    if model_args.model_type in {'gpt_flash', 'gpt_neox_flash'}:\n",
    "        model_cls = doremi_models.GPTFlashAttnLMHeadModel\n",
    "        reference_model = model_cls.from_pretrained(\n",
    "            training_args.reference_model_name_or_path,\n",
    "            config=config)\n",
    "    elif model_args.model_type in {'gpt2'}:\n",
    "        model_cls = doremi_models.GPT2LMHeadModelDoReMi\n",
    "        reference_model = model_cls.from_pretrained(\n",
    "            training_args.reference_model_name_or_path,\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "            torch_dtype=torch_dtype,\n",
    "        )\n",
    "    else:\n",
    "        model_cls = AutoModelForCausalLM\n",
    "\n",
    "        reference_model = model_cls.from_pretrained(\n",
    "            training_args.reference_model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "            torch_dtype=torch_dtype,\n",
    "        )\n",
    "    for param in reference_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    reference_model.eval()\n",
    "    model.reference_model = reference_model\n",
    "    # initialize α0 as α_ref\n",
    "    model.register_buffer('train_domain_weights', torch.tensor(\n",
    "            [train_domain_weights_dict[domain] for domain in domain_list]))\n",
    "    model.register_buffer('avg_domain_weights', model.train_domain_weights.clone())\n",
    "    model.register_buffer('perdomain_scores', torch.ones(len(train_domain_weights_dict)) * np.log(len(tokenizer)))\n",
    "    model.register_buffer('update_counter', torch.tensor(1))\n",
    "\n",
    "else:\n",
    "    reference_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66735565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.register_buffer('train_domain_weights', torch.tensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3789d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:565] 2023-08-07 18:12:15,074 >> max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 None auto False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<doremi.trainer.DoReMiTrainer at 0x7ff6b1166740>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# turn off find unused parameters\n",
    "training_args.ddp_find_unused_parameters = False\n",
    "\n",
    "# We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "# on a small vocab and want a smaller embedding size, remove this test.\n",
    "# embedding_size = model.get_input_embeddings.weight.shape[0]\n",
    "# if len(tokenizer) > embedding_size:\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = DoReMiTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_utils.get_data_collator(tokenizer, do_padding=data_args.do_padding),\n",
    ")\n",
    "\n",
    "print(trainer.args.max_grad_norm, \\\n",
    "    trainer.sharded_ddp, \\\n",
    "    trainer.args.half_precision_backend, \\\n",
    "    trainer.do_grad_scaling)\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "872a02f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "False\n",
      "[]\n",
      "[]\n",
      "False\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(trainer.args.n_gpu)\n",
    "print(trainer.args.local_rank)\n",
    "print(trainer.is_model_parallel)\n",
    "print(trainer.args.sharded_ddp)\n",
    "print(trainer.args.fsdp)\n",
    "print(trainer.is_fsdp_enabled)\n",
    "print(trainer.args.distributed_state)\n",
    "print(trainer.args.world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a27bcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "\n",
    "    if training_args.reweight_domains:\n",
    "        avg_domain_weights_dict = {}\n",
    "        for i in range(len(model.avg_domain_weights)):\n",
    "            domain_name = domain_list[i]\n",
    "            metrics[f'avg_domain_weight:{domain_name}'] = model.avg_domain_weights[i].item()\n",
    "            avg_domain_weights_dict[domain_name] = model.avg_domain_weights[i].item()\n",
    "\n",
    "        # save avg domain weights to json\n",
    "        avg_domain_weights_file = Path(training_args.output_dir) / 'avg_domain_weights.json'\n",
    "        with open(avg_domain_weights_file, 'w') as f:\n",
    "            json.dump(avg_domain_weights_dict, f, indent=2)\n",
    "\n",
    "        # also save to configs dir\n",
    "        config_dict = {\"train_domain_weights\": avg_domain_weights_dict,\n",
    "                       \"eval_domain_weights\": avg_domain_weights_dict}\n",
    "        config_dict_file = Path(__file__).parent.parent / 'configs' / f\"{Path(training_args.output_dir).name}.json\"\n",
    "        with open(config_dict_file, 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "584e6abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/07/2023 16:23:59 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-993147edcef24e24.arrow\n",
      "08/07/2023 16:23:59 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ab041099bd1d2f87.arrow\n"
     ]
    }
   ],
   "source": [
    "data_collator=data_utils.get_data_collator(tokenizer, do_padding=data_args.do_padding)\n",
    "self = trainer\n",
    "\n",
    "inputs = list(train_dataset.take(3))\n",
    "inputs = data_collator(inputs)\n",
    "\n",
    "model.train()\n",
    "inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "with self.compute_loss_context_manager():\n",
    "    loss, pertoken_loss, reference_pertoken_loss, token_mask = self.compute_loss(model, inputs, return_pertoken_losses=True)\n",
    "    excess_loss = pertoken_loss - reference_pertoken_loss\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d2ba699",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = trainer\n",
    "\n",
    "pertoken_scores = excess_loss\n",
    "token_masks = token_mask.bool()\n",
    "domain_ids = inputs['domain_ids']\n",
    "\n",
    "# self.update_domain_weights(pertoken_scores, token_masks, domain_ids)\n",
    "\n",
    "scores = pertoken_scores\n",
    "scores_mask = token_masks\n",
    "\n",
    "\n",
    "train_domain_weights = self.read_weights()\n",
    "\n",
    "scores = scores.detach()\n",
    "domain_ids = domain_ids.detach()\n",
    "\n",
    "\n",
    "perdomain_scores = []\n",
    "for domain_id in range(len(train_domain_weights)):\n",
    "    domain_mask = (domain_ids == domain_id)\n",
    "    perdomain_scores_mask = scores_mask[domain_mask]\n",
    "    if domain_mask.sum() > 0:\n",
    "        curr_domain_scores = torch.clip(scores[domain_mask][perdomain_scores_mask], min=0).mean()\n",
    "    else:\n",
    "        curr_domain_scores = self.model.perdomain_scores[domain_id]\n",
    "    perdomain_scores.append(curr_domain_scores)\n",
    "\n",
    "# `perdomain_scores` with size (#domains,) is the `λₜ` in the paper, \n",
    "#     e.g., the gradient of loss w.r.t. αₜ dL/dαₜ\n",
    "self.model.perdomain_scores[:] = torch.tensor(perdomain_scores)\n",
    "\n",
    "# `train_domain_weights` is αₜ; `log_new_train_domain_weights` is log(a_t')\n",
    "# following implements: α_t' = α_{t-1} exp(η·λ_t), e.g., log(a_t') = log(α_{t-1}) + η·λ_t\n",
    "log_new_train_domain_weights = torch.log(train_domain_weights+1e-30) + self.args.reweight_eta * self.model.perdomain_scores\n",
    "\n",
    "# following implements: \n",
    "# softmax(log(α_t')) = (exp(log(a_t'[1])) / Σᵢ exp(log(a_t'[i])), ...)\n",
    "#                    = (a_t'[1]/Σᵢ a_t'[i], ...)\n",
    "new_train_domain_weights = nn.functional.softmax(log_new_train_domain_weights, dim=-1)\n",
    "\n",
    "# following implements:\n",
    "# αₜ = (1-c)·softmax(log(α_t')) + c·u where c=1e-4 by default, and u=1/#domains.\n",
    "train_domain_weights = (1-self.args.reweight_eps) * new_train_domain_weights + self.args.reweight_eps / len(new_train_domain_weights)\n",
    "\n",
    "# wpq: clip the weights to [ϵ, 1-ϵ] just to make sure.\n",
    "eps = 1e-8\n",
    "train_domain_weights = torch.clip(train_domain_weights, min=eps, max=1-eps)\n",
    "\n",
    "self.write_weights(train_domain_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "794732f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1023]) torch.Size([3, 1023])\n"
     ]
    }
   ],
   "source": [
    "print(pertoken_loss.shape, token_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7dccfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# compute the rescaled loss, divide by domain weights\n",
    "train_domain_weights = self.read_weights().to(pertoken_loss.device)\n",
    "# if doing non-uniform sampling, normalize by inverse sampling weight\n",
    "train_domain_weights = train_domain_weights / self.sampling_weights.to(train_domain_weights.device)\n",
    "train_domain_weights = train_domain_weights / train_domain_weights.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ed4216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05,\n",
      "        1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05,\n",
      "        1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05,\n",
      "        1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05,\n",
      "        1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05,\n",
      "        1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05,\n",
      "        1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2719e-05, 1.2907e-05,\n",
      "        1.2907e-05, 1.2907e-05, 1.2907e-05, 1.2907e-05, 1.2907e-05, 1.2907e-05,\n",
      "        1.2907e-05, 1.2907e-05, 1.2907e-05, 1.2907e-05, 1.2907e-05, 1.2907e-05,\n",
      "        1.2907e-05, 1.2907e-05, 1.2907e-05, 1.2907e-05, 1.2907e-05, 1.2907e-05,\n",
      "        1.2907e-05, 1.2907e-05, 1.2907e-05, 1.2907e-05, 1.2907e-05, 1.2907e-05,\n",
      "        1.2907e-05, 1.2907e-05, 1.2907e-05, 1.2907e-05, 1.2907e-05, 1.2907e-05,\n",
      "        1.2907e-05, 1.2907e-05, 1.2907e-05], device='cuda:0') tensor(0.0010, device='cuda:0')\n",
      "tensor([0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
      "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
      "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
      "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
      "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0134, 0.0134, 0.0134, 0.0134,\n",
      "        0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134,\n",
      "        0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134,\n",
      "        0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134,\n",
      "        0.0134, 0.0134, 0.0134], device='cuda:0')\n",
      "tensor(1.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## wpq: the implementation not the same as specified in the paper.\n",
    "# specifically, the normalizer is wrong, should be #tokens in a domain!\n",
    "#\n",
    "# assume bsz=3, token_mask = torch.ones(bsz, 1023)\n",
    "# domain_ids = [2,2,0]\n",
    "# curr_domain_weights = [αₜ[2], αₜ[2], αₜ[0]] where αₜ normalized to 1.\n",
    "# curr_domain_weights = [[αₜ[2]]*1023, [αₜ[2]]*1023, [αₜ[0]]*1023] \\in (3, 1023)\n",
    "# normalizer = αₜ[2]*1023 + αₜ[2]*1023 + αₜ[0]*1023\n",
    "# curr_domain_weights = [[αₜ[2] / ( αₜ[2]*1023 + αₜ[2]*1023 + αₜ[0]*1023 )]*1023, ...]\n",
    "#\n",
    "\n",
    "\n",
    "# compute the rescaled loss, divide by domain weights\n",
    "train_domain_weights = self.read_weights().to(pertoken_loss.device)\n",
    "# if doing non-uniform sampling, normalize by inverse sampling weight\n",
    "train_domain_weights = train_domain_weights / self.sampling_weights.to(train_domain_weights.device)\n",
    "train_domain_weights = train_domain_weights / train_domain_weights.sum()\n",
    "\n",
    "curr_domain_weights = train_domain_weights[inputs['domain_ids']]\n",
    "# (bsz,) -> (bsz, seq_len-1)\n",
    "curr_domain_weights = curr_domain_weights.unsqueeze(-1).expand_as(pertoken_loss).detach()\n",
    "curr_domain_weights = curr_domain_weights * token_mask\n",
    "\n",
    "normalizer = curr_domain_weights.sum()\n",
    "\n",
    "\n",
    "curr_domain_weights = curr_domain_weights / normalizer\n",
    "\n",
    "print(curr_domain_weights.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d7cd299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0010, device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# token_mask = token_mask.detach().type(pertoken_loss.dtype)\n",
    "# curr_domain_weights = curr_domain_weights / normalizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756aee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute the rescaled loss, divide by domain weights\n",
    "# train_domain_weights = self.read_weights().to(pertoken_loss.device)\n",
    "# if doing non-uniform sampling, normalize by inverse sampling weight\n",
    "train_domain_weights = train_domain_weights / self.sampling_weights.to(train_domain_weights.device)\n",
    "train_domain_weights = train_domain_weights / train_domain_weights.sum()\n",
    "train_domain_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
