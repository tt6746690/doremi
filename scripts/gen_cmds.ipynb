{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ff5ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('ppc64le', 'dcs')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "import re\n",
    "from llm.submit import (\n",
    "    multiline_to_singleline,\n",
    "    submit_job_ccc,\n",
    "    submit_job_aimos,\n",
    "    submit_job,\n",
    "    get_run_statistics)\n",
    "import pandas as pd\n",
    "import json\n",
    "import platform\n",
    "import tempfile\n",
    "import subprocess\n",
    "import shlex\n",
    "import datetime\n",
    "import itertools\n",
    "import socket\n",
    "\n",
    "arch = platform.uname().processor\n",
    "hostname = socket.gethostname()\n",
    "cluster = 'ccc' if hostname.startswith('ccc') else ('dcs' if hostname.startswith('dcs') else 'npl')\n",
    "arch, cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db349029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "echo \"Running on $SLURM_JOB_NODELIST\"\n",
      "echo \"======\"\n",
      "\n",
      "master_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n",
      "master_port=10002\n",
      "RDZV_ENDPOINT=$master_addr:$master_port\n",
      "\n",
      "source ~/.profile\n",
      "conda activate open-instruct\n",
      "cd /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/\n",
      "\n",
      "set -e\n",
      "set -x\n",
      "echo \"======\"\n",
      "srun {cmd}\n",
      "\n",
      "[ ! -f \"{log_dir}/$SLURM_JOB_ID*.out\" ] && mv {log_dir}/$SLURM_JOB_ID*.out {save_dir}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shell_scripts_template_slurm = \"\"\"\n",
    "echo \"Running on $SLURM_JOB_NODELIST\"\n",
    "echo \"======\"\n",
    "\n",
    "master_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n",
    "master_port=10002\n",
    "RDZV_ENDPOINT=$master_addr:$master_port\n",
    "\n",
    "source ~/.profile\n",
    "conda activate open-instruct\n",
    "cd /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/\n",
    "\n",
    "set -e\n",
    "set -x\n",
    "echo \"======\"\n",
    "srun {cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/$SLURM_JOB_ID*.out\" ] && mv {log_dir}/$SLURM_JOB_ID*.out {save_dir}\n",
    "\"\"\"\n",
    "\n",
    "shell_scripts_template_lsf = \"\"\"\n",
    "echo \"Running on $LSB_DJOB_HOSTFILE\"\n",
    "echo \"======\"\n",
    "\n",
    "master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\")\n",
    "master_port=10002\n",
    "RDZV_ENDPOINT=$master_addr:$master_port\n",
    "\n",
    "source ~/.profile\n",
    "conda activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/doremi/\n",
    "\n",
    "set -e\n",
    "set -x\n",
    "echo \"======\"\n",
    "srun {cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/$LSB_JOBID*.out\" ] && mv {log_dir}/$LSB_JOBID*.out {save_dir}\n",
    "\"\"\"\n",
    "\n",
    "shell_scripts_template = shell_scripts_template_slurm \\\n",
    "    if arch == 'ppc64le' else shell_scripts_template_lsf\n",
    "\n",
    "print(shell_scripts_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2aeea5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CACHE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache\n",
      "export DOREMI_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi\n",
      "export PILE_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/raw\n",
      "export PREPROCESSED_PILE_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed\n",
      "export MODEL_OUTPUT_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results\n",
      "export PARTITION=el8\n",
      "export HF_HOME=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache\n",
      "export TRANSFORMERS_CACHE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache\n",
      "export HF_DATASETS_CACHE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache\n",
      "export HF_DATASETS_IN_MEMORY_MAX_SIZE=0\n",
      "export TORCH_EXTENSIONS_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache\n",
      "export TMPDIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache\n",
      "export WANDB_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/wandb\n",
      "export WANDB_MODE=offline\n",
      "export PREPROCESSED_DATA=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed\n",
      "export PREPROCESSED_CACHE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/preprocessed_cache/perdomain_pile_preprocessed\n"
     ]
    }
   ],
   "source": [
    "## note setting the env in this notebook, then launch jobs \n",
    "# will inherit those env variables.\n",
    "\n",
    "package_dir = \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi\"\n",
    "cache_dir = '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache'\n",
    "preprocessed_data = \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed\"\n",
    "\n",
    "envs = {\n",
    "    \"CACHE\": cache_dir,\n",
    "    \"DOREMI_DIR\": package_dir,\n",
    "    \"PILE_DIR\": os.path.join(package_dir, \"data\", 'raw'),\n",
    "    \"PREPROCESSED_PILE_DIR\": preprocessed_data,\n",
    "    \"MODEL_OUTPUT_DIR\": os.path.join(package_dir, 'results'),\n",
    "    \"PARTITION\": \"el8\",\n",
    "    \"HF_HOME\": cache_dir,\n",
    "    \"TRANSFORMERS_CACHE\": cache_dir,\n",
    "    \"HF_DATASETS_CACHE\": cache_dir,\n",
    "    \"HF_DATASETS_IN_MEMORY_MAX_SIZE\": \"0\",\n",
    "    \"TORCH_EXTENSIONS_DIR\": cache_dir,\n",
    "    \"TMPDIR\": cache_dir,\n",
    "    \"WANDB_DIR\": os.path.join(cache_dir, \"wandb\"),\n",
    "    \"WANDB_MODE\": 'offline',\n",
    "    \"PREPROCESSED_DATA\": preprocessed_data,\n",
    "    'PREPROCESSED_CACHE': os.path.join(cache_dir, 'preprocessed_cache', 'perdomain_pile_preprocessed'),\n",
    "}\n",
    "\n",
    "for k, v in envs.items():\n",
    "    os.environ[k] = v\n",
    "    \n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "print('\\n'.join([f'export {k}={v}' for k, v in envs.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cad02415",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(package_dir, 'constants.sh'), 'w') as f:\n",
    "    f.writelines('#!/bin/bash\\n')\n",
    "    f.writelines('\\n'.join([f'{k}={v}' for k, v in envs.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15e072ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70685306, 0.70685306, 0.01061057, 0.02459495])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# cot, flanv2, dolly, oasst1\n",
    "x = np.array([1000000, 1000000, 15011, 34795])\n",
    "x/np.sqrt((x*x).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f3085fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate domain weight config\n",
    "import json\n",
    "domain_config_path = os.path.abspath('../configs/humanmix_baseline_50kvocab.json')\n",
    "domain_weights = {\"cot\": .25, \"flan_v2\": .25, \"dolly\": .25, \"oasst1\": .25}\n",
    "domain_weights = {'cot': 0.5, 'flan_v2': 0.25, 'dolly': 0.12, 'oasst1': 0.13}\n",
    "domain_weights = {'cot': 0.70685306, 'flan_v2': 0.70685306, 'dolly': 0.01061057, 'oasst1': 0.02459495}\n",
    "abbr_train_file = 'humanmix'\n",
    "\n",
    "domain_config = {\"train_domain_weights\": domain_weights, \"eval_domain_weights\": domain_weights}\n",
    "with open(domain_config_path, 'w') as f:\n",
    "    json.dump(domain_config, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f36c8542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/gpt2-medium using 6 GPUs, 2 batch size per GPU, 10 gradient accumulation steps,for 1562 max steps.\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"ft1\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=ft1 --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/scripts/tmpgsl3_eur', 'job_id': 709549}]\n"
     ]
    }
   ],
   "source": [
    "# train gpt2 base model\n",
    "job_name = 'ft1'\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "job_duration = 6\n",
    "\n",
    "nodes = 1 # 128/(5*6*2)~=2.1\n",
    "num_cpus, cpu_mem = (144, 512) if arch == 'ppc64le' else (32, 64)\n",
    "num_gpus = 6; gpu_type = 'v100'\n",
    "# num_gpus = 1; gpu_type = 'v100'\n",
    "\n",
    "use_doremi = False\n",
    "overwrite_output_dir = True if test_run else False\n",
    "\n",
    "model_name_or_path = ('/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/'\n",
    "                      'gpt2-medium')\n",
    "model_type = 'gpt2'; abbr_model_name = 'gpt2-medium'\n",
    "\n",
    "cache_dir = envs['CACHE']\n",
    "domain_config_path = os.path.abspath('../configs/humanmix_baseline_50kvocab.json')\n",
    "dataset_dir = preprocessed_data\n",
    "\n",
    "if not use_doremi:\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_checkpointing = False\n",
    "else:\n",
    "    per_device_train_batch_size = 1\n",
    "    gradient_checkpointing = True\n",
    "\n",
    "total_batch_size = 128 # # 64*8=512\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_accumulation_steps = int(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "max_steps = int(200000/total_batch_size); save_steps = 100 # 200k steps.\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{per_device_train_batch_size} batch size per GPU, \"\n",
    "      f\"{gradient_accumulation_steps} gradient accumulation steps,\"\n",
    "      f\"for {max_steps} max steps.\")\n",
    "\n",
    "\n",
    "if nodes == 1:\n",
    "    exe = 'python' if num_gpus==1 else \\\n",
    "        f\"\"\"torchrun \\\n",
    "        --nproc_per_node={num_gpus} \\\n",
    "        --master_port=10002\"\"\"\n",
    "else:\n",
    "    exe = f\"\"\"torchrun \\\n",
    "              --nnodes={nodes} \\\n",
    "              --nproc_per_node={num_gpus} \\\n",
    "              --rdzv-id=$SLURM_JOB_ID \\\n",
    "              --rdzv-backend=c10d \\\n",
    "              --rdzv-endpoint=$RDZV_ENDPOINT\"\"\"\n",
    "if test_run:\n",
    "    exe = f\"CUDA_VISIBLE_DEVICES={','.join(map(str, range(num_gpus)))} {exe}\"\n",
    "\n",
    "    \n",
    "output_dirname = f'{abbr_model_name}_{abbr_train_file}'\n",
    "if use_doremi:\n",
    "    output_dirname += '_doremi'\n",
    "else:\n",
    "    output_dirname += '_baseline'\n",
    "if test_run:\n",
    "    output_dirname = 'jpt_'+output_dirname\n",
    "run_name = os.path.join(job_name, output_dirname)\n",
    "output_dir = os.path.join(envs['MODEL_OUTPUT_DIR'], run_name)\n",
    "\n",
    "\n",
    "if use_doremi:\n",
    "    reference_model_name_or_path = (\n",
    "        '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/'\n",
    "        'ft1/jpt_gpt2-medium_humanmix_baseline/checkpoint-10')\n",
    "    doremi_options = f\"\"\"\n",
    "        --doremi_optimizer=doremiv1 \\\n",
    "        --reweight_eta=1 \\\n",
    "        --reweight_eps=1e-4 \\\n",
    "        --train_domain_weights_tmp_file={os.path.join(output_dir, 'domain_weights')} \\\n",
    "        --reweight_domains \\\n",
    "        --remove_unused_columns=False \\\n",
    "        --reference_model_name_or_path={reference_model_name_or_path} \\\n",
    "    \"\"\"\n",
    "else:\n",
    "    doremi_options = ''\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    {'!cd .. && ' if test_run else ''}{exe}\n",
    "    doremi/train.py \\\n",
    "    --model_name_or_path={model_name_or_path} \\\n",
    "    --model_type={model_type} \\\n",
    "    --do_train \\\n",
    "    --cache_dir={cache_dir} \\\n",
    "    --dataset_dir={dataset_dir} \\\n",
    "    --domain_config_path={domain_config_path} \\\n",
    "    --max_token_length=1024 \\\n",
    "    --per_device_train_batch_size={per_device_train_batch_size} \\\n",
    "    --gradient_accumulation_steps={gradient_accumulation_steps} \\\n",
    "    --dataloader_num_workers=1 \\\n",
    "    --learning_rate=2e-5 \\\n",
    "    --lr_scheduler_type=linear \\\n",
    "    --warmup_ratio=0.03 \\\n",
    "    --weight_decay=0. \\\n",
    "    --max_grad_norm=1.0 \\\n",
    "    --max_steps={max_steps} \\\n",
    "    --evaluation_strategy=no \\\n",
    "    --save_strategy=steps \\\n",
    "    --save_steps={save_steps} \\\n",
    "    --save_total_limit=1 \\\n",
    "    --run_name={run_name} \\\n",
    "    --seed=1111 \\\n",
    "    --logging_strategy=steps \\\n",
    "    --logging_steps=10 \\\n",
    "    --logging_first_step \\\n",
    "    --report_to=all \\\n",
    "    --optim=adamw_hf \\\n",
    "    --adam_beta1=0.9 \\\n",
    "    --adam_beta2=0.99 \\\n",
    "    {'--gradient_checkpointing' if gradient_checkpointing  else ''} \\\n",
    "    {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "    --torch_dtype=float32 \\\n",
    "    --add_domain_id=True \\\n",
    "    --do_padding=True \\\n",
    "    --fp16 \\\n",
    "    {doremi_options if doremi_options else ''} \\\n",
    "    --output_dir={output_dir} \\\n",
    "\"\"\"\n",
    "\n",
    "if test_run:\n",
    "    cmds = [x.strip() for x in cmd.split('    ') if x.strip()]\n",
    "    cmds = ['    '+x if x.startswith('--') else x for x in cmds]\n",
    "    print()\n",
    "    print(' \\\\\\n'.join(cmds))\n",
    "cmd = multiline_to_singleline(cmd)\n",
    "\n",
    "shell_scripts = shell_scripts_template.format(\n",
    "    cmd=cmd,\n",
    "    log_dir=os.getcwd(),\n",
    "    save_dir=output_dir)\n",
    "out = submit_job(\n",
    "    shell_scripts, \n",
    "    job_name=job_name, \n",
    "    nodes=nodes,\n",
    "    num_cpus=num_cpus,\n",
    "    cpu_mem=cpu_mem,\n",
    "    num_gpus=num_gpus,\n",
    "    gpu_type=gpu_type,\n",
    "    test_run=test_run,\n",
    "    job_duration=job_duration,\n",
    ")\n",
    "if not test_run:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7f4f218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a897ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# shared: v100_32gb gpu \n",
    "#\n",
    "# train baseline on 200k data\n",
    "# gpt2, nodes=1, num_gpus=6, micro-bsz=8, no-grad-ckpt, fp32, 1hr\n",
    "# gpt2-medium, nodes=1, num_gpus=6, micro-bsz=4, no-grad-ckpt, fp32, oom\n",
    "# gpt2-medium, nodes=1, num_gpus=6, micro-gsz=2, no-grad-ckpt, fp32, 2.5hrs\n",
    "# gpt2-medium, nodes=1, num_gpus=6, micro-gsz=2, no-grad-ckpt, fp16, 1.5hrs\n",
    "#\n",
    "# train doremi on 200k data\n",
    "# gpt2-medium, nodes=1, num_gpus=6, micro-gsz=2, no-grad-ckpt, fp32, oom\n",
    "# gpt2-medium, nodes=1, num_gpus=6, micro-gsz=1, grad-ckpt, fp16, oom\n",
    "# gpt2-medium, nodes=1, num_gpus=6, micro-gsz=1, grad-ckpt, fp32, 4hrs\n",
    "# gpt2-medium, nodes=1, num_gpus=6, micro-gsz=1, grad-ckpt, fp16, 2.5hrs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
