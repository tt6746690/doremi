{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ff5ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('ppc64le', 'dcs')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "import re\n",
    "from llm.submit import (\n",
    "    multiline_to_singleline,\n",
    "    submit_job_ccc,\n",
    "    submit_job_aimos,\n",
    "    submit_job,\n",
    "    get_run_statistics)\n",
    "import pandas as pd\n",
    "import json\n",
    "import platform\n",
    "import tempfile\n",
    "import subprocess\n",
    "import shlex\n",
    "import datetime\n",
    "import itertools\n",
    "import socket\n",
    "\n",
    "arch = platform.uname().processor\n",
    "hostname = socket.gethostname()\n",
    "cluster = 'ccc' if hostname.startswith('ccc') else ('dcs' if hostname.startswith('dcs') else 'npl')\n",
    "arch, cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db349029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "echo \"Running on $SLURM_JOB_NODELIST\"\n",
      "echo \"======\"\n",
      "\n",
      "master_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n",
      "master_port=10002\n",
      "RDZV_ENDPOINT=$master_addr:$master_port\n",
      "\n",
      "source ~/.profile\n",
      "conda activate open-instruct\n",
      "cd /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/\n",
      "\n",
      "set -e\n",
      "set -x\n",
      "echo \"======\"\n",
      "srun {cmd}\n",
      "\n",
      "[ ! -f \"{log_dir}/$SLURM_JOB_ID*.out\" ] && mv {log_dir}/$SLURM_JOB_ID*.out {save_dir}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shell_scripts_template_slurm = \"\"\"\n",
    "echo \"Running on $SLURM_JOB_NODELIST\"\n",
    "echo \"======\"\n",
    "\n",
    "master_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n",
    "master_port=10002\n",
    "RDZV_ENDPOINT=$master_addr:$master_port\n",
    "\n",
    "source ~/.profile\n",
    "conda activate open-instruct\n",
    "cd /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/\n",
    "\n",
    "set -e\n",
    "set -x\n",
    "echo \"======\"\n",
    "srun {cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/$SLURM_JOB_ID*.out\" ] && mv {log_dir}/$SLURM_JOB_ID*.out {save_dir}\n",
    "\"\"\"\n",
    "\n",
    "shell_scripts_template_lsf = \"\"\"\n",
    "echo \"Running on $LSB_DJOB_HOSTFILE\"\n",
    "echo \"======\"\n",
    "\n",
    "master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\")\n",
    "master_port=10002\n",
    "RDZV_ENDPOINT=$master_addr:$master_port\n",
    "\n",
    "source ~/.profile\n",
    "conda activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/doremi/\n",
    "\n",
    "set -e\n",
    "set -x\n",
    "echo \"======\"\n",
    "srun {cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/$LSB_JOBID*.out\" ] && mv {log_dir}/$LSB_JOBID*.out {save_dir}\n",
    "\"\"\"\n",
    "\n",
    "shell_scripts_template = shell_scripts_template_slurm \\\n",
    "    if arch == 'ppc64le' else shell_scripts_template_lsf\n",
    "\n",
    "print(shell_scripts_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aeea5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CACHE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache\n",
      "export DOREMI_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi\n",
      "export PILE_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/raw\n",
      "export PREPROCESSED_PILE_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed\n",
      "export MODEL_OUTPUT_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results\n",
      "export PARTITION=el8\n",
      "export HF_HOME=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache\n",
      "export TRANSFORMERS_CACHE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache\n",
      "export HF_DATASETS_CACHE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache\n",
      "export HF_DATASETS_IN_MEMORY_MAX_SIZE=0\n",
      "export TORCH_EXTENSIONS_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache\n",
      "export TMPDIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache\n",
      "export WANDB_DIR=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/wandb\n",
      "export WANDB_MODE=offline\n",
      "export WANDB_PROJECT=huggingface\n",
      "export PREPROCESSED_DATA=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed\n",
      "export PREPROCESSED_CACHE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/preprocessed_cache/perdomain_pile_preprocessed\n"
     ]
    }
   ],
   "source": [
    "## note setting the env in this notebook, then launch jobs \n",
    "# will inherit those env variables.\n",
    "\n",
    "package_dir = \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi\"\n",
    "cache_dir = '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache'\n",
    "preprocessed_data = \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed\"\n",
    "\n",
    "envs = {\n",
    "    \"CACHE\": cache_dir,\n",
    "    \"DOREMI_DIR\": package_dir,\n",
    "    \"PILE_DIR\": os.path.join(package_dir, \"data\", 'raw'),\n",
    "    \"PREPROCESSED_PILE_DIR\": preprocessed_data,\n",
    "    \"MODEL_OUTPUT_DIR\": os.path.join(package_dir, 'results'),\n",
    "    \"PARTITION\": \"el8\",\n",
    "    \"HF_HOME\": cache_dir,\n",
    "    \"TRANSFORMERS_CACHE\": cache_dir,\n",
    "    \"HF_DATASETS_CACHE\": cache_dir,\n",
    "    \"HF_DATASETS_IN_MEMORY_MAX_SIZE\": \"0\",\n",
    "    \"TORCH_EXTENSIONS_DIR\": cache_dir,\n",
    "    \"TMPDIR\": cache_dir,\n",
    "    \"WANDB_DIR\": os.path.join(cache_dir, \"wandb\"),\n",
    "    \"WANDB_MODE\": 'offline',\n",
    "    'WANDB_PROJECT': 'huggingface',\n",
    "    \"PREPROCESSED_DATA\": preprocessed_data,\n",
    "    'PREPROCESSED_CACHE': os.path.join(cache_dir, 'preprocessed_cache', 'perdomain_pile_preprocessed'),\n",
    "}\n",
    "\n",
    "for k, v in envs.items():\n",
    "    os.environ[k] = v\n",
    "    \n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "print('\\n'.join([f'export {k}={v}' for k, v in envs.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cad02415",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(package_dir, 'constants.sh'), 'w') as f:\n",
    "    f.writelines('#!/bin/bash\\n')\n",
    "    f.writelines('\\n'.join([f'{k}={v}' for k, v in envs.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e072ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48785105, 0.48785105, 0.00732313, 0.01697478])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# cot, flanv2, dolly, oasst1\n",
    "x = np.array([1000000, 1000000, 15011, 34795])\n",
    "x/x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f3085fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate domain weight config\n",
    "import json\n",
    "\n",
    "\n",
    "for abbr_train_file_mix, domain_weights in [\n",
    "    ('humanmix_scaled',  {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}),\n",
    "    ('humanmix_uniform', {\"cot\": .25, \"flan_v2\": .25, \"dolly\": .25, \"oasst1\": .25})\n",
    "]:\n",
    "    domain_config_path = os.path.abspath(f'../configs/{abbr_train_file_mix}_baseline_50kvocab.json')\n",
    "    domain_config = {\"train_domain_weights\": domain_weights, \"eval_domain_weights\": domain_weights}\n",
    "    with open(domain_config_path, 'w') as f:\n",
    "        json.dump(domain_config, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8cf21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f36c8542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/EleutherAI/pythia-6.9b using 6 GPUs, 4 batch size per GPU, 1 gradient accumulation steps, for 1666 max steps. Effective batch size 120\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"drm2\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=drm2 --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/scripts/tmpo65z2a8n', 'job_id': 730024}]\n"
     ]
    }
   ],
   "source": [
    "# train gpt2 base&doremi model\n",
    "job_name = 'ft1'\n",
    "\n",
    "# train gpt2 base&doremi model on correct humanmix data\n",
    "# try to fix error with domain weights -> NaN\n",
    "job_name = 'drm2'\n",
    "\n",
    "test_run = 0\n",
    "test_run = bool(test_run)\n",
    "job_duration = 6\n",
    "\n",
    "test_oom = False\n",
    "report_to = 'all'\n",
    "num_cpus, cpu_mem = (144, 512) if arch == 'ppc64le' else (32, 64)\n",
    "nodes = 1; num_gpus = 6; gpu_type = 'v100'\n",
    "nodes = 5; num_gpus = 6; gpu_type = 'v100'\n",
    "\n",
    "use_doremi = False\n",
    "doremi_optimizer = 'doremiv1'\n",
    "doremi_optimizer = 'doremiv2'\n",
    "overwrite_output_dir = True if test_run else False\n",
    "\n",
    "hf_models_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/'\n",
    "model_name_or_path = os.path.join(hf_models_dir, 'gpt2-medium'); model_type = 'gpt2'; abbr_model_name = 'gpt2-medium'\n",
    "model_name_or_path = os.path.join(hf_models_dir, 'gpt2-xl'); model_type = 'gpt2'; abbr_model_name = 'gpt2-xl'\n",
    "model_name_or_path = os.path.join(hf_models_dir, 'EleutherAI/pythia-1.4b'); model_type = 'pythia'; abbr_model_name = 'pythia-1.4b'\n",
    "model_name_or_path = os.path.join(hf_models_dir, 'EleutherAI/pythia-6.9b'); model_type = 'pythia'; abbr_model_name = 'pythia-6.9b'\n",
    "# model_name_or_path = os.path.join(hf_models_dir, 'huggyllama/llama-7b'); model_type = 'llama'; abbr_model_name = 'llama-7b'\n",
    "\n",
    "abbr_train_file = 'humanmix_scaled'\n",
    "abbr_train_file = 'humanmix_uniform'\n",
    "\n",
    "cache_dir = envs['CACHE']\n",
    "domain_config_path = os.path.abspath(f'../configs/{abbr_train_file}_baseline_50kvocab.json')\n",
    "dataset_dir = preprocessed_data\n",
    "\n",
    "if abbr_model_name in {'llama-7b', 'pythia-6.9b'}:\n",
    "    if nodes == 1: per_device_train_batch_size = 2\n",
    "    elif nodes == 5: per_device_train_batch_size = 4\n",
    "    else: raise ValueError\n",
    "    gradient_checkpointing = True\n",
    "else:\n",
    "    per_device_train_batch_size = 11\n",
    "    gradient_checkpointing = True\n",
    "\n",
    "\n",
    "total_data_points = 200000\n",
    "total_batch_size = 128\n",
    "gradient_accumulation_steps = round(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "effective_batch_size = per_device_train_batch_size*nodes*num_gpus*gradient_accumulation_steps\n",
    "max_steps = int(total_data_points/effective_batch_size); save_steps = 100\n",
    "# save_steps = 10\n",
    "\n",
    "lr = 2e-5\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{per_device_train_batch_size} batch size per GPU, \"\n",
    "      f\"{gradient_accumulation_steps} gradient accumulation steps, \"\n",
    "      f\"for {max_steps} max steps. \"\n",
    "      f\"Effective batch size {effective_batch_size}\")\n",
    "\n",
    "fsdp = \"full_shard auto_wrap\" if num_gpus*nodes > 1 else False\n",
    "if 'gpt2' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "elif 'mpt' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MPTBlock'\n",
    "elif 'pythia' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPTNeoXLayer'\n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "    \n",
    "    \n",
    "# if nodes == 1:\n",
    "if False:\n",
    "    exe = 'python' if num_gpus==1 else \\\n",
    "        f\"\"\"torchrun \\\n",
    "        --nproc_per_node={num_gpus} \\\n",
    "        --master_port=10002\"\"\"\n",
    "else:\n",
    "    exe = f\"\"\"torchrun \\\n",
    "              --nnodes={nodes} \\\n",
    "              --nproc_per_node={num_gpus} \\\n",
    "              --rdzv-id=$SLURM_JOB_ID \\\n",
    "              --rdzv-backend=c10d \\\n",
    "              --rdzv-endpoint=$RDZV_ENDPOINT\"\"\"\n",
    "if test_run:\n",
    "    exe = f\"CUDA_VISIBLE_DEVICES={','.join(map(str, range(num_gpus)))} {exe}\"\n",
    "\n",
    "    \n",
    "output_dirname = f'{abbr_model_name}_{abbr_train_file}:{int(total_data_points/1000)}k'\n",
    "if use_doremi:\n",
    "    output_dirname += f'_{doremi_optimizer}'\n",
    "else:\n",
    "    output_dirname += '_baseline'\n",
    "\n",
    "if test_run:\n",
    "    run_name = 'jpt_'+output_dirname\n",
    "else:\n",
    "    run_name = os.path.join(job_name, output_dirname)\n",
    "if test_oom:\n",
    "    run_name += \\\n",
    "        ('_fsdp='+fsdp.split(' ')[0] if fsdp else '')+\\\n",
    "        ('_gradckpt='+str(gradient_checkpointing) if gradient_checkpointing else '')+\\\n",
    "        '_mbsz='+str(per_device_train_batch_size)+\\\n",
    "        '_seqlen='+str(1024)+\\\n",
    "        '_nodes='+str(nodes)\n",
    "output_dir = os.path.join(envs['MODEL_OUTPUT_DIR'], run_name)\n",
    "\n",
    "if test_run or test_oom:\n",
    "    report_to = 'none'\n",
    "\n",
    "\n",
    "if use_doremi:\n",
    "    reference_model_name_or_path = \\\n",
    "        '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/'\n",
    "    if model_type == 'gpt2':\n",
    "        if abbr_train_file == 'humanmix_uniform':\n",
    "            reference_model_name_or_path += f'drm2/{abbr_model_name}_humanmix_uniform:200k_baseline/'\n",
    "        elif abbr_train_file == 'humanmix_scaled':\n",
    "            reference_model_name_or_path += f'drm2/{abbr_model_name}_humanmix_scaled:200k_baseline/'\n",
    "    elif model_type == 'pythia':\n",
    "        reference_model_name_or_path += f'drm2/{abbr_model_name}_humanmix_uniform:200k_baseline/'\n",
    "    elif model_type == 'llama':\n",
    "        reference_model_name_or_path += f'drm2/{abbr_model_name}_humanmix_uniform:200k_baseline/'\n",
    "    else:\n",
    "        raise ValueError\n",
    "    doremi_options = f\"\"\"\n",
    "        --doremi_optimizer={doremi_optimizer} \\\n",
    "        --reweight_eta=1 \\\n",
    "        --reweight_eps=1e-4 \\\n",
    "        --train_domain_weights_tmp_file={os.path.join(output_dir, 'domain_weights')} \\\n",
    "        --reweight_domains \\\n",
    "        --remove_unused_columns=False \\\n",
    "        --reference_model_name_or_path={reference_model_name_or_path} \\\n",
    "    \"\"\"\n",
    "else:\n",
    "    doremi_options = ''\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    {'!cd .. && ' if test_run else ''}{exe}\n",
    "    doremi/train.py \\\n",
    "    --model_name_or_path={model_name_or_path} \\\n",
    "    --model_type={model_type} \\\n",
    "    --do_train \\\n",
    "    --cache_dir={cache_dir} \\\n",
    "    --dataset_dir={dataset_dir} \\\n",
    "    --domain_config_path={domain_config_path} \\\n",
    "    --max_token_length=1024 \\\n",
    "    --per_device_train_batch_size={per_device_train_batch_size} \\\n",
    "    --gradient_accumulation_steps={gradient_accumulation_steps} \\\n",
    "    --dataloader_num_workers=1 \\\n",
    "    --learning_rate={lr} \\\n",
    "    --lr_scheduler_type=linear \\\n",
    "    --warmup_ratio=0.03 \\\n",
    "    --weight_decay=0. \\\n",
    "    --max_grad_norm=1.0 \\\n",
    "    --max_steps={max_steps} \\\n",
    "    --evaluation_strategy=no \\\n",
    "    --save_strategy=steps \\\n",
    "    --save_steps={save_steps} \\\n",
    "    --save_total_limit=1 \\\n",
    "    --run_name={run_name} \\\n",
    "    --seed=1111 \\\n",
    "    --logging_strategy=steps \\\n",
    "    --logging_steps=10 \\\n",
    "    --logging_first_step \\\n",
    "    --report_to={report_to} \\\n",
    "    --optim=adamw_hf \\\n",
    "    --adam_beta1=0.9 \\\n",
    "    --adam_beta2=0.99 \\\n",
    "    {'--fsdp=\"'+fsdp+'\"' if fsdp else ''} \\\n",
    "    {'--fsdp_transformer_layer_cls_to_wrap=\"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "        if fsdp else ''} \\\n",
    "    {'--gradient_checkpointing' if gradient_checkpointing  else ''} \\\n",
    "    --torch_dtype=float32 \\\n",
    "    --add_domain_id=True \\\n",
    "    --do_padding=True \\\n",
    "    --fp16 \\\n",
    "    {doremi_options if doremi_options else ''} \\\n",
    "    {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "    --output_dir={output_dir} \\\n",
    "\"\"\"\n",
    "\n",
    "if test_run:\n",
    "    cmds = [x.strip() for x in cmd.split('    ') if x.strip()]\n",
    "    cmds = ['    '+x if x.startswith('--') else x for x in cmds]\n",
    "    print()\n",
    "    print(' \\\\\\n'.join(cmds))\n",
    "cmd = multiline_to_singleline(cmd)\n",
    "\n",
    "shell_scripts = shell_scripts_template.format(\n",
    "    cmd=cmd,\n",
    "    log_dir=os.getcwd(),\n",
    "    save_dir=output_dir)\n",
    "out = submit_job(\n",
    "    shell_scripts, \n",
    "    job_name=job_name, \n",
    "    nodes=nodes,\n",
    "    num_cpus=num_cpus,\n",
    "    cpu_mem=cpu_mem,\n",
    "    num_gpus=num_gpus,\n",
    "    gpu_type=gpu_type,\n",
    "    test_run=test_run,\n",
    "    job_duration=job_duration,\n",
    ")\n",
    "if not test_run:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a29b534c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[2023-08-09 15:13:11,482] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-08-09 15:13:11,482] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-08-09 15:13:11,511] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-08-09 15:13:11,543] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-08-09 15:13:11,547] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-08-09 15:13:11,553] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/training_args.py:1519: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/training_args.py:1519: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "08/09/2023 15:13:21 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "08/09/2023 15:13:21 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/training_args.py:1519: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "08/09/2023 15:13:21 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/training_args.py:1519: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "08/09/2023 15:13:21 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/training_args.py:1519: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "08/09/2023 15:13:21 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/training_args.py:1519: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "08/09/2023 15:13:21 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "08/09/2023 15:13:21 - INFO - __main__ - Training/evaluation parameters FullTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.99,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=1,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "domain_config_path=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/configs/humanmix_uniform_baseline_50kvocab.json,\n",
      "doremi_optimizer=doremiv2,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'fsdp_transformer_layer_cls_to_wrap': ['GPTNeoXLayer'], 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=GPTNeoXLayer,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=3,\n",
      "gradient_checkpointing=True,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/jpt_pythia-1.4b_humanmix_uniform:200k_doremiv2_fsdp=full_shard_gradckpt=True_mbsz=7_seqlen=1024_nodes=1/runs/Aug09_15-13-20_dcs147,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_end=0.001,\n",
      "lr_scheduler_name=None,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=1587,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/jpt_pythia-1.4b_humanmix_uniform:200k_doremiv2_fsdp=full_shard_gradckpt=True_mbsz=7_seqlen=1024_nodes=1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=7,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "reference_model_name_or_path=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2/pythia-1.4b_humanmix_uniform:200k_baseline/,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "reweight_domains=True,\n",
      "reweight_eps=0.0001,\n",
      "reweight_eta=1.0,\n",
      "run_name=jpt_pythia-1.4b_humanmix_uniform:200k_doremiv2_fsdp=full_shard_gradckpt=True_mbsz=7_seqlen=1024_nodes=1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=100,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=1111,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_domain_weights_tmp_file=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/jpt_pythia-1.4b_humanmix_uniform:200k_doremiv2_fsdp=full_shard_gradckpt=True_mbsz=7_seqlen=1024_nodes=1/domain_weights,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "[INFO|configuration_utils.py:710] 2023-08-09 15:13:21,446 >> loading configuration file /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/EleutherAI/pythia-1.4b/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-09 15:13:21,448 >> Model config GPTNeoXConfig {\n",
      "  \"_name_or_path\": \"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/EleutherAI/pythia-1.4b\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoXForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neox\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rotary_emb_base\": 10000,\n",
      "  \"rotary_pct\": 0.25,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.32.0.dev0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_parallel_residual\": true,\n",
      "  \"vocab_size\": 50304\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-09 15:13:21,450 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-09 15:13:21,450 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-09 15:13:21,450 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-09 15:13:21,450 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-09 15:13:21,450 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-09 15:13:21,450 >> loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:2604] 2023-08-09 15:13:21,522 >> loading weights file /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/EleutherAI/pythia-1.4b/model.safetensors\n",
      "[INFO|modeling_utils.py:1176] 2023-08-09 15:13:21,570 >> Instantiating GPTNeoXForCausalLMDoReMi model under default dtype torch.float32.\n",
      "[INFO|configuration_utils.py:603] 2023-08-09 15:13:21,570 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.32.0.dev0\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3333] 2023-08-09 15:13:39,811 >> All model checkpoint weights were used when initializing GPTNeoXForCausalLMDoReMi.\n",
      "\n",
      "[INFO|modeling_utils.py:3341] 2023-08-09 15:13:39,812 >> All the weights of GPTNeoXForCausalLMDoReMi were initialized from the model checkpoint at /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/EleutherAI/pythia-1.4b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoXForCausalLMDoReMi for predictions without further training.\n",
      "[INFO|modeling_utils.py:2953] 2023-08-09 15:13:39,818 >> Generation config file not found, using a generation config created from the model config.\n",
      "08/09/2023 15:13:39 - WARNING - doremi.dataloader - No split used or split directory not found: using same data for all splits.\n",
      "08/09/2023 15:13:39 - WARNING - doremi.dataloader - No split used or split directory not found: using same data for all splits.\n",
      "08/09/2023 15:13:39 - INFO - datasets.builder - Using custom data configuration default-20e8d8f5df875937\n",
      "08/09/2023 15:13:39 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/09/2023 15:13:39 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/09/2023 15:13:39 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/09/2023 15:13:39 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/09/2023 15:13:39 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 253.36it/s]\n",
      "08/09/2023 15:13:39 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 280.76it/s]\n",
      "08/09/2023 15:13:39 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1ef1c4d5d5eb45a9_00000_of_00008.arrow\n",
      "08/09/2023 15:13:39 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1ef1c4d5d5eb45a9_00001_of_00008.arrow\n",
      "08/09/2023 15:13:39 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1ef1c4d5d5eb45a9_00002_of_00008.arrow\n",
      "08/09/2023 15:13:39 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1ef1c4d5d5eb45a9_00003_of_00008.arrow\n",
      "08/09/2023 15:13:39 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1ef1c4d5d5eb45a9_00004_of_00008.arrow\n",
      "08/09/2023 15:13:39 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1ef1c4d5d5eb45a9_00005_of_00008.arrow\n",
      "08/09/2023 15:13:39 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1ef1c4d5d5eb45a9_00006_of_00008.arrow\n",
      "08/09/2023 15:13:39 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1ef1c4d5d5eb45a9_00007_of_00008.arrow\n",
      "08/09/2023 15:13:39 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1ef1c4d5d5eb45a9_*_of_00008.arrow\n",
      "08/09/2023 15:13:39 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "08/09/2023 15:13:40 - WARNING - doremi.dataloader - No split used or split directory not found: using same data for all splits.\n",
      "08/09/2023 15:13:40 - WARNING - doremi.dataloader - No split used or split directory not found: using same data for all splits.\n",
      "08/09/2023 15:13:40 - WARNING - doremi.dataloader - No split used or split directory not found: using same data for all splits.\n",
      "08/09/2023 15:13:40 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 268.08it/s]\n",
      "08/09/2023 15:13:40 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]08/09/2023 15:13:40 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 301.49it/s]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 298.44it/s]\n",
      "08/09/2023 15:13:40 - WARNING - doremi.dataloader - No split used or split directory not found: using same data for all splits.\n",
      "08/09/2023 15:13:40 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 298.19it/s]\n",
      "08/09/2023 15:13:43 - INFO - datasets.builder - Using custom data configuration default-0832d4a20d2543d7\n",
      "08/09/2023 15:13:43 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/09/2023 15:13:43 - INFO - datasets.builder - Using custom data configuration default-68fd04897f8e942c\n",
      "08/09/2023 15:13:43 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/09/2023 15:13:43 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/09/2023 15:13:43 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/09/2023 15:13:43 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 486.80it/s]\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1ef1c4d5d5eb45a9_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1ef1c4d5d5eb45a9_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1ef1c4d5d5eb45a9_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1ef1c4d5d5eb45a9_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1ef1c4d5d5eb45a9_*_of_00008.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-05217fbe08965255_00000_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-05217fbe08965255_00001_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-05217fbe08965255_00002_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-05217fbe08965255_00003_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-05217fbe08965255_00004_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-05217fbe08965255_00005_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-05217fbe08965255_00006_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-05217fbe08965255_00007_of_00008.arrow\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-05217fbe08965255_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 547.70it/s]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 560.14it/s]\n",
      "08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 572.29it/s]\n",
      "08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 559.76it/s]\n",
      "08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 569.26it/s]\n",
      "08/09/2023 15:13:43 - INFO - datasets.builder - Using custom data configuration default-77827ab405161143\n",
      "08/09/2023 15:13:43 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/09/2023 15:13:43 - INFO - datasets.builder - Using custom data configuration default-a01381664fd2589b\n",
      "08/09/2023 15:13:43 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/09/2023 15:13:43 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/09/2023 15:13:43 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/09/2023 15:13:43 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 282.35it/s]\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-05217fbe08965255_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-05217fbe08965255_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-05217fbe08965255_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-05217fbe08965255_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-05217fbe08965255_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2316d34d5db85751_00000_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2316d34d5db85751_00001_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2316d34d5db85751_00002_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2316d34d5db85751_00003_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2316d34d5db85751_00004_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2316d34d5db85751_00005_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2316d34d5db85751_00006_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2316d34d5db85751_00007_of_00008.arrow\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2316d34d5db85751_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 319.23it/s]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 328.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 325.87it/s]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 326.58it/s]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 323.46it/s]\n",
      "08/09/2023 15:13:43 - INFO - datasets.builder - Using custom data configuration default-5dedb6916d948b95\n",
      "08/09/2023 15:13:43 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/09/2023 15:13:43 - INFO - datasets.builder - Using custom data configuration default-d43591dedd2996a1\n",
      "08/09/2023 15:13:43 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/09/2023 15:13:43 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/09/2023 15:13:43 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/09/2023 15:13:43 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 354.73it/s]\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2316d34d5db85751_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2316d34d5db85751_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2316d34d5db85751_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2316d34d5db85751_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2316d34d5db85751_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c795b203283bfb_00000_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c795b203283bfb_00001_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c795b203283bfb_00002_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c795b203283bfb_00003_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c795b203283bfb_00004_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c795b203283bfb_00005_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c795b203283bfb_00006_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c795b203283bfb_00007_of_00008.arrow\n",
      "08/09/2023 15:13:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c795b203283bfb_*_of_00008.arrow\n",
      "08/09/2023 15:13:43 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
      "08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 419.30it/s]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 416.47it/s]\n",
      "08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 422.43it/s]\n",
      "08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]08/09/2023 15:13:43 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 422.94it/s]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 419.60it/s]\n",
      "08/09/2023 15:13:44 - INFO - datasets.builder - Using custom data configuration default-4de9e1c73daeb19f\n",
      "08/09/2023 15:13:44 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR|tokenization_utils_base.py:1056] 2023-08-09 15:13:44,010 >> Using pad_token, but it is not set yet.\n",
      "08/09/2023 15:13:44 - INFO - datasets.builder - Using custom data configuration default-32577c47fcca545c\n",
      "08/09/2023 15:13:44 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/09/2023 15:13:44 - INFO - datasets.builder - Using custom data configuration default-e0bc5ff1a9bb86c3\n",
      "08/09/2023 15:13:44 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/09/2023 15:13:44 - INFO - datasets.builder - Using custom data configuration default-e236a85a58e3a7de\n",
      "08/09/2023 15:13:44 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/09/2023 15:13:44 - INFO - datasets.builder - Using custom data configuration default-862ac8b1ada15789\n",
      "08/09/2023 15:13:44 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "08/09/2023 15:13:44 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c795b203283bfb_*_of_00008.arrow\n",
      "08/09/2023 15:13:44 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c795b203283bfb_*_of_00008.arrow\n",
      "08/09/2023 15:13:44 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c795b203283bfb_*_of_00008.arrow\n",
      "08/09/2023 15:13:44 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c795b203283bfb_*_of_00008.arrow\n",
      "08/09/2023 15:13:44 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c795b203283bfb_*_of_00008.arrow\n",
      "[ERROR|tokenization_utils_base.py:1056] 2023-08-09 15:13:44,077 >> Using pad_token, but it is not set yet.\n",
      "[ERROR|tokenization_utils_base.py:1056] 2023-08-09 15:13:44,077 >> Using pad_token, but it is not set yet.\n",
      "[ERROR|tokenization_utils_base.py:1056] 2023-08-09 15:13:44,078 >> Using pad_token, but it is not set yet.\n",
      "[ERROR|tokenization_utils_base.py:1056] 2023-08-09 15:13:44,078 >> Using pad_token, but it is not set yet.\n",
      "08/09/2023 15:13:44 - INFO - datasets.builder - Using custom data configuration default-d3877c231c0f677d\n",
      "08/09/2023 15:13:44 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "[INFO|modeling_utils.py:2604] 2023-08-09 15:13:44,083 >> loading weights file /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2/pythia-1.4b_humanmix_uniform:200k_baseline/pytorch_model.bin\n",
      "[ERROR|tokenization_utils_base.py:1056] 2023-08-09 15:13:44,128 >> Using pad_token, but it is not set yet.\n",
      "[INFO|modeling_utils.py:1176] 2023-08-09 15:13:45,779 >> Instantiating GPTNeoXForCausalLMDoReMi model under default dtype torch.float32.\n",
      "[INFO|configuration_utils.py:603] 2023-08-09 15:13:45,780 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.32.0.dev0\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3333] 2023-08-09 15:14:02,935 >> All model checkpoint weights were used when initializing GPTNeoXForCausalLMDoReMi.\n",
      "\n",
      "[INFO|modeling_utils.py:3341] 2023-08-09 15:14:02,935 >> All the weights of GPTNeoXForCausalLMDoReMi were initialized from the model checkpoint at /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2/pythia-1.4b_humanmix_uniform:200k_baseline/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoXForCausalLMDoReMi for predictions without further training.\n",
      "[INFO|configuration_utils.py:563] 2023-08-09 15:14:02,942 >> loading configuration file /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2/pythia-1.4b_humanmix_uniform:200k_baseline/generation_config.json\n",
      "[INFO|configuration_utils.py:603] 2023-08-09 15:14:02,942 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.32.0.dev0\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[INFO|trainer.py:565] 2023-08-09 15:14:02,972 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "08/09/2023 15:14:04 - WARNING - accelerate.accelerator - FSDP Warning: When using FSDP, it is efficient and recommended to call prepare for the model before creating the optimizer\n",
      "08/09/2023 15:14:04 - WARNING - accelerate.accelerator - FSDP Warning: When using FSDP, several parameter groups will be conflated into a single one due to nested module wrapping and parameter flattening.\n",
      "[INFO|trainer.py:1682] 2023-08-09 15:14:04,941 >> ***** Running training *****\n",
      "[INFO|trainer.py:1683] 2023-08-09 15:14:04,941 >>   Num examples = 199,962\n",
      "[INFO|trainer.py:1684] 2023-08-09 15:14:04,941 >>   Num Epochs = 9,223,372,036,854,775,807\n",
      "[INFO|trainer.py:1685] 2023-08-09 15:14:04,941 >>   Instantaneous batch size per device = 7\n",
      "[INFO|trainer.py:1688] 2023-08-09 15:14:04,941 >>   Total train batch size (w. parallel, distributed & accumulation) = 126\n",
      "[INFO|trainer.py:1689] 2023-08-09 15:14:04,941 >>   Gradient Accumulation steps = 3\n",
      "[INFO|trainer.py:1690] 2023-08-09 15:14:04,941 >>   Total optimization steps = 1,587\n",
      "[INFO|trainer.py:1691] 2023-08-09 15:14:04,944 >>   Number of trainable parameters = 471,549,302\n",
      "  0%|                                                  | 0/1587 [00:00<?, ?it/s]08/09/2023 15:14:04 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bb309e822886c4af.arrow\n",
      "08/09/2023 15:14:04 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bb309e822886c4af.arrow\n",
      "08/09/2023 15:14:04 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-42137e4a0daf14fd.arrow\n",
      "08/09/2023 15:14:04 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-42137e4a0daf14fd.arrow\n",
      "08/09/2023 15:14:04 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c8e164afe1ef85.arrow\n",
      "08/09/2023 15:14:04 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c8e164afe1ef85.arrow\n",
      "[WARNING|logging.py:280] 2023-08-09 15:14:04,993 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 15:14:04,994 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "08/09/2023 15:14:05 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0dc5944932fab725.arrow\n",
      "08/09/2023 15:14:05 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0dc5944932fab725.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7760, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "08/09/2023 15:14:05 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bb309e822886c4af.arrow\n",
      "08/09/2023 15:14:05 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-42137e4a0daf14fd.arrow\n",
      "08/09/2023 15:14:05 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c8e164afe1ef85.arrow\n",
      "[WARNING|logging.py:280] 2023-08-09 15:14:05,555 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "08/09/2023 15:14:05 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0dc5944932fab725.arrow\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "08/09/2023 15:14:05 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bb309e822886c4af.arrow\n",
      "08/09/2023 15:14:05 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-42137e4a0daf14fd.arrow\n",
      "08/09/2023 15:14:05 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c8e164afe1ef85.arrow\n",
      "08/09/2023 15:14:05 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bb309e822886c4af.arrow\n",
      "[WARNING|logging.py:280] 2023-08-09 15:14:05,928 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "08/09/2023 15:14:05 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-42137e4a0daf14fd.arrow\n",
      "08/09/2023 15:14:05 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0dc5944932fab725.arrow\n",
      "08/09/2023 15:14:05 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c8e164afe1ef85.arrow\n",
      "[WARNING|logging.py:280] 2023-08-09 15:14:05,945 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "08/09/2023 15:14:05 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0dc5944932fab725.arrow\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "08/09/2023 15:14:06 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-d43591dedd2996a1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bb309e822886c4af.arrow\n",
      "08/09/2023 15:14:06 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-a01381664fd2589b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-42137e4a0daf14fd.arrow\n",
      "08/09/2023 15:14:06 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-68fd04897f8e942c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d9c8e164afe1ef85.arrow\n",
      "[WARNING|logging.py:280] 2023-08-09 15:14:06,113 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "08/09/2023 15:14:06 - WARNING - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache/json/default-20e8d8f5df875937/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0dc5944932fab725.arrow\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7760, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7760, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "{'loss': 1.9694, 'learning_rate': 4.1666666666666667e-07, 'epoch': 0.0}         \n",
      "  0%|                                        | 1/1587 [00:14<6:35:31, 14.96s/it]_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7760, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7760, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7760, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "  0%|                                        | 2/1587 [00:26<5:42:33, 12.97s/it]_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7761, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7761, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7761, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "  0%|                                        | 3/1587 [00:38<5:25:32, 12.33s/it]_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7758, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7758, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7758, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "  0%|                                        | 4/1587 [00:49<5:17:39, 12.04s/it]_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7758, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7758, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7758, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "  0%|▏                                       | 5/1587 [01:01<5:13:16, 11.88s/it]_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7758, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7758, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7758, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "  0%|▏                                       | 6/1587 [01:12<5:10:24, 11.78s/it]_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7758, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7758, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7758, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "  0%|▏                                       | 7/1587 [01:24<5:08:49, 11.73s/it]_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7759, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7759, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7759, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "  1%|▏                                       | 8/1587 [01:36<5:07:24, 11.68s/it]_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7759, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7759, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7759, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "  1%|▏                                       | 9/1587 [01:47<5:06:30, 11.65s/it]_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7760, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7760, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7760, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "{'loss': 1.9431, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.01}         \n",
      "  1%|▏                                      | 10/1587 [01:59<5:05:51, 11.64s/it]_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7761, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7761, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7761, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "  1%|▎                                      | 11/1587 [02:10<5:05:11, 11.62s/it]_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7762, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7762, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7762, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "  1%|▎                                      | 12/1587 [02:22<5:04:37, 11.60s/it]_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7766, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7766, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7766, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "  1%|▎                                      | 13/1587 [02:34<5:04:19, 11.60s/it]_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7767, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7767, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7767, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "  1%|▎                                      | 14/1587 [02:45<5:03:49, 11.59s/it]_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7769, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7769, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7769, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "  1%|▎                                      | 15/1587 [02:57<5:03:30, 11.58s/it]_fsdp_wrapped_module.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.7772, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "_fsdp_wrapped_module.reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param tensor(1958.6390, device='cuda:0', grad_fn=<SumBackward0>) tensor(1.4175, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "!cd .. && CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 torchrun \\\n",
    "    --nnodes=1 \\\n",
    "    --nproc_per_node=6 \\\n",
    "    --rdzv-id=$SLURM_JOB_ID \\\n",
    "    --rdzv-backend=c10d \\\n",
    "    --rdzv-endpoint=$RDZV_ENDPOINT \\\n",
    "doremi/train.py \\\n",
    "    --model_name_or_path=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/EleutherAI/pythia-1.4b \\\n",
    "    --model_type=pythia \\\n",
    "    --do_train \\\n",
    "    --cache_dir=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache \\\n",
    "    --dataset_dir=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed \\\n",
    "    --domain_config_path=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/configs/humanmix_uniform_baseline_50kvocab.json \\\n",
    "    --max_token_length=1024 \\\n",
    "    --per_device_train_batch_size=7 \\\n",
    "    --gradient_accumulation_steps=3 \\\n",
    "    --dataloader_num_workers=1 \\\n",
    "    --learning_rate=2e-05 \\\n",
    "    --lr_scheduler_type=linear \\\n",
    "    --warmup_ratio=0.03 \\\n",
    "    --weight_decay=0. \\\n",
    "    --max_grad_norm=1.0 \\\n",
    "    --max_steps=1587 \\\n",
    "    --evaluation_strategy=no \\\n",
    "    --save_strategy=steps \\\n",
    "    --save_steps=100 \\\n",
    "    --save_total_limit=1 \\\n",
    "    --run_name=jpt_pythia-1.4b_humanmix_uniform:200k_doremiv2_fsdp=full_shard_gradckpt=True_mbsz=7_seqlen=1024_nodes=1 \\\n",
    "    --seed=1111 \\\n",
    "    --logging_strategy=steps \\\n",
    "    --logging_steps=10 \\\n",
    "    --logging_first_step \\\n",
    "    --report_to=none \\\n",
    "    --optim=adamw_hf \\\n",
    "    --adam_beta1=0.9 \\\n",
    "    --adam_beta2=0.99 \\\n",
    "    --fsdp=\"full_shard auto_wrap\" \\\n",
    "    --fsdp_transformer_layer_cls_to_wrap=\"GPTNeoXLayer\" \\\n",
    "    --gradient_checkpointing \\\n",
    "    --torch_dtype=float32 \\\n",
    "    --add_domain_id=True \\\n",
    "    --do_padding=True \\\n",
    "    --fp16 \\\n",
    "    --doremi_optimizer=doremiv2 \\\n",
    "    --reweight_eta=1 \\\n",
    "    --reweight_eps=1e-4 \\\n",
    "    --train_domain_weights_tmp_file=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/jpt_pythia-1.4b_humanmix_uniform:200k_doremiv2_fsdp=full_shard_gradckpt=True_mbsz=7_seqlen=1024_nodes=1/domain_weights \\\n",
    "    --reweight_domains \\\n",
    "    --remove_unused_columns=False \\\n",
    "    --reference_model_name_or_path=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2/pythia-1.4b_humanmix_uniform:200k_baseline/ \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/jpt_pythia-1.4b_humanmix_uniform:200k_doremiv2_fsdp=full_shard_gradckpt=True_mbsz=7_seqlen=1024_nodes=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ccd170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "!cd .. && CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 torchrun \\\n",
    "    --nnodes=1 \\\n",
    "    --nproc_per_node=6 \\\n",
    "    --rdzv-id=$SLURM_JOB_ID \\\n",
    "    --rdzv-backend=c10d \\\n",
    "    --rdzv-endpoint=$RDZV_ENDPOINT \\\n",
    "doremi/train.py \\\n",
    "    --model_name_or_path=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/EleutherAI/pythia-1.4b \\\n",
    "    --model_type=pythia \\\n",
    "    --do_train \\\n",
    "    --cache_dir=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/cache \\\n",
    "    --dataset_dir=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/data/processed \\\n",
    "    --domain_config_path=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/configs/humanmix_uniform_baseline_50kvocab.json \\\n",
    "    --max_token_length=1024 \\\n",
    "    --per_device_train_batch_size=7 \\\n",
    "    --gradient_accumulation_steps=3 \\\n",
    "    --dataloader_num_workers=1 \\\n",
    "    --learning_rate=2e-05 \\\n",
    "    --lr_scheduler_type=linear \\\n",
    "    --warmup_ratio=0.03 \\\n",
    "    --weight_decay=0. \\\n",
    "    --max_grad_norm=1.0 \\\n",
    "    --max_steps=1587 \\\n",
    "    --evaluation_strategy=no \\\n",
    "    --save_strategy=steps \\\n",
    "    --save_steps=100 \\\n",
    "    --save_total_limit=1 \\\n",
    "    --run_name=jpt_pythia-1.4b_humanmix_uniform:200k_doremiv2_fsdp=full_shard_gradckpt=True_mbsz=7_seqlen=1024_nodes=1 \\\n",
    "    --seed=1111 \\\n",
    "    --logging_strategy=steps \\\n",
    "    --logging_steps=10 \\\n",
    "    --logging_first_step \\\n",
    "    --report_to=none \\\n",
    "    --optim=adamw_hf \\\n",
    "    --adam_beta1=0.9 \\\n",
    "    --adam_beta2=0.99 \\\n",
    "    --fsdp=\"full_shard auto_wrap\" \\\n",
    "    --fsdp_transformer_layer_cls_to_wrap=\"GPTNeoXLayer\" \\\n",
    "    --gradient_checkpointing \\\n",
    "    --torch_dtype=float32 \\\n",
    "    --add_domain_id=True \\\n",
    "    --do_padding=True \\\n",
    "    --fp16 \\\n",
    "    --doremi_optimizer=doremiv2 \\\n",
    "    --reweight_eta=1 \\\n",
    "    --reweight_eps=1e-4 \\\n",
    "    --train_domain_weights_tmp_file=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/jpt_pythia-1.4b_humanmix_uniform:200k_doremiv2_fsdp=full_shard_gradckpt=True_mbsz=7_seqlen=1024_nodes=1/domain_weights \\\n",
    "    --reweight_domains \\\n",
    "    --remove_unused_columns=False \\\n",
    "    --reference_model_name_or_path=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2/pythia-1.4b_humanmix_uniform:200k_baseline/ \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/doremi/results/jpt_pythia-1.4b_humanmix_uniform:200k_doremiv2_fsdp=full_shard_gradckpt=True_mbsz=7_seqlen=1024_nodes=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80d02dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.distributed.fsdp import FlatParameter\n",
    "from torch.nn import Parameter\n",
    "\n",
    "\n",
    "x = FlatParameter(torch.tensor([1,2,3.]), requires_grad=True)\n",
    "x.sum()\n",
    "\n",
    "if 'gpt_neox.layers.0._fsdp_wrapped_module._flat_param' in param_name:\n",
    "    print(param_name, param)\n",
    "    print(param.sum(), param_max())\n",
    "\n",
    "if 'reference_model.gpt_neox.layers.0._fsdp_wrapped_module._flat_param' in param_name:\n",
    "    print(param_name, param)\n",
    "    print(param.sum(), param_max())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        if self.is_local_process_zero():\n",
    "            logger.info('logging reference model weights:')\n",
    "            if self.model.reference_model:\n",
    "                logger.info(self.model.reference_model.gpt_neox.embed_in.weight.sum())\n",
    "                logger.info(self.model.reference_model.gpt_neox.embed_in.weight.max())\n",
    "                logger.info(self.model.reference_model.embed_out.weight.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6cc4b9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6s/it, 2.6hrs\n"
     ]
    }
   ],
   "source": [
    "t = '00:01:30'\n",
    "n = 16\n",
    "# total = 1515; nnodes = 1\n",
    "# total = 2083; nnodes = 1\n",
    "# total = 1587; nnodes = 1\n",
    "total = 1666; nnoodes = 5\n",
    "# total = 1041; nnodes = 1\n",
    "# total = 4228; nnodes = 1\n",
    "# total = 4512; nnodes = 4\n",
    "# total = 4296; nnodes = 1\n",
    "# total = 2254; nnodes = 2\n",
    "# total = 1128; nnodes = 4\n",
    "# total = 1074; nnodes = 4\n",
    "# total = 1252; nnodes = 4\n",
    "\n",
    "l = [int(x) for x in t.split(':')]\n",
    "t = l[0]*60*60+l[1]*60+l[2]\n",
    "# t = t/60/60 # in hr\n",
    "\n",
    "print(f'{t/n/nnodes:.0f}s/it, {t/n*total/60/60:.1f}hrs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
