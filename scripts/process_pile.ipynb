{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18c95852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ce043fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.iterable_dataset.IterableDataset at 0x7ff70ea302e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "def pile_transform(tokenizer, max_length, seed=None):\n",
    "    def transform(batch):\n",
    "        # tokenize\n",
    "        examples = tokenizer(batch['text'])\n",
    "\n",
    "        # Concatenate all texts.\n",
    "        examples = {k: list(itertools.chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "        if total_length >= max_length:\n",
    "            total_length = (total_length // max_length) * max_length\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "            for k, t in examples.items()\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    return transform\n",
    "transform = pile_transform(tokenizer, 1024, 111)\n",
    "\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset('EleutherAI/pile', 'all', streaming=True)['train']\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed8d96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3180 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([1026,  318, 1760,  ..., 1088,  838, 4201]),\n",
       "  'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1])},\n",
       " {'input_ids': tensor([ 329,  257, 7480,  ..., 6626,  656, 1811]),\n",
       "  'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1])},\n",
       " {'input_ids': tensor([1180, 2628,   11,  ...,  284,  564,  250]),\n",
       "  'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1])}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_ds = ds.map(transform, batched=True, remove_columns=['text', 'meta'], batch_size=30)\n",
    "mapped_ds = mapped_ds.with_format(\"torch\")\n",
    "\n",
    "o = list(mapped_ds.take(3))\n",
    "o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
